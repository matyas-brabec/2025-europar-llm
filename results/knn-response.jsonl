{"id": "batch_req_689f7db6ebf48190a7372102e9f4568e", "custom_id": "knn01-01", "response": {"status_code": 200, "request_id": "b22c006ba1a9ba8a04abd650afd87a06", "body": {"id": "chatcmpl-C4sfw7githIF4u2fLdCFtcpyTTUV5", "object": "chat.completion", "created": 1755279520, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <math_constants.h>\n#include <utility>\n\n// This implementation assigns one thread block per query point.\n// Each block scans the entire data set in tiles of size k and maintains the current\n// top-k nearest neighbors in shared memory. For each tile, we merge the current top-k\n// with the k new candidates by performing an in-place bitonic sort on 2k elements\n// in shared memory and then keep the first k elements as the updated top-k.\n// This approach avoids any additional global memory allocations and keeps the\n// memory footprint and synchronization confined within a block.\n//\n// Notes:\n// - k is assumed to be a power of two in [32, 1024] (as per problem statement).\n// - The distance metric is squared Euclidean distance.\n// - Shared memory required per block: 2*k*(sizeof(float)+sizeof(int)) bytes (<= 16KB for k=1024).\n// - threads per block: 256 (chosen as a good default for A100/H100).\n//\n// Performance considerations:\n// - The kernel is memory-bandwidth bound for large datasets; the selection cost per tile is kept\n//   moderate by sorting 2k elements per tile (bitonic sort).\n// - Memory loads are coalesced as threads within a block read contiguous data indices.\n// - The union arrays are maintained in shared memory to minimize global traffic.\n\nstruct PairIF {\n    int   first;\n    float second;\n};\n\n// In-place bitonic sort of N pairs (keys = dists ascending, values = idxs) in shared memory.\n// N must be a power of two. Sorting order: ascending by distance; ties broken by index to be deterministic.\n__device__ inline void bitonic_sort_pairs_asc(float* dists, int* idxs, int N) {\n    // Standard bitonic sort network operating on shared memory arrays.\n    // Each thread processes multiple elements in strides of blockDim.x.\n    for (int size = 2; size <= N; size <<= 1) {\n        // Sort in ascending order when ((i & size) == 0), descending otherwise.\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            __syncthreads();\n            for (int i = threadIdx.x; i < N; i += blockDim.x) {\n                int ixj = i ^ stride;\n                if (ixj > i) {\n                    bool ascending = ((i & size) == 0);\n                    float di = dists[i];\n                    float dj = dists[ixj];\n                    int   ii = idxs[i];\n                    int   ij = idxs[ixj];\n\n                    // Compare by distance first, then by index to break ties deterministically.\n                    bool should_swap;\n                    if (ascending) {\n                        should_swap = (di > dj) || ((di == dj) && (ii > ij));\n                    } else {\n                        should_swap = (di < dj) || ((di == dj) && (ii < ij));\n                    }\n                    if (should_swap) {\n                        dists[i]  = dj;\n                        dists[ixj] = di;\n                        idxs[i]   = ij;\n                        idxs[ixj]  = ii;\n                    }\n                }\n            }\n        }\n    }\n    __syncthreads();\n}\n\n__global__ void knn2d_kernel(const float2* __restrict__ query,\n                              int query_count,\n                              const float2* __restrict__ data,\n                              int data_count,\n                              PairIF* __restrict__ result,\n                              int k)\n{\n    int qid = blockIdx.x;\n    if (qid >= query_count) return;\n\n    // Shared memory layout:\n    // [0 .. 2k-1] float union_dists\n    // [0 .. 2k-1] int   union_idxs\n    extern __shared__ unsigned char smem[];\n    float* union_dists = reinterpret_cast<float*>(smem);\n    int*   union_idxs  = reinterpret_cast<int*>(union_dists + 2 * k);\n\n    const int two_k = 2 * k;\n\n    // Load query point into registers\n    float2 q = query[qid];\n\n    // Initialize the union arrays to +inf and -1.\n    // Lower half [0..k-1] will hold the current best k results (initialized to +inf).\n    // Upper half [k..2k-1] will be filled with the current tile's candidates.\n    for (int i = threadIdx.x; i < two_k; i += blockDim.x) {\n        union_dists[i] = CUDART_INF_F;\n        union_idxs[i]  = -1;\n    }\n    __syncthreads();\n\n    // Process the dataset in tiles of size k\n    for (int base = 0; base < data_count; base += k) {\n        const int tile_count = min(k, data_count - base);\n\n        // Fill the upper half of union arrays with distances for the current tile.\n        // For positions beyond tile_count, fill with +inf to keep N=2k consistent.\n        for (int t = threadIdx.x; t < k; t += blockDim.x) {\n            if (t < tile_count) {\n                int idx = base + t;\n                float2 p = data[idx];\n                float dx = p.x - q.x;\n                float dy = p.y - q.y;\n                // Squared L2 distance; use FMA to improve throughput/precision slightly.\n                float d = __fmaf_rn(dx, dx, dy * dy);\n                union_dists[k + t] = d;\n                union_idxs[k + t]  = idx;\n            } else {\n                union_dists[k + t] = CUDART_INF_F;\n                union_idxs[k + t]  = -1;\n            }\n        }\n        __syncthreads();\n\n        // Sort 2k elements; the smallest k will end up in positions [0..k-1]\n        bitonic_sort_pairs_asc(union_dists, union_idxs, two_k);\n        // No need to move/copy: union[0..k-1] now holds the updated best k,\n        // union[k..2k-1] can be overwritten by the next tile.\n    }\n\n    // Write out the final top-k neighbors for this query\n    // The output array is row-major: result[qid * k + j] corresponds to the j-th NN.\n    for (int j = threadIdx.x; j < k; j += blockDim.x) {\n        PairIF r;\n        r.first  = union_idxs[j];\n        r.second = union_dists[j];\n        result[static_cast<size_t>(qid) * static_cast<size_t>(k) + static_cast<size_t>(j)] = r;\n    }\n}\n\n// Host-side launcher\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    // Ensure our device-side PairIF layout matches std::pair<int,float>\n    static_assert(sizeof(PairIF) == sizeof(std::pair<int, float>), \"PairIF must match std::pair<int,float> layout\");\n\n    const int threads_per_block = 256;  // Good default for A100/H100\n    const int blocks = query_count;     // One block per query\n\n    // Shared memory: 2k floats + 2k ints\n    const size_t smem_bytes = static_cast<size_t>(2 * k) * (sizeof(float) + sizeof(int));\n\n    PairIF* result_alias = reinterpret_cast<PairIF*>(result);\n\n    knn2d_kernel<<<blocks, threads_per_block, smem_bytes>>>(query, query_count, data, data_count, result_alias, k);\n\n    // Optional synchronization to ensure completion before returning.\n    // cudaDeviceSynchronize();\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 563, "completion_tokens": 18622, "total_tokens": 19185, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 16960, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db6dda48190abba5c6319d42c7a", "custom_id": "knn01-02", "response": {"status_code": 200, "request_id": "6a72a65110ee2ff13f48c9723df54d07", "body": {"id": "chatcmpl-C4siJPvAiDodNYuyiG5CYj07BsyBs", "object": "chat.completion", "created": 1755279667, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// This implementation assigns one CUDA block per query point.\n// Each block scans all data points and maintains a top-K set using a shared-memory max-heap.\n// Threads cooperatively initialize the heap with the first K distances, then stream through\n// the remaining data, attempting to insert a candidate into the heap only if it beats\n// the current worst (root of the heap). Concurrency on heap updates is controlled via\n// a lightweight shared-memory spinlock. This approach is scalable for large data_count,\n// and efficient because only O(K) successful heap updates are expected per query.\n//\n// At the end, the heap is heap-sorted in-place (O(K log K)) to produce ascending results,\n// and the indices/distances are written to the output.\n//\n// Notes:\n// - Distance is squared Euclidean: (dx*dx + dy*dy)\n// - k is a power of two between 32 and 1024 inclusive, and data_count >= k\n// - Uses dynamic shared memory sized as k*(sizeof(float)+sizeof(int)) bytes\n// - Uses 256 threads per block by default, which provides a good balance on A100/H100\n\n#ifndef KNN_THREADS_PER_BLOCK\n#define KNN_THREADS_PER_BLOCK 256\n#endif\n\n// Sift-down operation for a max-heap stored in arrays (distances and corresponding indices).\n// This function assumes the heap property may be violated at 'start' and restores it.\nstatic __device__ __forceinline__\nvoid heap_sift_down(float* dists, int* idxs, int start, int heap_size) {\n    int i = start;\n    float valDist = dists[i];\n    int valIdx = idxs[i];\n\n    while (true) {\n        int left = (i << 1) + 1; // 2*i + 1\n        if (left >= heap_size) break;\n        int right = left + 1;\n\n        // Select the larger child (by distance) for max-heap\n        int larger = left;\n        float largerDist = dists[left];\n        if (right < heap_size) {\n            float rightDist = dists[right];\n            if (rightDist > largerDist) {\n                larger = right;\n                largerDist = rightDist;\n            }\n        }\n\n        if (valDist >= largerDist) {\n            break;\n        }\n\n        // Move child up\n        dists[i] = largerDist;\n        idxs[i] = idxs[larger];\n        i = larger;\n    }\n\n    dists[i] = valDist;\n    idxs[i] = valIdx;\n}\n\n// Bottom-up heap construction (Floyd's algorithm) for a max-heap.\nstatic __device__ __forceinline__\nvoid heap_build(float* dists, int* idxs, int heap_size) {\n    for (int i = (heap_size >> 1) - 1; i >= 0; --i) {\n        heap_sift_down(dists, idxs, i, heap_size);\n    }\n}\n\n// In-place heap sort on a max-heap to produce ascending order (smallest distance first).\nstatic __device__ __forceinline__\nvoid heap_heapsort(float* dists, int* idxs, int heap_size) {\n    for (int n = heap_size; n > 1; --n) {\n        // Swap root (current max) with last element\n        float tmpD = dists[0];\n        int tmpI = idxs[0];\n        dists[0] = dists[n - 1];\n        idxs[0] = idxs[n - 1];\n        dists[n - 1] = tmpD;\n        idxs[n - 1] = tmpI;\n\n        // Restore heap property on the reduced heap [0, n-2]\n        heap_sift_down(dists, idxs, 0, n - 1);\n    }\n}\n\n// Kernel: each block handles one query point.\n__global__ void knn2d_block_streaming_kernel(\n    const float2* __restrict__ query,\n    int query_count,\n    const float2* __restrict__ data,\n    int data_count,\n    std::pair<int, float>* __restrict__ result,\n    int k)\n{\n    int qid = blockIdx.x;\n    if (qid >= query_count) return;\n\n    // Load query point into registers.\n    float2 q = query[qid];\n    float qx = q.x;\n    float qy = q.y;\n\n    // Dynamic shared memory layout:\n    // [0 .. k-1]: float distances (heap)\n    // [k .. 2k-1]: int indices (heap)\n    extern __shared__ unsigned char smem[];\n    float* sDists = reinterpret_cast<float*>(smem);\n    int* sIdx = reinterpret_cast<int*>(sDists + k);\n\n    // Shared lock to serialize heap updates.\n    __shared__ int heapLock;\n    if (threadIdx.x == 0) {\n        heapLock = 0;\n    }\n    __syncthreads();\n\n    // Phase 1: Parallel initialization - compute distances for the first k data points.\n    // Assumption: data_count >= k\n    for (int i = threadIdx.x; i < k; i += blockDim.x) {\n        float2 dp = data[i];\n        float dx = dp.x - qx;\n        float dy = dp.y - qy;\n        sDists[i] = dx * dx + dy * dy;\n        sIdx[i] = i;\n    }\n    __syncthreads();\n\n    // Build a max-heap of size k in shared memory (single thread; O(k)).\n    if (threadIdx.x == 0) {\n        heap_build(sDists, sIdx, k);\n    }\n    __syncthreads();\n\n    // Phase 2: Stream through remaining data and attempt to insert better candidates.\n    // Each thread processes indices in a strided fashion for coalesced global loads.\n    for (int i = threadIdx.x + k; i < data_count; i += blockDim.x) {\n        float2 dp = data[i];\n        float dx = dp.x - qx;\n        float dy = dp.y - qy;\n        float dist = dx * dx + dy * dy;\n\n        // Fast-path rejection using current heap root (worst among top-k).\n        // Reading sDists[0] is a broadcast from shared memory.\n        if (dist < sDists[0]) {\n            // Try to acquire lock and insert if still better than current root.\n            // This while loop spins only when a candidate is competitive and the lock is busy.\n            while (true) {\n                // Re-check before attempting lock to avoid unnecessary contention.\n                if (dist >= sDists[0]) break;\n                if (atomicCAS(&heapLock, 0, 1) == 0) {\n                    // Inside critical section; validate condition again.\n                    if (dist < sDists[0]) {\n                        sDists[0] = dist;\n                        sIdx[0] = i;\n                        heap_sift_down(sDists, sIdx, 0, k);\n                    }\n                    __threadfence_block(); // Ensure shared updates are visible within the block\n                    atomicExch(&heapLock, 0);\n                    break;\n                }\n            }\n        }\n    }\n    __syncthreads();\n\n    // Phase 3: Sort the heap to ascending order for output (single thread; O(k log k)).\n    if (threadIdx.x == 0) {\n        heap_heapsort(sDists, sIdx, k);\n    }\n    __syncthreads();\n\n    // Phase 4: Write results to global memory. For query qid, results occupy [qid*k .. qid*k + k - 1].\n    int out_base = qid * k;\n    for (int j = threadIdx.x; j < k; j += blockDim.x) {\n        // Write index and distance to std::pair fields.\n        result[out_base + j].first = sIdx[j];\n        result[out_base + j].second = sDists[j];\n    }\n}\n\n// Host function to launch the kernel.\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    // Choose launch configuration.\n    const int threads = KNN_THREADS_PER_BLOCK;\n    const int blocks = query_count;\n\n    // Dynamic shared memory size: k floats + k ints\n    size_t shared_bytes = static_cast<size_t>(k) * (sizeof(float) + sizeof(int));\n\n    // Launch the kernel: one block per query.\n    knn2d_block_streaming_kernel<<<blocks, threads, shared_bytes>>>(query, query_count, data, data_count, result, k);\n\n    // Ensure completion before returning (so that results are ready).\n    cudaDeviceSynchronize();\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 563, "completion_tokens": 14497, "total_tokens": 15060, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 12608, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db6d3608190b43dc2cd4376a907", "custom_id": "knn01-03", "response": {"status_code": 200, "request_id": "8e25d2b7cd9e00647bd101b81b5ae256", "body": {"id": "chatcmpl-C4sk5RMxK9pFh2R1v0DWP6nllfcyy", "object": "chat.completion", "created": 1755279777, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cuda_runtime_api.h>\n#include <utility>\n\n// Optimized CUDA k-NN for 2D points (squared Euclidean distance).\n// Key design:\n// - One thread block processes one query point (gridDim.x = query_count).\n// - All threads collaboratively stream through the data points in tiles.\n// - Each tile: every thread computes ITEMS_PER_THREAD distances; a warp-wide ballot\n//   compacts only those candidates that beat the current block-wide threshold into a\n//   shared \"keeper\" buffer.\n// - A single thread (thread 0) maintains a block-wide max-heap of size k in shared\n//   memory. Only \"keeper\" candidates are merged into the heap. This minimizes the\n//   sequential heap maintenance work.\n// - After all tiles, the heap is heap-sorted into ascending distance order and written\n//   to the output result for the query.\n//\n// Notes:\n// - No additional device memory is allocated; only shared memory is used.\n// - k is a power of two in [32, 1024]. threads_per_block is chosen as min(256, k).\n// - ITEMS_PER_THREAD is set to 16 for good arithmetic intensity and memory coalescing.\n// - Shared memory per block usage: 8 * (T + k) bytes, where T = threads_per_block * ITEMS_PER_THREAD.\n//   For worst-case threads_per_block=256, k<=1024, T=4096 -> smem ~ 8*(4096+1024)=40KB per block.\n// - This code assumes modern NVIDIA datacenter GPUs (e.g., A100/H100) and a recent CUDA toolkit.\n\n#ifndef KNN_ITEMS_PER_THREAD\n#define KNN_ITEMS_PER_THREAD 16\n#endif\n\n// Device utility: compute squared L2 distance between a data point and query.\nstatic __device__ __forceinline__ float squared_l2(const float2 a, const float qx, const float qy) {\n    float dx = a.x - qx;\n    float dy = a.y - qy;\n    // FMA to improve throughput and precision: dx*dx + dy*dy\n    return __fmaf_rn(dy, dy, dx * dx);\n}\n\n// Max-heap helpers for distances and indices stored in shared memory.\n// Heap property: parent has distance >= children distance.\nstatic __device__ __forceinline__ void heap_sift_up(float* hdist, int* hidx, int pos) {\n    while (pos > 0) {\n        int parent = (pos - 1) >> 1;\n        float dp = hdist[parent];\n        float dc = hdist[pos];\n        if (dp < dc) {\n            // Swap parent and child\n            int ip = hidx[parent];\n            hdist[parent] = dc; hidx[parent] = hidx[pos];\n            hdist[pos] = dp;     hidx[pos] = ip;\n            pos = parent;\n        } else {\n            break;\n        }\n    }\n}\n\nstatic __device__ __forceinline__ void heap_sift_down(float* hdist, int* hidx, int size, int pos = 0) {\n    while (true) {\n        int left = (pos << 1) + 1;\n        if (left >= size) break;\n        int right = left + 1;\n        int maxc = left;\n        if (right < size && hdist[right] > hdist[left]) {\n            maxc = right;\n        }\n        if (hdist[pos] < hdist[maxc]) {\n            float dp = hdist[pos];\n            int   ip = hidx[pos];\n            hdist[pos] = hdist[maxc]; hidx[pos] = hidx[maxc];\n            hdist[maxc] = dp;         hidx[maxc] = ip;\n            pos = maxc;\n        } else {\n            break;\n        }\n    }\n}\n\nstatic __device__ __forceinline__ void heap_push(float* hdist, int* hidx, int& size, float d, int i) {\n    int pos = size;\n    hdist[pos] = d;\n    hidx[pos] = i;\n    size++;\n    heap_sift_up(hdist, hidx, pos);\n}\n\nstatic __device__ __forceinline__ void heap_replace_root(float* hdist, int* hidx, int size, float d, int i) {\n    // Replace root and restore heap property\n    hdist[0] = d;\n    hidx[0] = i;\n    heap_sift_down(hdist, hidx, size, 0);\n}\n\nstatic __device__ __forceinline__ void heap_sort_ascending(float* hdist, int* hidx, int size) {\n    // In-place heapsort on a max-heap to produce ascending order.\n    for (int i = size - 1; i > 0; --i) {\n        // Swap root (current maximum) with the end\n        float dr = hdist[0];\n        int   ir = hidx[0];\n        hdist[0] = hdist[i]; hidx[0] = hidx[i];\n        hdist[i] = dr;       hidx[i] = ir;\n        // Restore heap on the reduced heap [0..i-1]\n        heap_sift_down(hdist, hidx, i, 0);\n    }\n}\n\n// Kernel: One block per query. Uses dynamic shared memory for both the keeper buffer and the heap.\ntemplate<int ITEMS_PER_THREAD>\n__global__ void knn_2d_kernel(const float2* __restrict__ query,\n                              int query_count,\n                              const float2* __restrict__ data,\n                              int data_count,\n                              std::pair<int, float>* __restrict__ result,\n                              int k)\n{\n    const int qid = blockIdx.x;\n    if (qid >= query_count) return;\n\n    // Load query point into registers\n    const float2 q = query[qid];\n    const float qx = q.x;\n    const float qy = q.y;\n\n    // Dynamic shared memory layout:\n    // [ keepDist | keepIdx | heapDist | heapIdx ]\n    extern __shared__ unsigned char smem_raw[];\n    float* keepDist = reinterpret_cast<float*>(smem_raw);\n    int*   keepIdx  = reinterpret_cast<int*>(keepDist + blockDim.x * ITEMS_PER_THREAD);\n    float* heapDist = reinterpret_cast<float*>(keepIdx + blockDim.x * ITEMS_PER_THREAD);\n    int*   heapIdx  = reinterpret_cast<int*>(heapDist + k);\n\n    // Shared scalars for coordination\n    __shared__ int sKeepCount;\n    __shared__ int sHeapSize;\n    __shared__ float sThreshold;\n\n    if (threadIdx.x == 0) {\n        sKeepCount = 0;\n        sHeapSize = 0;\n        sThreshold = CUDART_INF_F; // Until heap is filled to size k\n    }\n    __syncthreads();\n\n    // Process the dataset in tiles of T = blockDim.x * ITEMS_PER_THREAD points\n    const int T = blockDim.x * ITEMS_PER_THREAD;\n    const unsigned FULL_MASK = 0xffffffffu;\n    const int lane = threadIdx.x & 31;\n\n    for (int base = 0; base < data_count; base += T) {\n        // Snapshot current threshold at tile start\n        float tileThreshold = sThreshold;\n        int tileStart = base;\n\n        // Reset keeper counter for this tile\n        if (threadIdx.x == 0) sKeepCount = 0;\n        __syncthreads();\n\n        // Each thread computes up to ITEMS_PER_THREAD distances and compacts \"keepers\"\n        #pragma unroll\n        for (int it = 0; it < ITEMS_PER_THREAD; ++it) {\n            int idx = tileStart + it * blockDim.x + threadIdx.x;\n            bool in_range = (idx < data_count);\n\n            float d = 0.0f;\n            if (in_range) {\n                float2 p = data[idx];\n                d = squared_l2(p, qx, qy);\n            }\n\n            // Decide whether to keep this candidate based on the current block threshold.\n            // If heap is not full yet, keep all valid candidates.\n            bool heap_full = (sHeapSize >= k);\n            bool keep = in_range && (!heap_full || (d < tileThreshold));\n\n            // Warp-wide compaction: each warp atomically reserves space for its keepers\n            unsigned mask = __ballot_sync(FULL_MASK, keep);\n            int warpCount = __popc(mask);\n\n            if (warpCount > 0) {\n                int basePos = 0;\n                if (lane == 0) {\n                    basePos = atomicAdd(&sKeepCount, warpCount);\n                }\n                basePos = __shfl_sync(FULL_MASK, basePos, 0);\n\n                if (keep) {\n                    unsigned laneMask = mask & ((1u << lane) - 1u);\n                    int offset = basePos + __popc(laneMask);\n                    keepDist[offset] = d;\n                    keepIdx[offset]  = idx;\n                }\n            }\n        }\n\n        __syncthreads();\n\n        // Merge keepers into the heap (single-threaded to maintain heap invariants)\n        if (threadIdx.x == 0) {\n            int count = sKeepCount;\n            int hsize = sHeapSize;\n\n            // Fill the heap up to size k\n            int i = 0;\n            for (; i < count && hsize < k; ++i) {\n                heap_push(heapDist, heapIdx, hsize, keepDist[i], keepIdx[i]);\n            }\n\n            // Update threshold if heap now full\n            if (hsize == k) {\n                sThreshold = heapDist[0];\n            }\n\n            // For remaining candidates, only keep if better than current threshold\n            for (; i < count; ++i) {\n                float d = keepDist[i];\n                if (d < heapDist[0]) {\n                    heap_replace_root(heapDist, heapIdx, hsize, d, keepIdx[i]);\n                    sThreshold = heapDist[0]; // Update threshold to new root\n                }\n            }\n\n            sHeapSize = hsize;\n        }\n\n        __syncthreads();\n    }\n\n    // At this point, heap contains the k nearest neighbors for this query.\n    // Sort them in ascending distance order and write to output.\n    if (threadIdx.x == 0) {\n        int hsize = sHeapSize; // Should be k since data_count >= k\n        heap_sort_ascending(heapDist, heapIdx, hsize);\n\n        // Write results in ascending order: j-th nearest neighbor first\n        int out_base = qid * k;\n        for (int j = 0; j < k; ++j) {\n            result[out_base + j].first  = heapIdx[j];\n            result[out_base + j].second = heapDist[j];\n        }\n    }\n}\n\n// Host interface\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data, int data_count,\n             std::pair<int, float> *result, int k)\n{\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Threads per block: choose up to 256, but not exceeding k (and at least one warp).\n    int threads_per_block = k < 256 ? (k >= 128 ? 128 : (k >= 64 ? 64 : 32)) : 256;\n\n    // Launch one block per query\n    dim3 grid(query_count);\n    dim3 block(threads_per_block);\n\n    // Shared memory size: 8 * (T + k) bytes, where T = threads_per_block * ITEMS_PER_THREAD\n    const int T = threads_per_block * KNN_ITEMS_PER_THREAD;\n    size_t smem_size = static_cast<size_t>(T + k) * (sizeof(float) + sizeof(int));\n\n    // Increase the max dynamic shared memory size if needed (for Ampere/Hopper this can be large).\n    cudaFuncSetAttribute(knn_2d_kernel<KNN_ITEMS_PER_THREAD>,\n                         cudaFuncAttributeMaxDynamicSharedMemorySize,\n                         static_cast<int>(smem_size));\n\n    // Launch\n    knn_2d_kernel<KNN_ITEMS_PER_THREAD><<<grid, block, smem_size>>>(\n        query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 563, "completion_tokens": 17350, "total_tokens": 17913, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 14720, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db6e76481909c25808abf324b9d", "custom_id": "knn01-04", "response": {"status_code": 200, "request_id": "f7210dce463fc8c88c2aac18d90bd8fe", "body": {"id": "chatcmpl-C4smNwpzba9lLizAoowFb8fAdaUSS", "object": "chat.completion", "created": 1755279919, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n#include <utility>\n\n// Optimized k-NN (top-k) for 2D points using a single block per query.\n// Strategy:\n// - Each block processes one query and streams through the entire dataset in tiles.\n// - For each tile, threads compute distances and cooperatively sort the tile using\n//   CUB's BlockRadixSort (sorting key-value pairs: distance, index).\n// - Only the smallest k entries from the current tile are retained (stored in shared memory).\n// - These k entries are merged with the running global top-k for the query by sorting\n//   the union of size up to 2k using another BlockRadixSort; the smallest k are kept.\n// - This approach is correct and efficient: the global top-k must belong to the union\n//   of the per-tile top-k sets (any element not in a tile's top-k has at least k\n//   elements in the same tile that are smaller, so it cannot belong to the global top-k).\n// - All work is done in-place with registers, static shared memory (for CUB temp storage),\n//   and dynamic shared memory (for only 2k entries), without any device allocations.\n//\n// Assumptions (as per problem statement):\n// - k is a power of two in [32, 1024].\n// - data_count >= k.\n// - Inputs are large enough to benefit from GPU parallelism.\n// - Arrays are allocated by cudaMalloc; result is an array of std::pair<int, float>.\n//\n// Tuning choices:\n// - BLOCK_THREADS = 256: good occupancy and memory coalescing on A100/H100.\n// - TILE_ITEMS_PER_THREAD = 8: per tile we sort 256*8 = 2048 items, a good balance\n//   between shared memory usage and sort overhead.\n// - MERGE_ITEMS_PER_THREAD = 8: capacity for merging up to 2k = 2048 items when k=1024.\n//\n// Notes on memory usage per block:\n// - Dynamic shared memory: 2 * k * (sizeof(float) + sizeof(int)) = 8*k bytes (<= 8KB).\n// - Static shared memory: CUB BlockRadixSort temp storage (reused via a union).\n// - Fits within default per-block shared memory limits on A100/H100 without extra attributes.\n\ntemplate <int BLOCK_THREADS, int TILE_ITEMS_PER_THREAD, int MERGE_ITEMS_PER_THREAD>\n__global__ void knn2d_kernel(const float2* __restrict__ query,\n                             int query_count,\n                             const float2* __restrict__ data,\n                             int data_count,\n                             std::pair<int, float>* __restrict__ result,\n                             int k)\n{\n    using KeyT = float;\n    using ValT = int;\n\n    // Block-wide radix sort types for the tile sort and the merge sort.\n    using TileSort  = cub::BlockRadixSort<KeyT, BLOCK_THREADS, TILE_ITEMS_PER_THREAD, ValT>;\n    using MergeSort = cub::BlockRadixSort<KeyT, BLOCK_THREADS, MERGE_ITEMS_PER_THREAD, ValT>;\n\n    // Reuse the same static shared memory region for the two sorts via a union.\n    __shared__ union {\n        typename TileSort::TempStorage  tile_sort;\n        typename MergeSort::TempStorage merge_sort;\n    } sort_storage;\n\n    // Dynamic shared memory layout:\n    // [ topk_keys (k) | topk_vals (k) | tile_k_keys (k) | tile_k_vals (k) ]\n    extern __shared__ unsigned char smem[];\n    unsigned char* sptr = smem;\n\n    KeyT* s_topk_keys = reinterpret_cast<KeyT*>(sptr);       sptr += sizeof(KeyT) * k;\n    ValT* s_topk_vals = reinterpret_cast<ValT*>(sptr);       sptr += sizeof(ValT) * k;\n    KeyT* s_tilek_keys = reinterpret_cast<KeyT*>(sptr);      sptr += sizeof(KeyT) * k;\n    ValT* s_tilek_vals = reinterpret_cast<ValT*>(sptr);      // end\n\n    const int qid = blockIdx.x;\n    if (qid >= query_count) return;\n\n    // Load the query point once per block into shared memory to avoid redundant global loads.\n    __shared__ float2 q_sh;\n    if (threadIdx.x == 0) {\n        q_sh = query[qid];\n    }\n    __syncthreads();\n\n    const float qx = q_sh.x;\n    const float qy = q_sh.y;\n\n    const int TILE_SIZE = BLOCK_THREADS * TILE_ITEMS_PER_THREAD;\n    const KeyT INF = CUDART_INF_F;\n\n    int cur_k_count = 0; // number of valid entries in s_topk_* arrays\n\n    // Stream through data in tiles of TILE_SIZE elements.\n    for (int tile_start = 0; tile_start < data_count; tile_start += TILE_SIZE) {\n\n        const int tile_valid = min(TILE_SIZE, data_count - tile_start);\n\n        // Load distances and indices for this tile into per-thread registers.\n        KeyT keys_tile[TILE_ITEMS_PER_THREAD];\n        ValT vals_tile[TILE_ITEMS_PER_THREAD];\n\n        #pragma unroll\n        for (int i = 0; i < TILE_ITEMS_PER_THREAD; ++i) {\n            int idx = tile_start + threadIdx.x + i * BLOCK_THREADS;\n            if (idx < data_count) {\n                float2 p = data[idx];\n                float dx = p.x - qx;\n                float dy = p.y - qy;\n                // Squared L2 distance\n                KeyT d2 = fmaf(dy, dy, dx * dx);\n                keys_tile[i] = d2;\n                vals_tile[i] = idx;\n            } else {\n                keys_tile[i] = INF; // pad with +inf to keep processing simple\n                vals_tile[i] = -1;\n            }\n        }\n\n        // Sort the tile (blocked-to-blocked): after this, thread t owns the items in\n        // global sorted positions [t*IPT, t*IPT + IPT).\n        TileSort(sort_storage.tile_sort).SortBlockedToBlocked(keys_tile, vals_tile);\n        __syncthreads(); // ensure sort_storage reuse safety\n\n        // Keep only the smallest tile_k entries from this tile.\n        const int tile_k = (tile_valid < k) ? tile_valid : k;\n\n        // Write the smallest tile_k items from registers to shared memory buffer s_tilek_*.\n        // Each thread writes the portion of [0, tile_k) that it owns after the blocked sort.\n        {\n            const int base = threadIdx.x * TILE_ITEMS_PER_THREAD;\n            int count = tile_k - base;\n            if (count > TILE_ITEMS_PER_THREAD) count = TILE_ITEMS_PER_THREAD;\n            if (count < 0) count = 0;\n\n            #pragma unroll\n            for (int i = 0; i < count; ++i) {\n                s_tilek_keys[base + i] = keys_tile[i];\n                s_tilek_vals[base + i] = vals_tile[i];\n            }\n        }\n        __syncthreads();\n\n        // Merge current top-k and this tile's top-k by sorting their union (size <= 2k).\n        // Build the union in per-thread registers (blocked layout), padding with +inf as needed.\n        KeyT keys_merge[MERGE_ITEMS_PER_THREAD];\n        ValT vals_merge[MERGE_ITEMS_PER_THREAD];\n\n        const int union_total = cur_k_count + tile_k; // <= 2k by construction\n\n        #pragma unroll\n        for (int j = 0; j < MERGE_ITEMS_PER_THREAD; ++j) {\n            int pos = threadIdx.x * MERGE_ITEMS_PER_THREAD + j;\n            if (pos < cur_k_count) {\n                keys_merge[j] = s_topk_keys[pos];\n                vals_merge[j] = s_topk_vals[pos];\n            } else if (pos < union_total) {\n                int tpos = pos - cur_k_count;\n                keys_merge[j] = s_tilek_keys[tpos];\n                vals_merge[j] = s_tilek_vals[tpos];\n            } else {\n                keys_merge[j] = INF;\n                vals_merge[j] = -1;\n            }\n        }\n\n        // Sort the union and keep the smallest k into s_topk_*.\n        MergeSort(sort_storage.merge_sort).SortBlockedToBlocked(keys_merge, vals_merge);\n        __syncthreads(); // ensure sort_storage reuse safety\n\n        #pragma unroll\n        for (int j = 0; j < MERGE_ITEMS_PER_THREAD; ++j) {\n            int pos = threadIdx.x * MERGE_ITEMS_PER_THREAD + j;\n            if (pos < k) {\n                s_topk_keys[pos] = keys_merge[j];\n                s_topk_vals[pos] = vals_merge[j];\n            }\n        }\n        __syncthreads();\n\n        cur_k_count = (union_total < k) ? union_total : k;\n    }\n\n    // Store results for this query in ascending distance order.\n    // Each thread writes a striped subset of the k results.\n    std::pair<int, float>* out = result + static_cast<size_t>(qid) * static_cast<size_t>(k);\n    for (int j = threadIdx.x; j < k; j += BLOCK_THREADS) {\n        // Write fields individually to avoid invoking any std::pair constructors on device.\n        out[j].first  = s_topk_vals[j];\n        out[j].second = s_topk_keys[j];\n    }\n}\n\n// Host interface\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data, int data_count,\n             std::pair<int, float> *result, int k)\n{\n    // Tuned launch configuration for A100/H100:\n    // - 256 threads per block\n    // - Each block processes one query\n    // - Dynamic shared memory sized to hold 2*k (distance,index) pairs.\n    constexpr int BLOCK_THREADS = 256;\n    constexpr int TILE_ITEMS_PER_THREAD = 8;   // 256*8 = 2048 elements per tile\n    constexpr int MERGE_ITEMS_PER_THREAD = 8;  // 256*8 = 2048 capacity for up to 2k elements\n\n    dim3 block(BLOCK_THREADS);\n    dim3 grid(query_count);\n\n    // Dynamic shared memory needed per block:\n    // top-k (k) + tile-k (k), each having (float distance + int index).\n    size_t smem_bytes = static_cast<size_t>(2) * static_cast<size_t>(k) * (sizeof(float) + sizeof(int));\n\n    knn2d_kernel<BLOCK_THREADS, TILE_ITEMS_PER_THREAD, MERGE_ITEMS_PER_THREAD>\n        <<<grid, block, smem_bytes>>>(query, query_count, data, data_count, result, k);\n\n    // Optionally, synchronize or check for errors in production code.\n    // cudaDeviceSynchronize();\n    // cudaGetLastError();\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 563, "completion_tokens": 18844, "total_tokens": 19407, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 16512, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db6e4208190a664738a5204eb97", "custom_id": "knn01-05", "response": {"status_code": 200, "request_id": "e0a8eda180e80f644f502fc0fda6d331", "body": {"id": "chatcmpl-C4soxjxW2lgem3L1Z8zWNcdfli6Wi", "object": "chat.completion", "created": 1755280079, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cuda_runtime_api.h>\n#include <utility>\n\n// Optimized k-NN for 2D points using a block that processes multiple queries (one warp per query).\n// - Each block contains WARPS_PER_BLOCK warps (default 8), thus 256 threads.\n// - The block iterates over the data in tiles loaded into shared memory to maximize reuse across warps.\n// - Each warp maintains its own top-k as a max-heap in shared memory. Insertions/replacements are\n//   serialized within the warp via ballot/ffs to avoid races. Heap operations are O(log k).\n// - At the end, each warp performs an in-place heap sort to produce ascending nearest neighbors,\n//   then writes results [index, squared distance] for its assigned query.\n//\n// Notes:\n// - Distances are squared Euclidean distances (no sqrt).\n// - k is power-of-two in [32, 1024], and data_count >= k.\n// - The kernel uses only shared memory (no additional global allocations).\n// - For large k, we opt-in to larger dynamic shared memory (A100/H100 support up to ~164 KB).\n// - Hyperparameters chosen for H100/A100-class GPUs.\n//\n// You can tune WARPS_PER_BLOCK and BASE_TILE_POINTS below if needed.\n\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 8\n#endif\n\n#ifndef THREADS_PER_BLOCK\n#define THREADS_PER_BLOCK (WARPS_PER_BLOCK * 32)\n#endif\n\n// A reasonable base tile size. Actual tile_points used at runtime may be reduced to fit shared memory.\n#ifndef BASE_TILE_POINTS\n#define BASE_TILE_POINTS 2048\n#endif\n\n// Device utility: swap two entries in parallel arrays (distance and index).\n__device__ __forceinline__ void swap_pair(float &ad, int &ai, float &bd, int &bi) {\n    float td = ad;\n    int ti = ai;\n    ad = bd; ai = bi;\n    bd = td; bi = ti;\n}\n\n// Device utility: sift-up operation for max-heap (by distance) at position pos.\n__device__ __forceinline__ void heap_sift_up(float* __restrict__ hd, int* __restrict__ hi, int pos) {\n    while (pos > 0) {\n        int parent = (pos - 1) >> 1;\n        if (hd[parent] >= hd[pos]) break;\n        swap_pair(hd[parent], hi[parent], hd[pos], hi[pos]);\n        pos = parent;\n    }\n}\n\n// Device utility: sift-down operation for max-heap (by distance) starting at position pos with heap size n.\n__device__ __forceinline__ void heap_sift_down(float* __restrict__ hd, int* __restrict__ hi, int n, int pos) {\n    while (true) {\n        int left = (pos << 1) + 1;\n        if (left >= n) break;\n        int right = left + 1;\n        int largest = left;\n        if (right < n && hd[right] > hd[left]) largest = right;\n        if (hd[pos] >= hd[largest]) break;\n        swap_pair(hd[pos], hi[pos], hd[largest], hi[largest]);\n        pos = largest;\n    }\n}\n\n// Device utility: in-place heap sort (ascending) given a max-heap.\n// After completion, hd[0..n-1] and hi[0..n-1] are sorted ascending by distance.\n__device__ __forceinline__ void heap_sort_ascending(float* __restrict__ hd, int* __restrict__ hi, int n) {\n    for (int end = n - 1; end > 0; --end) {\n        swap_pair(hd[0], hi[0], hd[end], hi[end]);\n        heap_sift_down(hd, hi, end, 0);\n    }\n}\n\n// The main CUDA kernel.\n// Shared memory layout (dynamic):\n//   [0 .. tile_points-1]                               -> float2 tile data points (shared across warps in the block)\n//   [tile_points .. tile_points + warps*k - 1]         -> float warp_dists (per-warp heaps concatenated)\n//   [.. + warps*k .. + warps*k + warps*k - 1]          -> int   warp_indices (per-warp heaps concatenated)\n//   [.. + warps*k + warps*k .. + warps*k + warps*k + warps - 1] -> int warp_heap_size (one per warp)\n__global__ void knn2d_kernel(const float2* __restrict__ query,\n                             int query_count,\n                             const float2* __restrict__ data,\n                             int data_count,\n                             std::pair<int, float>* __restrict__ result,\n                             int k,\n                             int tile_points) {\n    extern __shared__ unsigned char smem[];\n    unsigned char* smem_ptr = smem;\n\n    // Shared memory pointers setup\n    float2* sm_tile = reinterpret_cast<float2*>(smem_ptr);\n    smem_ptr += static_cast<size_t>(tile_points) * sizeof(float2);\n\n    float* sm_warp_dists = reinterpret_cast<float*>(smem_ptr);\n    smem_ptr += static_cast<size_t>(WARPS_PER_BLOCK) * k * sizeof(float);\n\n    int* sm_warp_indices = reinterpret_cast<int*>(smem_ptr);\n    smem_ptr += static_cast<size_t>(WARPS_PER_BLOCK) * k * sizeof(int);\n\n    int* sm_warp_heap_size = reinterpret_cast<int*>(smem_ptr);\n    // smem_ptr += WARPS_PER_BLOCK * sizeof(int); // not needed further\n\n    const int warp_id = threadIdx.x >> 5;       // warp index within block\n    const int lane_id = threadIdx.x & 31;       // lane index\n    const unsigned full_mask = 0xFFFFFFFFu;\n\n    // This block handles WARPS_PER_BLOCK queries: global query index for this warp:\n    const int qidx = blockIdx.x * WARPS_PER_BLOCK + warp_id;\n    const bool warp_active = (qidx < query_count);\n\n    // Per-warp pointers into shared memory for the heap arrays.\n    float* __restrict__ heap_d = sm_warp_dists + warp_id * k;\n    int*   __restrict__ heap_i = sm_warp_indices + warp_id * k;\n    int*   __restrict__ heap_size_ptr = sm_warp_heap_size + warp_id;\n\n    // Initialize per-warp heap size.\n    if (lane_id == 0) {\n        *heap_size_ptr = 0;\n    }\n\n    // Broadcast the query point to all lanes in the warp.\n    float qx = 0.0f, qy = 0.0f;\n    if (warp_active) {\n        if (lane_id == 0) {\n            float2 q = query[qidx];\n            qx = q.x;\n            qy = q.y;\n        }\n        qx = __shfl_sync(full_mask, qx, 0);\n        qy = __shfl_sync(full_mask, qy, 0);\n    }\n\n    // Iterate over the data set in tiles.\n    for (int base = 0; base < data_count; base += tile_points) {\n        int count = data_count - base;\n        if (count > tile_points) count = tile_points;\n\n        // Cooperative load of the data tile into shared memory by all threads in the block.\n        for (int t = threadIdx.x; t < count; t += blockDim.x) {\n            sm_tile[t] = data[base + t];\n        }\n\n        __syncthreads();\n\n        // Each active warp processes this tile and updates its heap.\n        if (warp_active) {\n            for (int i = lane_id; i < count; i += 32) {\n                float2 p = sm_tile[i];\n                float dx = qx - p.x;\n                float dy = qy - p.y;\n                float dist = fmaf(dx, dx, dy * dy);\n                int   didx = base + i;\n\n                // Warp-serialized cooperative heap update.\n                // Lanes with candidates that should be inserted/replaced will be processed one at a time.\n                while (true) {\n                    // Read current heap size and threshold (root). Avoid reading heap_d[0] if empty.\n                    int hsz = *heap_size_ptr;\n                    float thr = (hsz > 0 ? heap_d[0] : CUDART_INF_F);\n\n                    // Decide if this lane wants to attempt an update.\n                    int want = (hsz < k) || (dist < thr);\n                    unsigned mask = __ballot_sync(full_mask, want);\n                    if (mask == 0u) break;\n\n                    int leader = __ffs(mask) - 1;  // 0..31\n                    if (lane_id == leader) {\n                        int cur_hsz = *heap_size_ptr;\n                        if (cur_hsz < k) {\n                            // Insert new element at the end, then sift-up.\n                            int pos = cur_hsz;\n                            heap_d[pos] = dist;\n                            heap_i[pos] = didx;\n                            *heap_size_ptr = cur_hsz + 1;\n                            heap_sift_up(heap_d, heap_i, pos);\n                        } else {\n                            // Replace root if better, then sift-down.\n                            if (dist < heap_d[0]) {\n                                heap_d[0] = dist;\n                                heap_i[0] = didx;\n                                heap_sift_down(heap_d, heap_i, k, 0);\n                            }\n                        }\n                    }\n                    __syncwarp(full_mask);\n                    // Re-evaluate; if this lane's candidate is no longer desirable, it will naturally stop.\n                }\n            }\n        }\n\n        __syncthreads();\n    }\n\n    // Finalize: each active warp sorts its heap ascending and writes out results.\n    if (warp_active) {\n        if (lane_id == 0) {\n            int hsz = *heap_size_ptr;\n            // hsz should be k (since data_count >= k). If not, heap_sort still works on hsz items.\n            if (hsz > 1) {\n                // The array is already a max-heap by construction; run in-place heap sort to ascending.\n                heap_sort_ascending(heap_d, heap_i, hsz);\n            }\n            // Write results in ascending order: j-th neighbor for query qidx.\n            int out_base = qidx * k;\n            // If hsz < k (shouldn't happen given data_count >= k), fill the rest with dummy values.\n            for (int j = 0; j < hsz; ++j) {\n                result[out_base + j].first  = heap_i[j];\n                result[out_base + j].second = heap_d[j];\n            }\n            for (int j = hsz; j < k; ++j) {\n                // Should not occur under given constraints, but kept for safety.\n                result[out_base + j].first  = -1;\n                result[out_base + j].second = CUDART_INF_F;\n            }\n        }\n    }\n}\n\n// Host launcher: selects tile size to fit shared memory, sets opt-in shared memory size, and launches the kernel.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data,  int data_count,\n             std::pair<int, float> *result, int k) {\n    // Hyperparameters tuned for A100/H100-class GPUs.\n    const int warps_per_block = WARPS_PER_BLOCK;\n    const int threads_per_block = THREADS_PER_BLOCK;\n\n    // Compute how much shared memory is needed as a function of tile_points.\n    auto smem_required = [&](int tile_points) -> size_t {\n        size_t tile_bytes = static_cast<size_t>(tile_points) * sizeof(float2);\n        size_t heaps_bytes = static_cast<size_t>(warps_per_block) * static_cast<size_t>(k) * (sizeof(float) + sizeof(int));\n        size_t control_bytes = static_cast<size_t>(warps_per_block) * sizeof(int); // heap sizes\n        return tile_bytes + heaps_bytes + control_bytes;\n    };\n\n    // Query the device's opt-in shared memory limit and the default per-block limit.\n    int device = 0;\n    cudaGetDevice(&device);\n    int max_optin_smem = 0;\n    cudaDeviceGetAttribute(&max_optin_smem, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    int default_smem = 0;\n    cudaDeviceGetAttribute(&default_smem, cudaDevAttrMaxSharedMemoryPerBlock, device);\n\n    // Base tile size. We may reduce it if needed to fit shared memory constraints.\n    int tile_points = BASE_TILE_POINTS;\n\n    // Compute minimum shared memory required without the tile to ensure k/warps fit.\n    size_t min_required_wo_tile = static_cast<size_t>(warps_per_block) * static_cast<size_t>(k) * (sizeof(float) + sizeof(int))\n                                + static_cast<size_t>(warps_per_block) * sizeof(int);\n\n    // Choose the maximum available shared memory (opt-in if supported).\n    int max_available_smem = max_optin_smem > 0 ? max_optin_smem : default_smem;\n\n    // Clamp tile_points to fit within available shared memory.\n    if (smem_required(tile_points) > static_cast<size_t>(max_available_smem)) {\n        // Reduce tile_points to the largest value that fits.\n        size_t available_for_tile = static_cast<size_t>(max_available_smem) - min_required_wo_tile;\n        int max_tile_points_fit = static_cast<int>(available_for_tile / sizeof(float2));\n        if (max_tile_points_fit < 1) {\n            // As a last resort, force tile_points to 1 (should not happen for target GPUs and k<=1024).\n            tile_points = 1;\n        } else {\n            tile_points = max_tile_points_fit;\n        }\n    }\n\n    size_t dynamic_smem_bytes = smem_required(tile_points);\n\n    // Opt-in to larger dynamic shared memory if supported.\n    if (max_optin_smem > 0 && dynamic_smem_bytes > static_cast<size_t>(default_smem)) {\n        cudaFuncSetAttribute(knn2d_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize,\n                             static_cast<int>(dynamic_smem_bytes));\n    }\n\n    // Grid sizing: one warp per query, WARPS_PER_BLOCK warps per block.\n    int blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Launch kernel.\n    knn2d_kernel<<<blocks, threads_per_block, dynamic_smem_bytes>>>(\n        query, query_count, data, data_count, result, k, tile_points\n    );\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 563, "completion_tokens": 22322, "total_tokens": 22885, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 19200, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db6e5cc8190a4a0e22ed1d4d3a7", "custom_id": "knn01-06", "response": {"status_code": 200, "request_id": "c7966f4baa5b2809c0763e2e5762d4cf", "body": {"id": "chatcmpl-C4srhGNw2WKLpme2jWHtMqcses3w8", "object": "chat.completion", "created": 1755280249, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#include <utility>\n#include <cfloat>\n\n// Optimized brute-force k-NN (squared L2) for 2D points.\n// Strategy:\n// - Assign one warp per query.\n// - The block cooperatively tiles the data points into shared memory to maximize reuse across warps.\n// - Within a warp, each lane scans a strided subset of the tile and maintains its top-L candidates,\n//   where L = k / 32 (k is guaranteed to be a power of two between 32 and 1024, so L in [1, 32]).\n//   For each lane, we store L best candidates in small unsorted buffers and track the current threshold\n//   (the maximum among them); when a better candidate is found, we replace the current max and recompute\n//   the new max by scanning L elements. Since replacements are relatively infrequent after warmup,\n//   this is efficient and avoids heavy per-candidate O(log L) heap costs.\n// - After scanning all data, each lane sorts its L candidates ascending (insertion sort; L <= 32).\n// - The 32 sorted lists (one per lane) are stored in shared memory per warp and then merged by lane 0\n//   using a small min-heap of size 32 to produce the final k sorted results, which are written to 'result'.\n//\n// Shared memory layout per block (dynamic):\n//   [tile of float2 points][warp_idx_buf (W*k ints)][warp_dist_buf (W*k floats)]\n//   W = WARPS_PER_BLOCK\n//\n// The shared memory size (worst case k=1024) is chosen to stay within 48KB default per-block limit\n// without requiring opt-in attributes: WARPS_PER_BLOCK=4, TILE_SIZE=2048 => 16KB (tile) + 32KB (merge buffers) = 48KB.\n//\n// Thread-block configuration: 4 warps (128 threads). Grid size covers all queries.\n// Modern GPUs (A100/H100) handle many such blocks concurrently; the dominant cost is memory bandwidth.\n\n// Tunable kernel parameters (chosen to fit 48KB shared mem limit at k=1024):\n#ifndef KNN_WARPS_PER_BLOCK\n#define KNN_WARPS_PER_BLOCK 4\n#endif\n\n#ifndef KNN_TILE_SIZE\n#define KNN_TILE_SIZE 2048\n#endif\n\nstatic constexpr int WARP_SIZE = 32;\n\n// Small per-lane top-L container with threshold (\"tau\") tracking.\n// - Stores up to MaxL = 32 best candidates (distance, index).\n// - Insertion: if not full, append and update tau; else if new dist < tau, replace tau slot and recompute tau in O(L).\n// - After scanning, supports in-place insertion sort to ascending order.\nstruct LaneTopL {\n    float dist[WARP_SIZE]; // capacity up to 32\n    int   idx[WARP_SIZE];\n    int   count;\n    int   capacity;\n    float tau;     // current maximum distance among stored\n    int   tau_pos; // index of the current maximum\n\n    __device__ inline void init(int cap) {\n        capacity = cap;\n        count = 0;\n        tau = -FLT_MAX;\n        tau_pos = -1;\n        // No need to initialize buffers here.\n    }\n\n    __device__ inline void consider(float d, int id) {\n        if (count < capacity) {\n            dist[count] = d;\n            idx[count]  = id;\n            if (d > tau) {\n                tau = d;\n                tau_pos = count;\n            }\n            ++count;\n        } else {\n            if (d < tau) {\n                // Replace current maximum and recompute tau.\n                dist[tau_pos] = d;\n                idx[tau_pos]  = id;\n                // Recompute max across the L elements.\n                int n = capacity;\n                float maxd = dist[0];\n                int maxp = 0;\n                #pragma unroll\n                for (int i = 1; i < WARP_SIZE; ++i) {\n                    if (i >= n) break;\n                    float v = dist[i];\n                    if (v > maxd) { maxd = v; maxp = i; }\n                }\n                tau = maxd;\n                tau_pos = maxp;\n            }\n        }\n    }\n\n    __device__ inline void sort_ascending() {\n        // Simple insertion sort; capacity <= 32.\n        int n = count;\n        for (int i = 1; i < n; ++i) {\n            float keyd = dist[i];\n            int   keyi = idx[i];\n            int j = i - 1;\n            while (j >= 0 && dist[j] > keyd) {\n                dist[j + 1] = dist[j];\n                idx[j + 1]  = idx[j];\n                --j;\n            }\n            dist[j + 1] = keyd;\n            idx[j + 1]  = keyi;\n        }\n    }\n};\n\n// Kernel to compute k-NN for 2D points (squared Euclidean distances).\ntemplate <int WARPS_PER_BLOCK, int TILE_SIZE>\n__global__ void knn2d_kernel(const float2* __restrict__ query,\n                             int query_count,\n                             const float2* __restrict__ data,\n                             int data_count,\n                             std::pair<int, float>* __restrict__ result,\n                             int k)\n{\n    // Shared memory:\n    extern __shared__ unsigned char smem[];\n    float2* s_points = reinterpret_cast<float2*>(smem);\n    const size_t s_points_bytes = static_cast<size_t>(TILE_SIZE) * sizeof(float2);\n\n    int* s_idx = reinterpret_cast<int*>(smem + s_points_bytes);\n    float* s_dist = reinterpret_cast<float*>(s_idx + WARPS_PER_BLOCK * k);\n\n    // Thread and warp identifiers\n    const int tid = threadIdx.x;\n    const int warp_local = tid / WARP_SIZE;  // warp index within block [0..WARPS_PER_BLOCK-1]\n    const int lane = tid % WARP_SIZE;\n\n    const int warp_global_q = blockIdx.x * WARPS_PER_BLOCK + warp_local; // query index processed by this warp\n    const bool active_warp = (warp_global_q < query_count);\n\n    // Per-warp shared memory buffers for merging:\n    int*   warp_idx_buf  = s_idx  + warp_local * k;\n    float* warp_dist_buf = s_dist + warp_local * k;\n\n    // Load query point once per warp and broadcast to lanes.\n    float qx = 0.0f, qy = 0.0f;\n    if (active_warp) {\n        if (lane == 0) {\n            float2 q = query[warp_global_q];\n            qx = q.x;\n            qy = q.y;\n        }\n        unsigned mask = 0xFFFFFFFFu;\n        qx = __shfl_sync(mask, qx, 0);\n        qy = __shfl_sync(mask, qy, 0);\n    }\n\n    // Each lane maintains top-L candidates.\n    const int L = k >> 5; // k / 32, guaranteed integer (k power-of-two >= 32)\n    LaneTopL top;\n    if (active_warp) {\n        top.init(L);\n    }\n\n    // Loop over data in tiles cooperatively loaded by the block.\n    for (int tile_base = 0; tile_base < data_count; tile_base += TILE_SIZE) {\n        int tile_elems = data_count - tile_base;\n        if (tile_elems > TILE_SIZE) tile_elems = TILE_SIZE;\n\n        // Cooperative load of the tile into shared memory.\n        for (int i = tid; i < tile_elems; i += blockDim.x) {\n            s_points[i] = data[tile_base + i];\n        }\n        __syncthreads();\n\n        // Each active warp processes the tile: strided access per lane.\n        if (active_warp) {\n            for (int i = lane; i < tile_elems; i += WARP_SIZE) {\n                float2 p = s_points[i];\n                float dx = p.x - qx;\n                float dy = p.y - qy;\n                float d = dx * dx + dy * dy; // squared L2\n                int idx = tile_base + i;\n                top.consider(d, idx);\n            }\n        }\n\n        __syncthreads(); // ensure tile not used before next load\n    }\n\n    // After processing all data: sort each lane's top-L ascending and write to per-warp shared memory.\n    if (active_warp) {\n        top.sort_ascending();\n        // Store lane's sorted list at [lane * L, lane * L + L)\n        int base = lane * L;\n        #pragma unroll\n        for (int j = 0; j < WARP_SIZE; ++j) {\n            if (j >= L) break;\n            warp_dist_buf[base + j] = top.dist[j];\n            warp_idx_buf [base + j] = top.idx[j];\n        }\n    }\n\n    // Synchronize lanes within the warp before merging.\n    __syncwarp();\n\n    // Lane 0 performs a 32-way merge using a small min-heap and writes directly to the result array.\n    if (active_warp && lane == 0) {\n        // Min-heap over the current head of each lane's list.\n        // Heap arrays: distances, indices, and source list IDs.\n        float hDist[WARP_SIZE];\n        int   hIdx [WARP_SIZE];\n        int   hList[WARP_SIZE];\n        int   hSize = WARP_SIZE;\n\n        // Current positions within each lane's list.\n        int pos[WARP_SIZE];\n        #pragma unroll\n        for (int i = 0; i < WARP_SIZE; ++i) pos[i] = 0;\n\n        // Initialize heap with the first element from each of the 32 lists.\n        // All lists have length L >= 1 because k >= 32 (guaranteed).\n        #pragma unroll\n        for (int i = 0; i < WARP_SIZE; ++i) {\n            int base = i * L;\n            hDist[i] = warp_dist_buf[base + 0];\n            hIdx[i]  = warp_idx_buf [base + 0];\n            hList[i] = i;\n            pos[i]   = 1;\n        }\n\n        // Helper: heapify-down for min-heap at position 0.\n        auto heapify_down = [&](int start, int size) {\n            int i = start;\n            while (true) {\n                int l = 2 * i + 1;\n                int r = l + 1;\n                int smallest = i;\n                if (l < size && hDist[l] < hDist[smallest]) smallest = l;\n                if (r < size && hDist[r] < hDist[smallest]) smallest = r;\n                if (smallest == i) break;\n                // swap i and smallest\n                float td = hDist[i]; int ti = hIdx[i]; int tl = hList[i];\n                hDist[i] = hDist[smallest]; hIdx[i] = hIdx[smallest]; hList[i] = hList[smallest];\n                hDist[smallest] = td; hIdx[smallest] = ti; hList[smallest] = tl;\n                i = smallest;\n            }\n        };\n\n        // Build initial heap (bottom-up).\n        for (int i = (hSize >> 1) - 1; i >= 0; --i) {\n            heapify_down(i, hSize);\n        }\n\n        // Merge and write results.\n        const int out_base = warp_global_q * k;\n        for (int out = 0; out < k; ++out) {\n            // Root of heap is the smallest current head.\n            float bestD = hDist[0];\n            int   bestI = hIdx[0];\n            int   list  = hList[0];\n\n            // Write to result (ascending order).\n            result[out_base + out].first  = bestI;\n            result[out_base + out].second = bestD;\n\n            // Advance in the source list; if exhausted, remove from heap.\n            if (pos[list] < L) {\n                int base = list * L;\n                hDist[0] = warp_dist_buf[base + pos[list]];\n                hIdx[0]  = warp_idx_buf [base + pos[list]];\n                // hList[0] remains 'list'\n                pos[list]++;\n                heapify_down(0, hSize);\n            } else {\n                // Replace root with last element and shrink heap.\n                --hSize;\n                if (hSize > 0) {\n                    hDist[0] = hDist[hSize];\n                    hIdx[0]  = hIdx[hSize];\n                    hList[0] = hList[hSize];\n                    heapify_down(0, hSize);\n                }\n            }\n        }\n    }\n}\n\n// Host API: launches the CUDA kernel.\n// query: pointer to query_count float2 points.\n// data: pointer to data_count float2 points.\n// result: pointer to query_count * k std::pair<int, float> output (indices and squared distances).\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Kernel configuration\n    constexpr int WARPS_PER_BLOCK = KNN_WARPS_PER_BLOCK;  // 4 warps\n    constexpr int BLOCK_THREADS = WARPS_PER_BLOCK * WARP_SIZE; // 128 threads\n    constexpr int TILE_SIZE = KNN_TILE_SIZE; // 2048 points per tile\n\n    dim3 block(BLOCK_THREADS, 1, 1);\n    dim3 grid((query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK, 1, 1);\n\n    // Dynamic shared memory size: tile + per-warp merge buffers (indices + distances).\n    size_t smem_bytes = static_cast<size_t>(TILE_SIZE) * sizeof(float2)\n                      + static_cast<size_t>(WARPS_PER_BLOCK) * static_cast<size_t>(k) * sizeof(int)\n                      + static_cast<size_t>(WARPS_PER_BLOCK) * static_cast<size_t>(k) * sizeof(float);\n\n    // Launch kernel\n    knn2d_kernel<WARPS_PER_BLOCK, TILE_SIZE><<<grid, block, smem_bytes>>>(query, query_count, data, data_count, result, k);\n\n    // The function assumes that the caller handles synchronization and error checking if needed.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 563, "completion_tokens": 18383, "total_tokens": 18946, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 15232, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db6e4a88190ae9eb8f62962d686", "custom_id": "knn01-07", "response": {"status_code": 200, "request_id": "2c54bdbcbd7f89e92f7ff46f80817826", "body": {"id": "chatcmpl-C4suawdMVGFjc8qIaSHfeOSrkfsjN", "object": "chat.completion", "created": 1755280428, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cfloat>\n\n// This implementation assigns one CUDA block to each query point.\n// It scans the dataset in tiles of size k (where k is a power-of-two between 32 and 1024).\n// For each tile, it computes distances to the query and sorts the tile's distances\n// in ascending order using a block-wide bitonic sort in shared memory. It maintains\n// a block-local top-k list (also sorted ascending) in shared memory and merges the\n// sorted tile with the current top-k using a linear two-pointer merge (O(k)).\n// This yields the k smallest distances across all tiles after the full scan.\n// It avoids any global memory allocations and uses only per-block shared memory.\n\nstruct PairIF {\n    int first;   // index in 'data'\n    float second; // squared distance\n};\n\n// Block-wide bitonic sort in shared memory for K elements.\n// Sorts keys[] ascending and permutes vals[] accordingly.\n// Requires exactly K threads per block.\ntemplate<int K>\n__device__ inline void bitonic_sort_shared(float* keys, int* vals) {\n    // Standard bitonic sort network\n    unsigned int tid = threadIdx.x;\n    // Outer loop controls the size of the sequences to merge\n    for (unsigned int size = 2; size <= (unsigned int)K; size <<= 1) {\n        // Inner loop controls the stride of the compare-exchange\n        for (unsigned int stride = size >> 1; stride > 0; stride >>= 1) {\n            __syncthreads();\n            unsigned int partner = tid ^ stride;\n            if (partner > tid) {\n                bool up = ((tid & size) == 0); // sort direction for this pair\n                float key_i = keys[tid];\n                float key_p = keys[partner];\n                int val_i = vals[tid];\n                int val_p = vals[partner];\n                // Decide if we need to swap\n                bool do_swap = up ? (key_i > key_p) : (key_i < key_p);\n                if (do_swap) {\n                    keys[tid] = key_p;\n                    keys[partner] = key_i;\n                    vals[tid] = val_p;\n                    vals[partner] = val_i;\n                }\n            }\n        }\n    }\n    __syncthreads();\n}\n\ntemplate<int K>\n__global__ void knn2d_kernel(const float2* __restrict__ query,\n                             int query_count,\n                             const float2* __restrict__ data,\n                             int data_count,\n                             PairIF* __restrict__ result) {\n    // Each block handles one query\n    int qid = blockIdx.x;\n    if (qid >= query_count) return;\n\n    // Shared memory layout:\n    // [0 .. K-1]   tile_dist (float)\n    // [K .. 2K-1]  tile_idx  (int)\n    // [2K .. 3K-1] topk_dist (float)\n    // [3K .. 4K-1] topk_idx  (int)\n    // [4K .. 5K-1] merge_dist (float)\n    // [5K .. 6K-1] merge_idx  (int)\n    extern __shared__ unsigned char smem_raw[];\n    float* tile_dist  = reinterpret_cast<float*>(smem_raw);\n    int*   tile_idx   = reinterpret_cast<int*>(tile_dist + K);\n    float* topk_dist  = reinterpret_cast<float*>(tile_idx + K);\n    int*   topk_idx   = reinterpret_cast<int*>(topk_dist + K);\n    float* merge_dist = reinterpret_cast<float*>(topk_idx + K);\n    int*   merge_idx  = reinterpret_cast<int*>(merge_dist + K);\n\n    const int tid = threadIdx.x;\n\n    // Load query into registers\n    float2 q = query[qid];\n\n    // Initialize top-k with +infinity distances and invalid indices\n    if (tid < K) {\n        topk_dist[tid] = CUDART_INF_F;\n        topk_idx[tid] = -1;\n    }\n    __syncthreads();\n\n    // Process dataset in tiles of K points\n    const int tiles = (data_count + K - 1) / K;\n    for (int t = 0; t < tiles; ++t) {\n        int data_idx = t * K + tid;\n\n        // Compute squared Euclidean distance for this thread's data element\n        if (data_idx < data_count) {\n            float2 p = data[data_idx];\n            float dx = p.x - q.x;\n            float dy = p.y - q.y;\n            // Use FMA to accumulate dx*dx + dy*dy with improved throughput\n            float dist = fmaf(dy, dy, dx * dx);\n            tile_dist[tid] = dist;\n            tile_idx[tid] = data_idx;\n        } else {\n            // Pad out-of-range elements with +inf so they sort to the end\n            tile_dist[tid] = CUDART_INF_F;\n            tile_idx[tid] = -1;\n        }\n        __syncthreads();\n\n        // Sort tile distances ascending\n        bitonic_sort_shared<K>(tile_dist, tile_idx);\n\n        // Merge sorted tile (tile_dist/tile_idx) with current top-k (topk_dist/topk_idx)\n        if (tid == 0) {\n            int i = 0; // index into tile_dist\n            int j = 0; // index into topk_dist\n            for (int out = 0; out < K; ++out) {\n                // Take the smaller of the two current candidates\n                if (tile_dist[i] <= topk_dist[j]) {\n                    merge_dist[out] = tile_dist[i];\n                    merge_idx[out] = tile_idx[i];\n                    // Advance in tile; i < K always true because tile is padded with +inf\n                    ++i;\n                } else {\n                    merge_dist[out] = topk_dist[j];\n                    merge_idx[out] = topk_idx[j];\n                    // Advance in top-k; j < K always true because top-k is filled with +inf initially\n                    ++j;\n                }\n            }\n            // Copy merged result back into top-k buffers\n            for (int x = 0; x < K; ++x) {\n                topk_dist[x] = merge_dist[x];\n                topk_idx[x] = merge_idx[x];\n            }\n        }\n        __syncthreads();\n    }\n\n    // Write results: k nearest neighbors for this query, in ascending distance order\n    if (tid < K) {\n        int out_idx = qid * K + tid;\n        result[out_idx].first  = topk_idx[tid];\n        result[out_idx].second = topk_dist[tid];\n    }\n}\n\n// Host-side launcher: selects a specialized kernel based on k for optimal performance.\n// k is guaranteed to be a power-of-two between 32 and 1024 inclusive.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data, int data_count,\n             std::pair<int, float> *result, int k) {\n    if (query_count <= 0) return;\n    if (k < 32 || k > 1024) return; // Basic guard; k is assumed valid per problem statement\n\n    // Use a type with the same layout as std::pair<int, float> for device I/O\n    PairIF* d_out = reinterpret_cast<PairIF*>(result);\n\n    dim3 grid(query_count);\n    size_t smem_bytes = 0;\n\n    // Launch one specialized kernel per supported k to benefit from compile-time unrolling\n    switch (k) {\n        case 32: {\n            dim3 block(32);\n            smem_bytes = 6 * 32 * sizeof(int); // 3*float + 3*int == 6*4 bytes per element\n            // Correct the byte count: 6 arrays * 32 elements * 4 bytes = 768 bytes\n            smem_bytes = 6u * 32u * sizeof(int);\n            knn2d_kernel<32><<<grid, block, smem_bytes>>>(query, query_count, data, data_count, d_out);\n            break;\n        }\n        case 64: {\n            dim3 block(64);\n            smem_bytes = 6u * 64u * sizeof(int);\n            knn2d_kernel<64><<<grid, block, smem_bytes>>>(query, query_count, data, data_count, d_out);\n            break;\n        }\n        case 128: {\n            dim3 block(128);\n            smem_bytes = 6u * 128u * sizeof(int);\n            knn2d_kernel<128><<<grid, block, smem_bytes>>>(query, query_count, data, data_count, d_out);\n            break;\n        }\n        case 256: {\n            dim3 block(256);\n            smem_bytes = 6u * 256u * sizeof(int);\n            knn2d_kernel<256><<<grid, block, smem_bytes>>>(query, query_count, data, data_count, d_out);\n            break;\n        }\n        case 512: {\n            dim3 block(512);\n            smem_bytes = 6u * 512u * sizeof(int);\n            knn2d_kernel<512><<<grid, block, smem_bytes>>>(query, query_count, data, data_count, d_out);\n            break;\n        }\n        case 1024: {\n            dim3 block(1024);\n            smem_bytes = 6u * 1024u * sizeof(int);\n            knn2d_kernel<1024><<<grid, block, smem_bytes>>>(query, query_count, data, data_count, d_out);\n            break;\n        }\n        default:\n            // Should not happen given constraints; fall back to nearest lower power-of-two if needed\n            // but we return to avoid producing incorrect results.\n            return;\n    }\n\n    // Ensure kernel completion before returning (synchronous behavior)\n    cudaDeviceSynchronize();\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 563, "completion_tokens": 14367, "total_tokens": 14930, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 12224, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db6d3b88190ae17b282a1dbd418", "custom_id": "knn01-08", "response": {"status_code": 200, "request_id": "acf641a1f378f337f471f787ef02132a", "body": {"id": "chatcmpl-C4swJKQH0MzPvqrU20obIzvA8gaxt", "object": "chat.completion", "created": 1755280535, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// Optimized CUDA k-NN for 2D points.\n// Strategy:\n// - One block per query point.\n// - Multiple warps per block cooperatively traverse the data set with block-stride.\n// - Each warp maintains its exact top-k (smallest distances) for its assigned subset as a max-heap in shared memory.\n//   Only lane 0 of the warp updates the heap to avoid synchronization complexity.\n// - After scanning all data, each warp converts its heap into a sorted ascending array via in-place heapsort.\n// - Finally, thread 0 of the block performs a simple k-way merge across the warps' sorted lists to produce the query's global top-k.\n// - This approach guarantees correctness because every data point is processed by exactly one warp, and each warp preserves\n//   the k smallest values from its subset (k is up to 1024). No additional device memory allocations are required.\n//\n// Notes:\n// - Assumes data_count >= k, k is a power-of-two in [32, 1024].\n// - Uses blockDim.x = 512 by default (16 warps), which balances memory bandwidth and shared memory usage on A100/H100.\n// - Shared memory per block = warps * k * (sizeof(float)+sizeof(int)) bytes. With 16 warps and k=1024, this is 131072 bytes, which fits on A100/H100.\n// - Distances are squared Euclidean distances (no sqrt), as requested.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Max warps per block we use by default.\n#ifndef KNN_WARPS_PER_BLOCK\n#define KNN_WARPS_PER_BLOCK 16  // 16 warps = 512 threads per block\n#endif\n\n// Device utility: swap two elements in parallel arrays (distance and index)\n__device__ __forceinline__ void swap_pair(float &ad, int &ai, float &bd, int &bi) {\n    float td = ad; ad = bd; bd = td;\n    int ti = ai; ai = bi; bi = ti;\n}\n\n// Device utility: sift-up operation for a max-heap represented by parallel arrays.\n__device__ __forceinline__ void heap_sift_up(float* __restrict__ hd, int* __restrict__ hi, int pos) {\n    while (pos > 0) {\n        int parent = (pos - 1) >> 1;\n        if (hd[parent] >= hd[pos]) break;\n        swap_pair(hd[parent], hi[parent], hd[pos], hi[pos]);\n        pos = parent;\n    }\n}\n\n// Device utility: sift-down operation for a max-heap represented by parallel arrays.\n__device__ __forceinline__ void heap_sift_down(float* __restrict__ hd, int* __restrict__ hi, int size, int pos) {\n    while (true) {\n        int left = (pos << 1) + 1;\n        if (left >= size) break;\n        int largest = pos;\n        float best = hd[largest];\n\n        float lval = hd[left];\n        if (lval > best) {\n            largest = left;\n            best = lval;\n        }\n        int right = left + 1;\n        if (right < size) {\n            float rval = hd[right];\n            if (rval > best) {\n                largest = right;\n                best = rval;\n            }\n        }\n        if (largest == pos) break;\n        swap_pair(hd[pos], hi[pos], hd[largest], hi[largest]);\n        pos = largest;\n    }\n}\n\n// In-place heapsort on a max-heap to produce ascending order in the same arrays.\n// After calling, hd[0..size-1] will be in ascending order with corresponding hi entries.\n__device__ __forceinline__ void heap_sort_ascending(float* __restrict__ hd, int* __restrict__ hi, int size) {\n    // Standard heap sort: repeatedly place the current max at the end and re-heapify.\n    for (int end = size - 1; end > 0; --end) {\n        // Move current maximum (root) to the end.\n        swap_pair(hd[0], hi[0], hd[end], hi[end]);\n        // Restore heap property for reduced heap [0..end-1].\n        heap_sift_down(hd, hi, end, 0);\n    }\n    // Now array is in ascending order (since we used a max-heap).\n}\n\n// Kernel implementing block-per-query k-NN with per-warp exact top-k heaps and final k-way merge.\n__global__ void knn_kernel(const float2* __restrict__ query,\n                           int query_count,\n                           const float2* __restrict__ data,\n                           int data_count,\n                           std::pair<int, float>* __restrict__ result,\n                           int k)\n{\n    const int qid = blockIdx.x;\n    if (qid >= query_count) return;\n\n    // Broadcast the query point to the whole block through shared memory.\n    __shared__ float2 s_query;\n    if (threadIdx.x == 0) {\n        s_query = query[qid];\n    }\n    __syncthreads();\n    const float qx = s_query.x;\n    const float qy = s_query.y;\n\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n    const int warp = threadIdx.x >> 5;\n    const int warps = blockDim.x >> 5;\n\n    // Dynamic shared memory layout:\n    // [warps * k floats for per-warp heap distances][warps * k ints for per-warp heap indices]\n    extern __shared__ unsigned char smem[];\n    float* s_heap_dists = reinterpret_cast<float*>(smem);\n    int*   s_heap_index = reinterpret_cast<int*>(s_heap_dists + (size_t)warps * (size_t)k);\n\n    // Pointers to this warp's heap storage\n    float* my_hd = s_heap_dists + (size_t)warp * (size_t)k;\n    int*   my_hi = s_heap_index + (size_t)warp * (size_t)k;\n\n    // Lane 0 of each warp maintains the heap.\n    int heap_size = 0;\n\n    // Traverse the data in block-stride steps so all warps cover the entire set without gaps.\n    // Each iteration, every lane computes one candidate distance; lane 0 collects all 32 via shuffles\n    // and updates the warp's heap.\n    for (int di = threadIdx.x; di < data_count; di += blockDim.x) {\n        // Compute squared distance for this lane's assigned data index, if valid.\n        int idx = -1;\n        float dist = 0.0f;\n        if (di < data_count) {\n            float2 p = data[di];\n            float dx = p.x - qx;\n            float dy = p.y - qy;\n            // Use FMA to improve performance/precision: dx*dx + dy*dy\n            dist = fmaf(dx, dx, dy * dy);\n            idx = di;\n        }\n\n        // Ensure all lanes have computed their candidate before shuffling.\n        __syncwarp();\n\n        // Lane 0 aggregates and maintains the heap.\n        if (lane == 0) {\n            // Local references for speed\n            float* __restrict__ hd = my_hd;\n            int*   __restrict__ hi = my_hi;\n\n            // For each lane's candidate in the warp\n            #pragma unroll\n            for (int l = 0; l < WARP_SIZE; ++l) {\n                int cidx  = __shfl_sync(0xFFFFFFFFu, idx, l);\n                if (cidx < 0) continue; // invalid candidate (out of range in tail iteration)\n                float cdst = __shfl_sync(0xFFFFFFFFu, dist, l);\n\n                if (heap_size < k) {\n                    // Insert into heap (max-heap): push back then sift up.\n                    int pos = heap_size++;\n                    hd[pos] = cdst;\n                    hi[pos] = cidx;\n                    heap_sift_up(hd, hi, pos);\n                } else {\n                    // Prune using current worst (root of max-heap).\n                    if (cdst >= hd[0]) continue;\n                    // Replace worst and sift down.\n                    hd[0] = cdst;\n                    hi[0] = cidx;\n                    heap_sift_down(hd, hi, heap_size, 0);\n                }\n            }\n        }\n\n        // Synchronize warp before next iteration.\n        __syncwarp();\n    }\n\n    // Convert each warp's heap to ascending order so we can perform an efficient k-way merge.\n    if (lane == 0) {\n        // Given data_count >= k, heap_size should be exactly k; if not, heap_sort_ascending still works with heap_size.\n        heap_sort_ascending(my_hd, my_hi, heap_size);\n    }\n\n    __syncthreads();\n\n    // Final k-way merge across warps' sorted lists, executed by a single thread.\n    if (threadIdx.x == 0) {\n        const int out_base = qid * k;\n\n        // Positions within each warp's sorted list.\n        int pos[KNN_WARPS_PER_BLOCK]; // Upper bound on warps per block; we launch with <= this.\n        // warps may be less than KNN_WARPS_PER_BLOCK; initialize and use only first 'warps' entries.\n        #pragma unroll\n        for (int w = 0; w < KNN_WARPS_PER_BLOCK; ++w) pos[w] = 0;\n\n        // Perform k selections by scanning current heads of all warps (warps is small: <= 16).\n        for (int t = 0; t < k; ++t) {\n            float best_d = CUDART_INF_F;\n            int best_idx = -1;\n            int best_w = -1;\n\n            // Scan warps for the smallest current head\n            for (int w = 0; w < warps; ++w) {\n                int p = pos[w];\n                if (p >= heap_size) continue; // Should not occur since heap_size==k, but kept for safety\n                float d = s_heap_dists[(size_t)w * (size_t)k + p];\n                int   i = s_heap_index[(size_t)w * (size_t)k + p];\n                if (d < best_d) {\n                    best_d = d;\n                    best_idx = i;\n                    best_w = w;\n                }\n            }\n\n            // Write the next nearest neighbor for this query\n            result[out_base + t].first  = best_idx;\n            result[out_base + t].second = best_d;\n\n            // Advance the chosen warp's pointer\n            pos[best_w] += 1;\n        }\n    }\n}\n\n// Host interface: configures and launches the kernel.\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k)\n{\n    // Choose threads per block. 512 (16 warps) offers strong memory-level parallelism and fits shared memory\n    // even for k=1024 on A100/H100.\n    const int warps_per_block = KNN_WARPS_PER_BLOCK;\n    const int threads_per_block = warps_per_block * WARP_SIZE;\n\n    // Grid: one block per query\n    dim3 grid(query_count);\n    dim3 block(threads_per_block);\n\n    // Dynamic shared memory size: per-warp heap of k floats + k ints.\n    size_t smem_bytes = static_cast<size_t>(warps_per_block) * static_cast<size_t>(k) * (sizeof(float) + sizeof(int));\n\n    knn_kernel<<<grid, block, smem_bytes>>>(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 563, "completion_tokens": 19296, "total_tokens": 19859, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 16768, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db6da488190b96449644ded4f83", "custom_id": "knn01-09", "response": {"status_code": 200, "request_id": "0fad4ea63c4cda8241ecad4043e8f5be", "body": {"id": "chatcmpl-C4sz46bhbpISoxMeuPmaHY6fuzDUq", "object": "chat.completion", "created": 1755280706, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// Optimized k-NN (k-nearest neighbors) for 2D points with squared Euclidean distances.\n// Strategy:\n// - One CUDA block handles one query point.\n// - The block streams through all data points in tiles.\n// - Each tile, threads compute distances and collect \"survivors\" whose distances are <= current kth distance (threshold).\n// - Survivors are merged with the current top-k using a shared-memory bitonic sort on (top_k + survivors) elements.\n// - Only when there are survivors do we perform a merge; otherwise, we skip sorting to minimize overhead.\n// - k is a power of two between 32 and 1024 inclusive, data_count >= k. Arrays are in device memory.\n// - No extra device memory is allocated; only shared memory is used.\n//\n// Notes on performance/tuning for A100/H100:\n// - BLOCK_DIM = 256 and ITEMS_PER_THREAD = 4 produce TILE_SIZE = 1024, which keeps the sort size bounded at <= 2048 elements,\n//   yielding a 16KB shared buffer (distances + indices), allowing high occupancy with 96KB per SM.\n// - Warp-aggregated atomics are used to pack survivors efficiently into the shared buffer.\n// - Sorting is bitonic in shared memory, padded to the next power-of-two with +INF sentinels as needed.\n\nstatic inline int next_pow2_host(int x) {\n    // Assumes x > 0\n    x--;\n    x |= x >> 1;\n    x |= x >> 2;\n    x |= x >> 4;\n    x |= x >> 8;\n    x |= x >> 16;\n    x++;\n    return x;\n}\n\n__device__ inline int next_pow2_device(int x) {\n    // Assumes x > 0\n    x--;\n    x |= x >> 1;\n    x |= x >> 2;\n    x |= x >> 4;\n    x |= x >> 8;\n    x |= x >> 16;\n    x++;\n    return x;\n}\n\n// Warp-aggregated append: packs items from a warp into a shared buffer using one atomicAdd per warp.\n// Returns the write position for this thread if pred is true, or -1 otherwise.\n__device__ __forceinline__ int warp_aggregated_append(bool pred, int* counter) {\n    unsigned int mask = __ballot_sync(0xffffffffu, pred);\n    int count = __popc(mask);\n    int lane = threadIdx.x & 31;\n    int pos = -1;\n    if (count) {\n        int leader = __ffs(mask) - 1; // first active lane in warp\n        int base = 0;\n        if (lane == leader) {\n            base = atomicAdd(counter, count);\n        }\n        // Broadcast base to all lanes in the warp that are active in this ballot\n        base = __shfl_sync(mask, base, leader);\n        int prefix = __popc(mask & ((1u << lane) - 1));\n        if (pred) {\n            pos = base + prefix;\n        }\n    }\n    return pos;\n}\n\n// In-place bitonic sort on pairs (vals, idx) of length Npow2 (power-of-two).\n// We assume that vals[nActive..Npow2-1] are padded with +INF and idx with -1 so that sorting correctness holds.\n// Sort ascending by vals.\n__device__ inline void bitonic_sort_pairs(float* vals, int* idx, int nPow2) {\n    // Classic bitonic sort network operating entirely in shared memory.\n    for (int size = 2; size <= nPow2; size <<= 1) {\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            for (int i = threadIdx.x; i < nPow2; i += blockDim.x) {\n                int ixj = i ^ stride;\n                if (ixj > i) {\n                    bool up = ((i & size) == 0);\n                    float vi = vals[i];\n                    float vj = vals[ixj];\n                    int ii = idx[i];\n                    int ij = idx[ixj];\n                    // For ascending sort in the \"up\" region:\n                    // - if up == true, ensure vals[i] <= vals[ixj]\n                    // - if up == false, ensure vals[i] >= vals[ixj]\n                    bool do_swap = (up ? (vi > vj) : (vi < vj));\n                    if (do_swap) {\n                        vals[i] = vj;\n                        vals[ixj] = vi;\n                        idx[i] = ij;\n                        idx[ixj] = ii;\n                    }\n                }\n            }\n            __syncthreads();\n        }\n    }\n}\n\ntemplate<int BLOCK_DIM, int ITEMS_PER_THREAD>\n__global__ void knn_kernel(const float2* __restrict__ query,\n                           int query_count,\n                           const float2* __restrict__ data,\n                           int data_count,\n                           std::pair<int, float>* __restrict__ result,\n                           int k) {\n    // Shared memory layout (dynamic):\n    // [0 .. SMEM_ELEMS-1] floats for distances\n    // [SMEM_ELEMS .. 2*SMEM_ELEMS-1] ints for indices\n    extern __shared__ unsigned char smem[];\n    // Capacity of shared arrays in elements = next_pow2(k + TILE_SIZE)\n    const int TILE_SIZE = BLOCK_DIM * ITEMS_PER_THREAD;\n    const int SMEM_ELEMS = next_pow2_device(k + TILE_SIZE);\n\n    float* sh_dists = reinterpret_cast<float*>(smem);\n    int* sh_index = reinterpret_cast<int*>(sh_dists + SMEM_ELEMS);\n\n    __shared__ int topk_len;       // number of valid items currently in top-k (<= k)\n    __shared__ float threshold;    // current k-th smallest distance (worst in top-k); +INF if topk_len < k\n\n    const int qid = blockIdx.x;\n    if (qid >= query_count) return;\n\n    // Load query into registers\n    float2 q = query[qid];\n    const float qx = q.x;\n    const float qy = q.y;\n\n    // Initialize top-k state\n    if (threadIdx.x == 0) {\n        topk_len = 0;\n        threshold = CUDART_INF_F; // No threshold until we have at least k items\n    }\n    __syncthreads();\n\n    // Stream across the dataset in tiles\n    for (int base = 0; base < data_count; base += TILE_SIZE) {\n        // Determine the bounds of this tile\n        int tile_end = base + TILE_SIZE;\n        if (tile_end > data_count) tile_end = data_count;\n\n        // Reset survivors counter\n        __shared__ int survivors_count;\n        if (threadIdx.x == 0) survivors_count = 0;\n        __syncthreads();\n\n        // Snapshot current top-k state for use within this tile\n        const int curr_topk_len = topk_len;\n        const float curr_threshold = threshold;\n\n        // Process the tile: each thread handles multiple items with a stride of BLOCK_DIM\n        for (int idx = base + threadIdx.x; idx < tile_end; idx += BLOCK_DIM) {\n            // Load point and compute squared L2 distance\n            float2 p = data[idx];\n            float dx = p.x - qx;\n            float dy = p.y - qy;\n            float dist = fmaf(dy, dy, dx * dx);\n\n            // Selection predicate:\n            // - If we don't have k items yet, keep everything to fill the top-k.\n            // - Otherwise, keep only candidates not worse than current threshold.\n            bool keep = (curr_topk_len < k) || (dist <= curr_threshold);\n\n            // Warp-aggregated append into survivors region [curr_topk_len .. curr_topk_len + survivors_count)\n            int pos = warp_aggregated_append(keep, &survivors_count);\n            if (pos >= 0) {\n                int write_idx = curr_topk_len + pos;\n                sh_dists[write_idx] = dist;\n                sh_index[write_idx] = idx;\n            }\n        }\n        __syncthreads();\n\n        // If there are survivors, merge them with current top-k using bitonic sort\n        int ns = topk_len + survivors_count; // total active candidates to consider\n        if (ns > 0) {\n            // For the first tile(s), when topk_len == 0, survivors_count == tile size; this builds initial top-k.\n            // Pad up to next power-of-two for the sort with +INF sentinels.\n            int nPow2 = next_pow2_device(ns);\n            for (int i = threadIdx.x + ns; i < nPow2; i += BLOCK_DIM) {\n                sh_dists[i] = CUDART_INF_F;\n                sh_index[i] = -1;\n            }\n            __syncthreads();\n\n            // Note: The existing top-k (if any) must be at sh_{dists,index}[0..topk_len-1] and already sorted ascending.\n            // Survivors are at [topk_len .. ns-1]. Sorting the full [0..nPow2-1] yields a merged ascending order.\n            bitonic_sort_pairs(sh_dists, sh_index, nPow2);\n\n            // Update top-k (keep only first k items, which are the smallest)\n            if (threadIdx.x == 0) {\n                topk_len = (ns < k ? ns : k);\n                threshold = sh_dists[topk_len - 1]; // kth smallest distance\n            }\n        }\n        __syncthreads();\n        // After this point, the first topk_len entries of sh_dists/sh_index hold the block's current top-k in ascending order.\n    }\n\n    // Write final top-k for this query to the result array (ascending order).\n    // Each element result[qid * k + j] is {index, distance} of j-th nearest neighbor.\n    // Parallelize the write across the block's threads.\n    const int out_base = qid * k;\n    for (int j = threadIdx.x; j < k; j += BLOCK_DIM) {\n        result[out_base + j].first = sh_index[j];\n        result[out_base + j].second = sh_dists[j];\n    }\n}\n\n// Host interface as requested.\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    // Tunable parameters selected for A100/H100-class GPUs:\n    // - 256 threads per block\n    // - 4 items per thread per tile (TILE_SIZE = 1024)\n    // This keeps the shared memory footprint modest (<= 16KB for worst nPow2=2048),\n    // allows high occupancy, and bounds merge/sort cost.\n    constexpr int BLOCK_DIM = 256;\n    constexpr int ITEMS_PER_THREAD = 4;\n    constexpr int TILE_SIZE = BLOCK_DIM * ITEMS_PER_THREAD;\n\n    // Dynamic shared memory size in bytes:\n    // We need storage for N elements where N is the next power-of-two of (k + TILE_SIZE).\n    // Each element has a float distance and an int index.\n    const int smem_elems = next_pow2_host(k + TILE_SIZE);\n    const size_t smem_bytes = static_cast<size_t>(smem_elems) * (sizeof(float) + sizeof(int));\n\n    dim3 block(BLOCK_DIM);\n    dim3 grid(query_count);\n\n    // Launch the kernel. No additional device memory is allocated.\n    knn_kernel<BLOCK_DIM, ITEMS_PER_THREAD><<<grid, block, smem_bytes>>>(query, query_count, data, data_count, result, k);\n\n    // Note: This function is intentionally non-blocking. Synchronization and error checks (if desired)\n    // should be performed by the caller.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 563, "completion_tokens": 19311, "total_tokens": 19874, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 16768, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db6f2208190b6725416bb32711d", "custom_id": "knn01-10", "response": {"status_code": 200, "request_id": "e87699125aab3afdbe32c3f03776de8a", "body": {"id": "chatcmpl-C4t1dXqnCJ7G7arIZiGlTL4yOmXpC", "object": "chat.completion", "created": 1755280865, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n#include <utility>\n\n// Optimized k-NN for 2D points using block-wide radix sort and tile-wise reduction.\n// Each block processes one query point and streams over the dataset in tiles.\n// For each tile, we compute all distances, perform a block-wide sort to get the tile's top-k,\n// then merge with the running block top-k using another block-wide sort on the union (2k items).\n// This approach guarantees correctness: a globally top-k element must belong to its tile's top-k.\n// It uses only on-chip (shared) memory and no auxiliary device allocations.\n//\n// Assumptions:\n// - data_count >= k\n// - k is a power of two in [32, 1024], but the algorithm doesn't rely on power-of-two specifically.\n// - query_count is typically thousands, data_count typically millions.\n// - result is an array of std::pair<int,float> in device memory.\n//\n// Hyperparameters:\n// - BLOCK_THREADS: number of threads per block.\n// - TILE_ITEMS_PER_THREAD: number of elements each thread loads per tile, giving a tile size of\n//   TILE_ITEMS_PER_THREAD * BLOCK_THREADS. Must be >= k to guarantee the tile-wide top-k includes\n//   the global top-k candidates.\n// - MERGE_ITEMS_PER_THREAD: items per thread for merging 2k candidates (block top-k and tile top-k).\n//   Must satisfy MERGE_ITEMS_PER_THREAD * BLOCK_THREADS >= 2 * KMAX.\n//\n// Choices below target Ampere/Hopper GPUs. They give tiles of 2048 items per block and\n// 2k merge capacity up to k=1024.\n\n#ifndef KNN_BLOCK_THREADS\n#define KNN_BLOCK_THREADS 128\n#endif\n\n#ifndef KNN_TILE_ITEMS_PER_THREAD\n#define KNN_TILE_ITEMS_PER_THREAD 16\n#endif\n\n#ifndef KNN_KMAX\n#define KNN_KMAX 1024\n#endif\n\n// For union(2k) sort, with k up to 1024 and BLOCK_THREADS=128, we need at least 2048 capacity.\n// 128 * 16 = 2048, so MERGE_ITEMS_PER_THREAD = 16 is sufficient.\n#ifndef KNN_MERGE_ITEMS_PER_THREAD\n#define KNN_MERGE_ITEMS_PER_THREAD 16\n#endif\n\nstatic_assert(KNN_BLOCK_THREADS > 0 && (KNN_BLOCK_THREADS % 32) == 0, \"BLOCK_THREADS must be a positive multiple of warp size.\");\nstatic_assert(KNN_TILE_ITEMS_PER_THREAD > 0, \"TILE_ITEMS_PER_THREAD must be positive.\");\nstatic_assert(KNN_MERGE_ITEMS_PER_THREAD > 0, \"MERGE_ITEMS_PER_THREAD must be positive.\");\nstatic_assert(KNN_BLOCK_THREADS * KNN_TILE_ITEMS_PER_THREAD >= KNN_KMAX, \"Tile capacity must be >= KMAX.\");\nstatic_assert(KNN_BLOCK_THREADS * KNN_MERGE_ITEMS_PER_THREAD >= 2 * KNN_KMAX, \"Merge capacity must be >= 2*KMAX.\");\n\n__device__ __forceinline__ float sq_l2_distance_2d(const float2 a, const float2 b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    // FMA helps reduce latency and improve precision slightly.\n    return fmaf(dx, dx, dy * dy);\n}\n\ntemplate <typename T>\n__device__ __forceinline__ size_t align_up(size_t offset) {\n    const size_t align = alignof(T);\n    return (offset + (align - 1)) & ~(align - 1);\n}\n\n__global__ void knn2d_kernel(const float2* __restrict__ query,\n                             int query_count,\n                             const float2* __restrict__ data,\n                             int data_count,\n                             std::pair<int, float>* __restrict__ result,\n                             int k)\n{\n    // One block per query\n    const int qid = blockIdx.x;\n    if (qid >= query_count) return;\n\n    constexpr int BLOCK_THREADS = KNN_BLOCK_THREADS;\n    constexpr int TILE_ITEMS_PER_THREAD = KNN_TILE_ITEMS_PER_THREAD;\n    constexpr int TILE_ITEMS = BLOCK_THREADS * TILE_ITEMS_PER_THREAD;\n\n    constexpr int KMAX = KNN_KMAX;\n\n    constexpr int MERGE_ITEMS_PER_THREAD = KNN_MERGE_ITEMS_PER_THREAD;\n    constexpr int MERGE_ITEMS = BLOCK_THREADS * MERGE_ITEMS_PER_THREAD; // Must cover up to 2*KMAX\n\n    // CUB block radix sort types\n    using TileBlockSort = cub::BlockRadixSort<float, BLOCK_THREADS, TILE_ITEMS_PER_THREAD, int>;\n    using MergeBlockSort = cub::BlockRadixSort<float, BLOCK_THREADS, MERGE_ITEMS_PER_THREAD, int>;\n\n    // Dynamic shared memory layout\n    extern __shared__ unsigned char smem_raw[];\n    size_t offset = 0;\n\n    offset = align_up<typename TileBlockSort::TempStorage>(offset);\n    auto* tile_sort_storage = reinterpret_cast<typename TileBlockSort::TempStorage*>(smem_raw + offset);\n    offset += sizeof(typename TileBlockSort::TempStorage);\n\n    offset = align_up<typename MergeBlockSort::TempStorage>(offset);\n    auto* merge_sort_storage = reinterpret_cast<typename MergeBlockSort::TempStorage*>(smem_raw + offset);\n    offset += sizeof(typename MergeBlockSort::TempStorage);\n\n    // Shared arrays for current block top-k and current tile top-k\n    offset = align_up<float>(offset);\n    float* sh_topk_dist = reinterpret_cast<float*>(smem_raw + offset);\n    offset += KMAX * sizeof(float);\n\n    float* sh_tile_topk_dist = reinterpret_cast<float*>(smem_raw + offset);\n    offset += KMAX * sizeof(float);\n\n    offset = align_up<int>(offset);\n    int* sh_topk_idx = reinterpret_cast<int*>(smem_raw + offset);\n    offset += KMAX * sizeof(int);\n\n    int* sh_tile_topk_idx = reinterpret_cast<int*>(smem_raw + offset);\n    offset += KMAX * sizeof(int);\n\n    // Load query point once into registers\n    const float2 q = query[qid];\n    const float INF = CUDART_INF_F;\n\n    // Local storage for CUB sorts\n    float tile_keys[TILE_ITEMS_PER_THREAD];\n    int   tile_vals[TILE_ITEMS_PER_THREAD];\n\n    float merge_keys[MERGE_ITEMS_PER_THREAD];\n    int   merge_vals[MERGE_ITEMS_PER_THREAD];\n\n    // Iterate over dataset in tiles\n    for (int tile_base = 0, tile_iter = 0; tile_base < data_count; tile_base += TILE_ITEMS, ++tile_iter) {\n        const int tile_count = min(TILE_ITEMS, data_count - tile_base);\n        const int keep_count = min(k, tile_count); // we only need top-k from this tile\n\n        // Load and compute distances for this tile in a striped pattern for perfect coalescing.\n        // Each thread processes TILE_ITEMS_PER_THREAD items.\n        #pragma unroll\n        for (int it = 0; it < TILE_ITEMS_PER_THREAD; ++it) {\n            const int pos_in_tile = it * BLOCK_THREADS + threadIdx.x;\n            const int idx = tile_base + pos_in_tile;\n            if (pos_in_tile < tile_count) {\n                const float2 d = data[idx];\n                const float dist2 = sq_l2_distance_2d(d, q);\n                tile_keys[it] = dist2;\n                tile_vals[it] = idx;\n            } else {\n                // Pad with +inf so padded items sort to the end\n                tile_keys[it] = INF;\n                tile_vals[it] = -1;\n            }\n        }\n\n        // Sort the entire tile across the block by distance (ascending)\n        TileBlockSort(*tile_sort_storage).Sort(tile_keys, tile_vals);\n\n        // After sort, items are in \"striped\" order: global_rank = it * BLOCK_THREADS + threadIdx.x\n        // Extract first keep_count items into the shared tile top-k buffers.\n        #pragma unroll\n        for (int it = 0; it < TILE_ITEMS_PER_THREAD; ++it) {\n            const int global_rank = it * BLOCK_THREADS + threadIdx.x;\n            if (global_rank < keep_count) {\n                sh_tile_topk_dist[global_rank] = tile_keys[it];\n                sh_tile_topk_idx[global_rank]  = tile_vals[it];\n            }\n        }\n\n        // For ranks [keep_count, k) fill padding with +inf to keep fixed-size merge\n        for (int r = threadIdx.x; r < k; r += BLOCK_THREADS) {\n            if (r >= keep_count) {\n                sh_tile_topk_dist[r] = INF;\n                sh_tile_topk_idx[r]  = -1;\n            }\n        }\n        __syncthreads();\n\n        if (tile_iter == 0) {\n            // First tile: initialize block top-k with tile top-k\n            for (int r = threadIdx.x; r < k; r += BLOCK_THREADS) {\n                sh_topk_dist[r] = sh_tile_topk_dist[r];\n                sh_topk_idx[r]  = sh_tile_topk_idx[r];\n            }\n            __syncthreads();\n        } else {\n            // Merge current block top-k and tile top-k (2k items) and keep the best k.\n            #pragma unroll\n            for (int it = 0; it < MERGE_ITEMS_PER_THREAD; ++it) {\n                const int global_rank = it * BLOCK_THREADS + threadIdx.x;\n                if (global_rank < 2 * k) {\n                    if (global_rank < k) {\n                        merge_keys[it] = sh_topk_dist[global_rank];\n                        merge_vals[it] = sh_topk_idx[global_rank];\n                    } else {\n                        const int r2 = global_rank - k;\n                        merge_keys[it] = sh_tile_topk_dist[r2];\n                        merge_vals[it] = sh_tile_topk_idx[r2];\n                    }\n                } else {\n                    merge_keys[it] = INF;\n                    merge_vals[it] = -1;\n                }\n            }\n\n            MergeBlockSort(*merge_sort_storage).Sort(merge_keys, merge_vals);\n\n            // Write back the first k items into block top-k\n            #pragma unroll\n            for (int it = 0; it < MERGE_ITEMS_PER_THREAD; ++it) {\n                const int global_rank = it * BLOCK_THREADS + threadIdx.x;\n                if (global_rank < k) {\n                    sh_topk_dist[global_rank] = merge_keys[it];\n                    sh_topk_idx[global_rank]  = merge_vals[it];\n                }\n            }\n            __syncthreads();\n        }\n    }\n\n    // Store final results for this query in row-major: result[qid * k + j]\n    for (int j = threadIdx.x; j < k; j += BLOCK_THREADS) {\n        const int out_idx = qid * k + j;\n        // Write directly to std::pair in device memory\n        result[out_idx].first  = sh_topk_idx[j];\n        result[out_idx].second = sh_topk_dist[j];\n    }\n}\n\n// Host-side driver\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k)\n{\n    // Configure launch\n    constexpr int BLOCK_THREADS = KNN_BLOCK_THREADS;\n    constexpr int TILE_ITEMS_PER_THREAD = KNN_TILE_ITEMS_PER_THREAD;\n    constexpr int MERGE_ITEMS_PER_THREAD = KNN_MERGE_ITEMS_PER_THREAD;\n    constexpr int KMAX = KNN_KMAX;\n\n    // Sanity caps on k\n    if (k <= 0) return;\n    if (k > KMAX) k = KMAX;\n\n    // Compute dynamic shared memory requirement\n    using TileBlockSort = cub::BlockRadixSort<float, BLOCK_THREADS, TILE_ITEMS_PER_THREAD, int>;\n    using MergeBlockSort = cub::BlockRadixSort<float, BLOCK_THREADS, MERGE_ITEMS_PER_THREAD, int>;\n\n    size_t smem_bytes = 0;\n    auto align_up_host = [](size_t offset, size_t alignment) -> size_t {\n        return (offset + (alignment - 1)) & ~(alignment - 1);\n    };\n\n    smem_bytes = align_up_host(smem_bytes, alignof(typename TileBlockSort::TempStorage));\n    smem_bytes += sizeof(typename TileBlockSort::TempStorage);\n\n    smem_bytes = align_up_host(smem_bytes, alignof(typename MergeBlockSort::TempStorage));\n    smem_bytes += sizeof(typename MergeBlockSort::TempStorage);\n\n    smem_bytes = align_up_host(smem_bytes, alignof(float));\n    smem_bytes += KMAX * sizeof(float); // sh_topk_dist\n    smem_bytes += KMAX * sizeof(float); // sh_tile_topk_dist\n\n    smem_bytes = align_up_host(smem_bytes, alignof(int));\n    smem_bytes += KMAX * sizeof(int);   // sh_topk_idx\n    smem_bytes += KMAX * sizeof(int);   // sh_tile_topk_idx\n\n    // Increase max dynamic shared memory size attribute if needed\n    cudaFuncSetAttribute(knn2d_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, static_cast<int>(smem_bytes));\n\n    dim3 block(BLOCK_THREADS);\n    dim3 grid(query_count);\n\n    // Launch kernel\n    knn2d_kernel<<<grid, block, smem_bytes>>>(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 563, "completion_tokens": 17616, "total_tokens": 18179, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 14784, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db80630819084ede023f28ef3b0", "custom_id": "knn02-01", "response": {"status_code": 200, "request_id": "e47f0b5adf2a8689385cc8bc16d249fb", "body": {"id": "chatcmpl-C4t3ruh5DJwionUuAxAixRuvqIew3", "object": "chat.completion", "created": 1755281003, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cuda.h>\n#include <utility>\n\n// This implementation assigns one warp (32 threads) per query point.\n// Each warp processes the full dataset in tiles cached in shared memory.\n// For each tile, every lane maintains a small per-thread top-L (L = k/32) list\n// of its best candidates in shared memory (descending order: worst at front).\n// Then, the warp performs a multiway merge (via warp shuffles) of the 32 per-lane\n// lists to form the tile's k best candidates and merges them with the current\n// per-warp global top-k (also kept in shared memory).\n// The per-warp global result is maintained in descending order (worst to best).\n// After all tiles are processed, the final per-query k-NN indices/distances are\n// written out in ascending order (best to worst).\n//\n// Notes:\n// - Distances are squared L2 (no sqrt), as requested.\n// - k is a power of two in [32, 1024], so L = k/32 is an integer in [1, 32].\n// - No additional device memory is allocated; only dynamic shared memory is used.\n// - Uses warp shuffles for fast intra-warp communication and __syncthreads for tile barriers.\n// - Tunables: WARPS_PER_BLOCK and TILE_SIZE control occupancy and shared memory usage.\n//   With WARPS_PER_BLOCK=4 and TILE_SIZE=2048, the maximum shared memory per block\n//   is ~114 KB for k=1024, which fits on A100/H100 with opt-in dynamic shared memory.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Tunable parameters: chosen to balance shared memory usage and occupancy on A100/H100.\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 4\n#endif\n\n#ifndef TILE_SIZE\n#define TILE_SIZE 2048\n#endif\n\nstatic __device__ __forceinline__ float sq_l2_dist_2d(const float2 a, const float2 b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    // FMA for better throughput/precision\n    return fmaf(dx, dx, dy * dy);\n}\n\n// Warp-level argmax over 32 lanes.\n// Inputs:\n//   myVal    - candidate value in each lane\n// Outputs (same in all lanes after return):\n//   maxVal   - maximum value across the warp\n//   maxLane  - lane index (0..31) that holds the maximum (lowest lane on tie)\nstatic __device__ __forceinline__ void warp_argmax(float myVal, float &maxVal, int &maxLane) {\n    const unsigned mask = 0xFFFFFFFFu;\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n    float bestVal = myVal;\n    int bestLane = lane;\n#pragma unroll\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        float othVal = __shfl_down_sync(mask, bestVal, offset);\n        int   othLan = __shfl_down_sync(mask, bestLane, offset);\n        if (othVal > bestVal || (othVal == bestVal && othLan < bestLane)) {\n            bestVal = othVal;\n            bestLane = othLan;\n        }\n    }\n    // Broadcast reduction result from lane 0 to all\n    maxVal  = __shfl_sync(mask, bestVal, 0);\n    maxLane = __shfl_sync(mask, bestLane, 0);\n}\n\n// CUDA kernel: one warp per query.\n__global__ void knn2d_kernel(const float2 *__restrict__ query,\n                             int query_count,\n                             const float2 *__restrict__ data,\n                             int data_count,\n                             std::pair<int, float> *__restrict__ result,\n                             int k) {\n    extern __shared__ unsigned char smem_raw[];\n    // Shared tile of data points (float2)\n    float2 *sData = reinterpret_cast<float2*>(smem_raw);\n\n    const int tid   = threadIdx.x;\n    const int lane  = tid & (WARP_SIZE - 1);\n    const int warp  = tid >> 5; // warp id within block [0..WARPS_PER_BLOCK-1]\n    const int warps_per_block = blockDim.x / WARP_SIZE;\n\n    // Per-warp shared memory layout after the tile:\n    // [globalDist k floats][globalIdx k ints][localDist k floats][localIdx k ints][outDist k floats][outIdx k ints]\n    size_t tile_bytes = sizeof(float2) * TILE_SIZE;\n    unsigned char *warp_base = smem_raw + tile_bytes + static_cast<size_t>(warp) * (static_cast<size_t>(k) * (sizeof(float) + sizeof(int)) * 3u);\n    float *gDist = reinterpret_cast<float*>(warp_base);\n    int   *gIdx  = reinterpret_cast<int*>(gDist + k);\n    float *lDist = reinterpret_cast<float*>(gIdx  + k);\n    int   *lIdx  = reinterpret_cast<int*>(lDist + k);\n    float *oDist = reinterpret_cast<float*>(lIdx  + k);\n    int   *oIdx  = reinterpret_cast<int*>(oDist + k);\n\n    // Grid-stride over queries, with one warp per query.\n    int warp_global = blockIdx.x * warps_per_block + warp;\n    int warp_stride = gridDim.x * warps_per_block;\n\n    for (int q = warp_global; q < query_count; q += warp_stride) {\n        // Load query point and broadcast within warp\n        float2 qpt;\n        if (lane == 0) qpt = query[q];\n        qpt.x = __shfl_sync(0xFFFFFFFFu, qpt.x, 0);\n        qpt.y = __shfl_sync(0xFFFFFFFFu, qpt.y, 0);\n\n        // Initialize per-warp global top-k (descending; worst at front)\n        for (int j = lane; j < k; j += WARP_SIZE) {\n            gDist[j] = CUDART_INF_F;\n            gIdx[j]  = -1;\n        }\n        __syncwarp();\n\n        // Iterate over data in tiles cached in shared memory\n        for (int t0 = 0; t0 < data_count; t0 += TILE_SIZE) {\n            int nThisTile = min(TILE_SIZE, data_count - t0);\n\n            // Cooperative load of the tile\n            for (int i = tid; i < nThisTile; i += blockDim.x) {\n                sData[i] = data[t0 + i];\n            }\n            __syncthreads(); // ensure tile is loaded\n\n            // Per-thread local top-L in shared memory (descending; worst at front)\n            const int L = k / WARP_SIZE; // k is guaranteed to be multiple of 32\n            int base = lane * L;\n#pragma unroll\n            for (int s = 0; s < L; ++s) {\n                lDist[base + s] = CUDART_INF_F;\n                lIdx [base + s] = -1;\n            }\n\n            // Each lane processes its strided subset of the tile\n            for (int i = lane; i < nThisTile; i += WARP_SIZE) {\n                float2 p = sData[i];\n                float d = sq_l2_dist_2d(p, qpt);\n                int   idx = t0 + i;\n\n                // Bubble-insert into descending local list\n                float candD = d;\n                int   candI = idx;\n#pragma unroll\n                for (int s = 0; s < L; ++s) {\n                    float prevD = lDist[base + s];\n                    int   prevI = lIdx [base + s];\n                    // If candidate is better (smaller), push it down the list\n                    if (candD < prevD) {\n                        lDist[base + s] = candD;\n                        lIdx [base + s] = candI;\n                        candD = prevD;\n                        candI = prevI;\n                    }\n                }\n            }\n\n            // Multiway merge of 32 local lists (descending) with the existing global top-k (descending).\n            // We stream the tile's k candidates via repeated warp-argmax over the 32 per-lane lists,\n            // including +INF placeholders for exhausted positions to keep list length = L per lane.\n            // Then merge the streamed tile sequence with the global sequence, keeping the k smallest.\n            // We implement this by:\n            // 1) Skipping the largest k elements from the union of both sequences.\n            // 2) Emitting the next k elements (descending) into oDist/oIdx.\n            // Finally, copy oDist/oIdx back to gDist/gIdx.\n\n            // Per-lane pointers into local lists (start at first valid element).\n            // Count valid (finite) entries in each local list; INF means unused.\n            int valid = 0;\n#pragma unroll\n            for (int s = 0; s < L; ++s) {\n                valid += (lDist[base + s] < CUDART_INF_F) ? 1 : 0;\n            }\n            int posLocal = L - valid; // first valid entry (descending list), may be == L if valid == 0\n\n            float curD = (posLocal < L) ? lDist[base + posLocal] : CUDART_INF_F;\n            int   curI = (posLocal < L) ? lIdx [base + posLocal] : -1;\n\n            // Skip the largest k elements from the union of A (gDist) and streamed S (curD across lanes).\n            int aPos = 0; // position in gDist (descending)\n            for (int skip = 0; skip < k; ++skip) {\n                // Compute max over the 32 lanes' current candidates (curD)\n                float sMax; int sLane;\n                warp_argmax(curD, sMax, sLane);\n\n                // Head of A (global) sequence\n                float aHead = (aPos < k) ? gDist[aPos] : -CUDART_INF_F; // if exhausted, treat as -inf so S wins\n\n                // Prefer A on ties to keep deterministic behavior\n                bool takeA = (aHead >= sMax);\n                // Lane 0 decides; broadcast decision and sLane to all lanes\n                unsigned mask = 0xFFFFFFFFu;\n                int takeA_int = __shfl_sync(mask, (int)takeA, 0);\n\n                if (takeA_int) {\n                    // Consume from A\n                    if (lane == 0) {\n                        ++aPos;\n                    }\n                } else {\n                    // Consume from S: the winning lane advances its pointer\n                    if (lane == sLane) {\n                        ++posLocal;\n                        if (posLocal < L) {\n                            curD = lDist[base + posLocal];\n                            curI = lIdx [base + posLocal];\n                        } else {\n                            curD = CUDART_INF_F; // exhausted: act as +inf (worst) placeholder\n                            curI = -1;\n                        }\n                    }\n                }\n            }\n\n            // Now emit the next k elements (descending) into output buffer\n            for (int out = 0; out < k; ++out) {\n                float sMax; int sLane;\n                warp_argmax(curD, sMax, sLane);\n\n                float aHead = (aPos < k) ? gDist[aPos] : -CUDART_INF_F;\n                int   aHeadI = (aPos < k) ? gIdx[aPos] : -1;\n\n                bool takeA = (aHead >= sMax);\n                unsigned mask = 0xFFFFFFFFu;\n                int takeA_int = __shfl_sync(mask, (int)takeA, 0);\n\n                if (lane == 0) {\n                    if (takeA_int) {\n                        oDist[out] = aHead;\n                        oIdx [out] = aHeadI;\n                    } else {\n                        // get winning lane's curI via shuffle\n                        int winIdx = __shfl_sync(mask, curI, sLane);\n                        oDist[out] = sMax;\n                        oIdx [out] = winIdx;\n                    }\n                }\n\n                if (takeA_int) {\n                    if (lane == 0) ++aPos;\n                } else {\n                    if (lane == sLane) {\n                        ++posLocal;\n                        if (posLocal < L) {\n                            curD = lDist[base + posLocal];\n                            curI = lIdx [base + posLocal];\n                        } else {\n                            curD = CUDART_INF_F;\n                            curI = -1;\n                        }\n                    }\n                }\n            }\n\n            // Copy output back to global per-warp buffers\n            for (int j = lane; j < k; j += WARP_SIZE) {\n                gDist[j] = oDist[j];\n                gIdx [j] = oIdx [j];\n            }\n            __syncwarp();\n\n            __syncthreads(); // before loading next tile\n        } // end tile loop\n\n        // Write final results in ascending order (best to worst)\n        // gDist/gIdx are descending; output should be ascending -> reverse order\n        std::pair<int, float> *out = result + static_cast<size_t>(q) * k;\n        for (int j = lane; j < k; j += WARP_SIZE) {\n            int src = k - 1 - j;\n            // Reverse with per-lane stride: compute actual source index for this lane's j\n            // j varies as lane, lane+32, lane+64, ...\n            // For correctness, compute real position as:\n            int jj = j;\n            int src_pos = k - 1 - jj;\n            out[jj].first  = gIdx[src_pos];\n            out[jj].second = gDist[src_pos];\n        }\n        __syncwarp();\n    }\n}\n\n// Host-side launcher.\n// query, data, result are device pointers (cudaMalloc'ed).\n// query_count: number of queries, data_count: number of data points, k: power of two in [32, 1024].\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    const int warps_per_block = WARPS_PER_BLOCK;\n    const int block_threads = warps_per_block * WARP_SIZE;\n\n    // Grid: one warp per query (grid-stride inside kernel handles leftovers)\n    int grid_blocks = (query_count + warps_per_block - 1) / warps_per_block;\n    if (grid_blocks <= 0) grid_blocks = 1;\n\n    // Dynamic shared memory size:\n    // - TILE_SIZE float2's for the data tile\n    // - For each warp:\n    //     6 arrays of length k: gDist, gIdx, lDist, lIdx, oDist, oIdx\n    //     (float/int are 4 bytes each) => 6 * k * 4 bytes = 24*k bytes per warp\n    size_t tile_bytes = sizeof(float2) * TILE_SIZE;\n    size_t per_warp_bytes = static_cast<size_t>(k) * (sizeof(float) + sizeof(int)) * 3u; // 3 pairs of (float/int)\n    size_t shared_bytes = tile_bytes + static_cast<size_t>(warps_per_block) * per_warp_bytes;\n\n    // Opt-in to use large dynamic shared memory if needed\n    cudaFuncSetAttribute(knn2d_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, static_cast<int>(shared_bytes));\n#if defined(cudaFuncAttributePreferredSharedMemoryCarveout)\n    cudaFuncSetAttribute(knn2d_kernel, cudaFuncAttributePreferredSharedMemoryCarveout, 100); // Prefer shared mem\n#endif\n\n    dim3 block(block_threads);\n    dim3 grid(grid_blocks);\n    knn2d_kernel<<<grid, block, shared_bytes>>>(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 641, "completion_tokens": 28311, "total_tokens": 28952, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 24832, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db707808190ad76becdd64b5034", "custom_id": "knn02-02", "response": {"status_code": 200, "request_id": "d1ff33b220883dfc086c1d8abf051d52", "body": {"id": "chatcmpl-C4t7TmKEv9bujpTD0R1aaNBBsDtwB", "object": "chat.completion", "created": 1755281227, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#include <utility>\n\n// This implementation assigns exactly one warp (32 threads) to each query point.\n// Each warp maintains a private max-heap of size k in shared memory to track the current top-k nearest neighbors.\n// The dataset is processed in tiles cached in shared memory for coalesced global loads and re-use within the block.\n// The heap is updated in a warp-synchronous manner: within each group of 32 candidate distances, only those\n// that are smaller than the current heap maximum are considered. Candidates are inserted serially by lane 0,\n// using a standard heap \"sift-down\" in shared memory with O(log k) time per insertion. Other lanes cooperate\n// via warp shuffles and ballots to pick the current best candidate(s) to insert, minimizing the number of heap updates.\n//\n// Notes:\n// - Distances are squared Euclidean (no sqrt).\n// - k is a power of two in [32, 1024]. One warp per query; multiple warps per block.\n// - Initial heap is built from the first k data points, loaded directly from global memory for simplicity.\n// - Remaining points are processed in shared-memory tiles.\n// - At the end, the heap is heap-sorted in-place to ascending order and results are stored to the output array.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Warp full mask for 32-thread warps\n#define FULL_MASK 0xFFFFFFFFu\n\n// Device inline: squared L2 distance between two float2 points\nstatic __device__ __forceinline__ float l2_sq(const float2 a, const float2 b) {\n    const float dx = a.x - b.x;\n    const float dy = a.y - b.y;\n    // Use fused multiply-add for accuracy and throughput: dx*dx + dy*dy\n    return fmaf(dy, dy, dx * dx);\n}\n\n// Heap utilities operating on shared-memory arrays (max-heap, key = distance)\n// The heap is per-warp and is updated only by a single thread (lane 0) at any time.\n\nstatic __device__ __forceinline__ void heap_sift_down(float* __restrict__ hDist,\n                                                      int* __restrict__ hIdx,\n                                                      int heap_size,\n                                                      int start_idx) {\n    int i = start_idx;\n    while (true) {\n        int left = (i << 1) + 1;\n        if (left >= heap_size) break;\n        int right = left + 1;\n        int largest = left;\n        if (right < heap_size && hDist[right] > hDist[left]) {\n            largest = right;\n        }\n        if (hDist[i] < hDist[largest]) {\n            // swap i and largest\n            float td = hDist[i];\n            hDist[i] = hDist[largest];\n            hDist[largest] = td;\n\n            int ti = hIdx[i];\n            hIdx[i] = hIdx[largest];\n            hIdx[largest] = ti;\n\n            i = largest;\n        } else {\n            break;\n        }\n    }\n}\n\nstatic __device__ __forceinline__ void heap_build(float* __restrict__ hDist,\n                                                  int* __restrict__ hIdx,\n                                                  int heap_size) {\n    // Build a max-heap from arbitrary array contents: O(heap_size)\n    for (int i = (heap_size >> 1) - 1; i >= 0; --i) {\n        heap_sift_down(hDist, hIdx, heap_size, i);\n    }\n}\n\nstatic __device__ __forceinline__ void heap_replace_root(float* __restrict__ hDist,\n                                                         int* __restrict__ hIdx,\n                                                         int heap_size,\n                                                         float new_dist,\n                                                         int new_idx) {\n    // Replace root with new value and restore heap property\n    hDist[0] = new_dist;\n    hIdx[0] = new_idx;\n    heap_sift_down(hDist, hIdx, heap_size, 0);\n}\n\nstatic __device__ __forceinline__ void heap_heapsort_ascending(float* __restrict__ hDist,\n                                                               int* __restrict__ hIdx,\n                                                               int heap_size) {\n    // Standard heapsort on a max-heap array: After completion, array is sorted ascending by distance.\n    for (int end = heap_size - 1; end > 0; --end) {\n        // swap root (max) with end\n        float td = hDist[0];\n        hDist[0] = hDist[end];\n        hDist[end] = td;\n\n        int ti = hIdx[0];\n        hIdx[0] = hIdx[end];\n        hIdx[end] = ti;\n\n        // sift-down root, with heap_size reduced by 1\n        int i = 0;\n        int size = end;\n        while (true) {\n            int left = (i << 1) + 1;\n            if (left >= size) break;\n            int right = left + 1;\n            int largest = left;\n            if (right < size && hDist[right] > hDist[left]) largest = right;\n            if (hDist[i] < hDist[largest]) {\n                float td2 = hDist[i];\n                hDist[i] = hDist[largest];\n                hDist[largest] = td2;\n\n                int ti2 = hIdx[i];\n                hIdx[i] = hIdx[largest];\n                hIdx[largest] = ti2;\n\n                i = largest;\n            } else {\n                break;\n            }\n        }\n    }\n}\n\n// Warp reduction to find lane index of minimal value among the 32 candidates.\n// Inactive lanes should pass val = +inf so they never win. The minimal value and its lane index\n// will become available in lane 0 after this function.\nstatic __device__ __forceinline__ void warp_find_min_lane(float& val, int& lane_of_val) {\n    // Track lane indices alongside values to extract the winning lane\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        float other_val = __shfl_down_sync(FULL_MASK, val, offset);\n        int other_lane = __shfl_down_sync(FULL_MASK, lane_of_val, offset);\n        // Choose strictly smaller; tie-breaker: smaller lane id wins for determinism\n        if ((other_val < val) || (other_val == val && other_lane < lane_of_val)) {\n            val = other_val;\n            lane_of_val = other_lane;\n        }\n    }\n}\n\n// Kernel implementing k-NN for 2D points with one warp per query.\n// Shared memory layout per block (dynamic):\n// [0 ... tile_points-1]                        : float2 tile buffer (shared across warps in block)\n// [tile_points ... tile_points + W*K - 1]     : int topIdx for each warp, contiguous segments of length K\n// [next ... next + W*K - 1]                   : float topDist for each warp, contiguous segments of length K\n__global__ void knn2d_warp_kernel(const float2* __restrict__ query,\n                                  int query_count,\n                                  const float2* __restrict__ data,\n                                  int data_count,\n                                  std::pair<int, float>* __restrict__ result,\n                                  int k,\n                                  int tile_points)\n{\n    extern __shared__ unsigned char smem_raw[];\n    float2* const tile = reinterpret_cast<float2*>(smem_raw);\n\n    // Compute number of warps per block and identifiers\n    const int warps_per_block = blockDim.x / WARP_SIZE;\n    const int thread_in_block = threadIdx.x;\n    const int lane = thread_in_block & (WARP_SIZE - 1);\n    const int warp_in_block = thread_in_block >> 5;\n    const int warp_global = warps_per_block * blockIdx.x + warp_in_block;\n\n    if (warp_global >= query_count) {\n        return;\n    }\n\n    // Pointers to per-warp top-k storage in shared memory\n    // Compute base pointers after tile buffer\n    // Align to 4-byte boundary (already aligned due to float2 usage, but be explicit)\n    unsigned char* p = reinterpret_cast<unsigned char*>(tile + tile_points);\n    // Integer indices\n    int* const topIdxBase = reinterpret_cast<int*>(p);\n    // Advance by warps_per_block * k ints\n    float* const topDistBase = reinterpret_cast<float*>(topIdxBase + warps_per_block * k);\n    // Per-warp slices\n    int* const heapIdx = topIdxBase + warp_in_block * k;\n    float* const heapDist = topDistBase + warp_in_block * k;\n\n    // Load query point into registers (all lanes get the same data)\n    const float2 q = query[warp_global];\n\n    // Phase 1: Initialize heap from the first k data points (global memory direct loads).\n    // Each lane processes m = k / 32 elements (k is power of 2, >= 32 => divisible by 32).\n    const int m = k / WARP_SIZE;\n    #pragma unroll\n    for (int i = 0; i < m; ++i) {\n        const int idx = i * WARP_SIZE + lane; // 0..k-1\n        // data_count >= k guaranteed; no bounds check needed\n        const float2 pnt = data[idx];\n        heapIdx[idx] = idx;\n        heapDist[idx] = l2_sq(pnt, q);\n    }\n    __syncwarp(); // ensure initial values are stored before building the heap\n\n    // Build max-heap from initial k distances using lane 0\n    if (lane == 0) {\n        heap_build(heapDist, heapIdx, k);\n    }\n    __syncwarp();\n\n    // Phase 2: Process remaining data in tiles cached in shared memory.\n    // Start from index = k because first k were used for heap initialization.\n    for (int base = k; base < data_count; base += tile_points) {\n        const int count = min(tile_points, data_count - base);\n\n        // Block-wide cooperative load into shared memory\n        for (int t = thread_in_block; t < count; t += blockDim.x) {\n            tile[t] = data[base + t];\n        }\n        __syncthreads();\n\n        // Each warp scans the tile in strides of 32\n        for (int off = 0; off < count; off += WARP_SIZE) {\n            const int idx_in_tile = off + lane;\n            const int gidx = base + idx_in_tile;\n\n            // Compute distance for this candidate; if out of bounds, set to +inf\n            float candDist = CUDART_INF_F;\n            int candIdx = -1;\n            if (idx_in_tile < count) {\n                const float2 pnt = tile[idx_in_tile];\n                candDist = l2_sq(pnt, q);\n                candIdx = gidx;\n            }\n\n            // Snapshot of current heap threshold (max distance in top-k) from lane 0\n            float tau = 0.0f;\n            if (lane == 0) {\n                tau = heapDist[0];\n            }\n            tau = __shfl_sync(FULL_MASK, tau, 0);\n\n            // Mark active if this candidate might improve the heap\n            bool active = (candDist < tau);\n\n            // Process all improving candidates from this warp-group of 32 elements.\n            // At each iteration, the best candidate among active lanes is inserted into the heap by lane 0.\n            while (true) {\n                unsigned int active_mask = __ballot_sync(FULL_MASK, active);\n                if (active_mask == 0u) break;\n\n                // Reduction to find lane with minimal candidate distance\n                float v = active ? candDist : CUDART_INF_F;\n                int min_lane = lane;\n                warp_find_min_lane(v, min_lane);\n\n                // Broadcast the winning lane and distance to lane 0\n                const int winner_lane = __shfl_sync(FULL_MASK, min_lane, 0);\n                const float winner_dist = __shfl_sync(FULL_MASK, v, 0);\n                const int winner_idx = __shfl_sync(FULL_MASK, candIdx, winner_lane);\n\n                // Insert into heap (lane 0 only), then update tau\n                if (lane == 0) {\n                    heap_replace_root(heapDist, heapIdx, k, winner_dist, winner_idx);\n                }\n                // Broadcast updated tau from lane 0 to all lanes\n                float new_tau = 0.0f;\n                if (lane == 0) {\n                    new_tau = heapDist[0];\n                }\n                new_tau = __shfl_sync(FULL_MASK, new_tau, 0);\n\n                // Update active flags: the winner is no longer active; others remain active only if they still beat new_tau\n                active = active && (lane != winner_lane) && (candDist < new_tau);\n            }\n            // Next group of 32 candidates\n        }\n        __syncthreads(); // ensure tile buffer is not read while being overwritten by other blocks\n    }\n\n    // Phase 3: Sort the heap into ascending order and write results\n    if (lane == 0) {\n        heap_heapsort_ascending(heapDist, heapIdx, k);\n    }\n    __syncwarp();\n\n    // Store results to global memory: each lane writes a strided subset for coalescing\n    const int out_base = warp_global * k;\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        result[out_base + i].first = heapIdx[i];\n        result[out_base + i].second = heapDist[i];\n    }\n}\n\n// Host helper to round down to a multiple\nstatic inline int round_down(int x, int multiple) {\n    return (x / multiple) * multiple;\n}\n\n// Host function to choose block size, shared memory sizes, and launch the kernel.\nvoid run_knn(const float2* query, int query_count,\n             const float2* data, int data_count,\n             std::pair<int, float>* result, int k)\n{\n    // Choose warps per block and threads per block. Start with 8 warps (256 threads),\n    // reduce if shared memory budget requires.\n    int warps_per_block = 8;\n    int threads_per_block = warps_per_block * WARP_SIZE;\n\n    // Determine maximum dynamically allocated shared memory per block (opt-in)\n    int device = 0;\n    cudaGetDevice(&device);\n    int max_optin_smem = 0;\n    cudaDeviceGetAttribute(&max_optin_smem, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    if (max_optin_smem == 0) {\n        // Fallback to the default maximum if opt-in not supported\n        cudaDeviceGetAttribute(&max_optin_smem, cudaDevAttrMaxSharedMemoryPerBlock, device);\n    }\n\n    // Compute shared memory needed for per-warp top-k storage\n    // Each entry is an int index and a float distance\n    size_t per_warp_topk_bytes = static_cast<size_t>(k) * (sizeof(int) + sizeof(float));\n    // Adjust warps per block if necessary to leave room for the tile\n    while (warps_per_block > 0) {\n        size_t topk_bytes = per_warp_topk_bytes * static_cast<size_t>(warps_per_block);\n        if (topk_bytes < static_cast<size_t>(max_optin_smem)) break;\n        warps_per_block >>= 1;\n    }\n    if (warps_per_block == 0) {\n        // As a last resort, run with a single warp per block (should not happen with given constraints)\n        warps_per_block = 1;\n    }\n    threads_per_block = warps_per_block * WARP_SIZE;\n\n    // Compute tile size so that total shared memory fits within the opt-in limit.\n    // tile_bytes = max_optin_smem - topk_bytes\n    size_t topk_bytes = per_warp_topk_bytes * static_cast<size_t>(warps_per_block);\n    size_t max_tile_bytes = (max_optin_smem > static_cast<int>(topk_bytes)) ? (max_optin_smem - topk_bytes) : 0;\n\n    // Each tile element is a float2\n    int tile_points = 0;\n    if (max_tile_bytes >= sizeof(float2)) {\n        tile_points = static_cast<int>(max_tile_bytes / sizeof(float2));\n    }\n    // Round tile_points down to a multiple of blockDim (for easy coalesced loads), but keep at least one warp\n    tile_points = round_down(tile_points, threads_per_block);\n    if (tile_points < WARP_SIZE) {\n        // Ensure at least one warp's worth of points in the tile\n        tile_points = WARP_SIZE;\n    }\n    // Also cap tile_points to data_count-k to avoid allocating beyond what's needed (optional)\n    if (tile_points > data_count) tile_points = data_count;\n\n    // Final dynamic shared memory size for the kernel launch\n    size_t dyn_smem_size = static_cast<size_t>(tile_points) * sizeof(float2) + topk_bytes;\n\n    // Set kernel attribute to allow using large dynamic shared memory if needed\n    // Use the max opt-in to be safe across different k\n    cudaFuncSetAttribute(knn2d_warp_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, max_optin_smem);\n\n    // Compute grid size: one warp per query\n    int grid_blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Launch kernel\n    dim3 grid(grid_blocks);\n    dim3 block(threads_per_block);\n    knn2d_warp_kernel<<<grid, block, dyn_smem_size>>>(query, query_count, data, data_count, result, k, tile_points);\n\n    // Optionally, synchronize or check errors here in production code\n    // cudaDeviceSynchronize();\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 641, "completion_tokens": 20971, "total_tokens": 21612, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 17152, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db712008190ad3cd6c0e13b962d", "custom_id": "knn02-03", "response": {"status_code": 200, "request_id": "9ec27147b1c70cd3c86a38bf5b0dfe5a", "body": {"id": "chatcmpl-C4tANS6sRWbt7hvQ08YD60DeCeCmO", "object": "chat.completion", "created": 1755281407, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#include <utility>\n#include <limits>\n\n// This implementation assigns one full warp (32 threads) to process a single query point.\n// Each warp maintains a distributed top-k structure across its 32 lanes.\n// The dataset is processed in shared-memory cached tiles for better memory bandwidth utilization.\n// Within each tile, we further process data in sub-chunks to limit per-thread register pressure.\n// For each sub-chunk, each lane computes distances for a small number (<= 16) of points, sorts them locally,\n// and then the warp performs a multiway merge by repeatedly inserting the current best candidate into the\n// global top-k (distributed across lanes), replacing the current worst element. Warp-wide argmin/argmax\n// reductions via shuffles are used to coordinate insertions. The intermediate top-k per query is stored\n// in shared memory, so no additional global memory allocations are required.\n// After processing all tiles, the warp performs a distributed bitonic sort across its k results to produce\n// the final ascending order required by the API, and then writes the (index, distance) pairs to the result.\n\nnamespace {\n\n// Hardware-tuned constants. Adjust with care; they balance occupancy, shared memory use, and arithmetic.\nconstexpr int WARP_SIZE = 32;\nconstexpr int WARPS_PER_BLOCK = 8;       // 8 warps -> 256 threads per block\nconstexpr int TILE_SIZE = 4096;          // Number of data points per shared-memory tile (float2 => 32KB)\nconstexpr int MAX_CHUNK = 512;           // Sub-chunk size per warp per merge step (must be multiple of 32)\n// MAX_CHUNK/32 = 16 local candidates per lane; keeps register usage modest.\n\n// Warp utilities\n__device__ __forceinline__ int lane_id() {\n    return threadIdx.x & (WARP_SIZE - 1);\n}\n\n__device__ __forceinline__ int warp_id_in_block() {\n    return threadIdx.x >> 5;\n}\n\n__device__ __forceinline__ int warps_per_block() {\n    return blockDim.x >> 5;\n}\n\n// Compute squared Euclidean distance between a query point q and data point p.\n__device__ __forceinline__ float l2_sq(const float2& q, const float2& p) {\n    float dx = q.x - p.x;\n    float dy = q.y - p.y;\n    // FMA for precision/perf: dx*dx + dy*dy\n    return fmaf(dx, dx, dy * dy);\n}\n\n// Warp-wide argmax reduction: returns (maxVal, maxLane) broadcast to all lanes in the warp.\n__device__ __forceinline__ void warp_argmax(float v, int lane, float& outVal, int& outLane, unsigned mask = 0xffffffffu) {\n    float bestVal = v;\n    int bestLane = lane;\n    // Tree reduction using shuffles\n    for (int offset = WARP_SIZE >> 1; offset > 0; offset >>= 1) {\n        float othVal = __shfl_down_sync(mask, bestVal, offset);\n        int othLane = __shfl_down_sync(mask, bestLane, offset);\n        if (othVal > bestVal) {\n            bestVal = othVal;\n            bestLane = othLane;\n        }\n    }\n    // Broadcast winner to all lanes\n    outVal = __shfl_sync(mask, bestVal, 0);\n    outLane = __shfl_sync(mask, bestLane, 0);\n}\n\n// Warp-wide argmin reduction: returns (minVal, minLane) broadcast to all lanes in the warp.\n__device__ __forceinline__ void warp_argmin(float v, int lane, float& outVal, int& outLane, unsigned mask = 0xffffffffu) {\n    float bestVal = v;\n    int bestLane = lane;\n    // Tree reduction using shuffles\n    for (int offset = WARP_SIZE >> 1; offset > 0; offset >>= 1) {\n        float othVal = __shfl_down_sync(mask, bestVal, offset);\n        int othLane = __shfl_down_sync(mask, bestLane, offset);\n        if (othVal < bestVal) {\n            bestVal = othVal;\n            bestLane = othLane;\n        }\n    }\n    // Broadcast winner to all lanes\n    outVal = __shfl_sync(mask, bestVal, 0);\n    outLane = __shfl_sync(mask, bestLane, 0);\n}\n\n// Simple insertion sort for very small arrays in registers (ascending by value).\ntemplate <int N>\n__device__ __forceinline__ void insertion_sort_asc(float (&vals)[N], int (&idxs)[N], int count) {\n    #pragma unroll\n    for (int i = 1; i < N; ++i) {\n        if (i >= count) break;\n        float keyV = vals[i];\n        int keyI = idxs[i];\n        int j = i - 1;\n        while (j >= 0 && vals[j] > keyV) {\n            vals[j + 1] = vals[j];\n            idxs[j + 1] = idxs[j];\n            --j;\n        }\n        vals[j + 1] = keyV;\n        idxs[j + 1] = keyI;\n    }\n}\n\n// Swap utility\n__device__ __forceinline__ void swap_float(float& a, float& b) { float t = a; a = b; b = t; }\n__device__ __forceinline__ void swap_int(int& a, int& b) { int t = a; a = b; b = t; }\n\n} // namespace\n\n// Kernel implementing warp-per-query KNN with shared-memory tiling and warp-cooperative top-k maintenance.\n__global__ void knn_warp_kernel(const float2* __restrict__ query,\n                                int query_count,\n                                const float2* __restrict__ data,\n                                int data_count,\n                                std::pair<int, float>* __restrict__ result,\n                                int k) {\n    // Map threads to warps and queries\n    const int lane = lane_id();\n    const int wid = warp_id_in_block();\n    const int wpb = warps_per_block();\n    const int warp_global = blockIdx.x * wpb + wid;\n    if (warp_global >= query_count) return;\n\n    // Shared memory layout:\n    // [0, TILE_SIZE) float2 sData\n    // then W warps' A-dist arrays (float): size W*k\n    // then W warps' A-idx arrays (int): size W*k\n    extern __shared__ unsigned char smem_raw[];\n    float2* sData = reinterpret_cast<float2*>(smem_raw);\n    size_t offset = static_cast<size_t>(TILE_SIZE) * sizeof(float2);\n\n    float* smem_A_dist_all = reinterpret_cast<float*>(smem_raw + offset);\n    offset += static_cast<size_t>(wpb) * static_cast<size_t>(k) * sizeof(float);\n\n    int* smem_A_idx_all = reinterpret_cast<int*>(smem_raw + offset);\n    // No further shared memory allocations.\n\n    // Per-warp A arrays in shared memory\n    float* A_dist = smem_A_dist_all + static_cast<size_t>(wid) * static_cast<size_t>(k);\n    int*   A_idx  = smem_A_idx_all  + static_cast<size_t>(wid) * static_cast<size_t>(k);\n\n    // Query point for this warp\n    const float2 q = query[warp_global];\n\n    // Per-thread chunk size parameters\n    const int m = k / WARP_SIZE;                    // elements per lane for A\n    const int chunk = (k < MAX_CHUNK ? k : MAX_CHUNK); // choose sub-chunk size\n    const int bmax = MAX_CHUNK / WARP_SIZE;         // max local candidates per lane (compile-time 16)\n    int bslots = chunk / WARP_SIZE;                 // runtime slots per lane for sub-chunk\n\n    // Initialize A with +inf distances and invalid indices (-1), keep local descending order\n    // We place the local worst at position 0 for each lane's block of size m.\n    for (int i = 0; i < m; ++i) {\n        const int g = lane * m + i; // global index into A arrays for this lane\n        A_dist[g] = CUDART_INF_F;   // Initially worst at head (position 0)\n        A_idx[g] = -1;\n    }\n    __syncwarp();\n\n    // Process data in shared-memory tiles\n    for (int tile_start = 0; tile_start < data_count; tile_start += TILE_SIZE) {\n        const int tile_count = min(TILE_SIZE, data_count - tile_start);\n\n        // Load tile into shared memory cooperatively\n        for (int t = threadIdx.x; t < tile_count; t += blockDim.x) {\n            sData[t] = data[tile_start + t];\n        }\n        __syncthreads();\n\n        // Within this tile, process in sub-chunks sized 'chunk'\n        for (int sub = 0; sub < tile_count; sub += chunk) {\n            const int sub_count = min(chunk, tile_count - sub);\n            // Number of local candidates per lane for this sub-chunk\n            bslots = (sub_count + WARP_SIZE - 1) / WARP_SIZE; // ceil\n            if (bslots > bmax) bslots = bmax;\n\n            // Local candidate buffers in registers (ascending after sort)\n            float bvals[bmax];\n            int   bidxs[bmax];\n\n            // Fill local candidates with distances and global indices\n            #pragma unroll\n            for (int i = 0; i < bmax; ++i) {\n                int idx_in_sub = i * WARP_SIZE + lane; // sub-chunk-local index processed by this lane\n                int sidx = sub + idx_in_sub;\n                if (i < bslots && sidx < tile_count) {\n                    // Valid candidate\n                    float2 p = sData[sidx];\n                    float dist = l2_sq(q, p);\n                    bvals[i] = dist;\n                    bidxs[i] = tile_start + sidx;\n                } else {\n                    // Pad with +inf (ignored)\n                    bvals[i] = CUDART_INF_F;\n                    bidxs[i] = -1;\n                }\n            }\n\n            // Sort local candidates ascending so bvals[head] is current best within lane\n            insertion_sort_asc<bmax>(bvals, bidxs, bslots);\n\n            // Initialize per-lane pointer into B list\n            int phead = 0;\n\n            // Prepare initial worst (tau) across A: read this lane's local worst at position 0\n            float laneWorst = A_dist[lane * m + 0];\n            float tau; int worstLane;\n            warp_argmax(laneWorst, lane, tau, worstLane);\n\n            // Warp-cooperative multiway merge: repeatedly insert best among lanes if it improves tau\n            while (true) {\n                // Each lane contributes its current best candidate (or +inf if exhausted)\n                float candVal = (phead < bslots ? bvals[phead] : CUDART_INF_F);\n                int   candIdx = (phead < bslots ? bidxs[phead] : -1);\n\n                float bestVal; int bestLane;\n                warp_argmin(candVal, lane, bestVal, bestLane);\n                // Broadcast best candidate's index from bestLane\n                int bestIdx = __shfl_sync(0xffffffffu, candIdx, bestLane);\n\n                // If the best available candidate does not improve over current worst, stop\n                if (!(bestVal < tau)) break;\n\n                // Replace current worst in A with best candidate and restore local descending order\n                if (lane == worstLane) {\n                    // Replace head (position 0) and bubble down to maintain descending order\n                    int base = lane * m;\n                    A_dist[base + 0] = bestVal;\n                    A_idx [base + 0] = bestIdx;\n                    // Bubble down until order A[0] >= A[1] >= ... holds for this lane's segment\n                    for (int pos = 0; pos + 1 < m; ++pos) {\n                        // If next is larger than current, swap\n                        if (A_dist[base + pos] < A_dist[base + pos + 1]) {\n                            swap_float(A_dist[base + pos], A_dist[base + pos + 1]);\n                            swap_int  (A_idx [base + pos], A_idx [base + pos + 1]);\n                        } else {\n                            break;\n                        }\n                    }\n                }\n                // Advance head in best lane's local candidate list\n                if (lane == bestLane) {\n                    ++phead;\n                }\n\n                // Ensure A updates are visible before next tau computation\n                __syncwarp();\n\n                // Recompute tau = global worst across A for next iteration\n                laneWorst = A_dist[lane * m + 0];\n                warp_argmax(laneWorst, lane, tau, worstLane);\n            }\n\n            // End sub-chunk loop for this tile segment\n            __syncwarp();\n        }\n\n        __syncthreads(); // Ensure all warps done reading this tile before loading the next\n    }\n\n    // Final sorting of A across the warp: produce ascending order required by API\n    // We perform an in-place bitonic sort over k elements stored in shared memory at A_dist/A_idx.\n    // Global index mapping for element g in [0, k): belongs to lane = g / m, slot = g % m.\n    // Each thread iterates over its local m elements and participates in pairwise compare-exchange using shared memory.\n    // After each compare-exchange stage, we synchronize the warp to ensure memory coherence.\n\n    // Bitonic sort network (ascending)\n    for (int size = 2; size <= k; size <<= 1) {\n        // Stride starts at size/2 and halves down to 1\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            // Each lane handles its m elements\n            for (int s = 0; s < m; ++s) {\n                int g = lane * m + s;           // global index in [0, k)\n                int partner = g ^ stride;       // partner index for compare-exchange\n                if (partner > g && partner < k) {\n                    // Determine comparison direction (up = ascending; down = descending) for this pair\n                    bool up = ((g & size) == 0);\n                    float v1 = A_dist[g];\n                    float v2 = A_dist[partner];\n                    int i1 = A_idx[g];\n                    int i2 = A_idx[partner];\n                    // For ascending sort, swap if v1 > v2 (and vice versa for descending)\n                    bool swapNeeded = up ? (v1 > v2) : (v1 < v2);\n                    if (swapNeeded) {\n                        A_dist[g] = v2; A_idx[g] = i2;\n                        A_dist[partner] = v1; A_idx[partner] = i1;\n                    }\n                }\n            }\n            __syncwarp();\n        }\n    }\n\n    // Write results to global memory: ascending by distance, result[q*k + j] = (index, distance)\n    const int outBase = warp_global * k;\n    for (int s = lane; s < k; s += WARP_SIZE) {\n        result[outBase + s].first  = A_idx[s];\n        result[outBase + s].second = A_dist[s];\n    }\n}\n\n// Host interface\nvoid run_knn(const float2* query,\n             int query_count,\n             const float2* data,\n             int data_count,\n             std::pair<int, float>* result,\n             int k) {\n    // Basic validation (assumed valid per problem statement); k is power of two between 32 and 1024 inclusive\n    const int wpb = WARPS_PER_BLOCK;\n    const int threads_per_block = wpb * WARP_SIZE;\n    const int blocks = (query_count + wpb - 1) / wpb;\n\n    // Dynamic shared memory size per block:\n    // TILE_SIZE * sizeof(float2) for data tile +\n    // WARPS_PER_BLOCK * k * (sizeof(float) + sizeof(int)) for per-warp A arrays\n    size_t smem_bytes =\n        static_cast<size_t>(TILE_SIZE) * sizeof(float2) +\n        static_cast<size_t>(WARPS_PER_BLOCK) * static_cast<size_t>(k) * (sizeof(float) + sizeof(int));\n\n    knn_warp_kernel<<<blocks, threads_per_block, smem_bytes>>>(query, query_count, data, data_count, result, k);\n    // The caller is responsible for CUDA error checking and stream synchronization if desired.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 641, "completion_tokens": 22744, "total_tokens": 23385, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 19136, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db717e08190be94648c924cf286", "custom_id": "knn02-04", "response": {"status_code": 200, "request_id": "8741a7df0f9fb5734cadc89da979a45e", "body": {"id": "chatcmpl-C4tD8CKTS9oKn26mTkFxKz9GSe9mr", "object": "chat.completion", "created": 1755281578, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// This implementation computes k-nearest neighbors (k-NN) for 2D points using a warp-per-query strategy.\n// Key design choices:\n// - One warp (32 threads) computes all k-NN for one query. We organize blocks with multiple warps.\n// - The dataset is processed in tiles that are cached in shared memory. We set the tile size equal to k.\n//   This ensures that the union of lane-processed elements equals exactly the tile, which makes merging exact and simple.\n// - For each warp (i.e., for each query), we maintain two private shared-memory buffers of size k:\n//   one for the \"current top-k\" (sorted ascending by distance) and one for the \"current tile candidates\" (to be sorted).\n//   After each tile, we sort the candidates with a bitonic sort (in-place in shared memory) and merge the two\n//   sorted arrays (top-k and candidates) to produce the new top-k, using a merge-path based parallel merge.\n//   To avoid needing a third buffer, each lane first copies its merge segment into registers (local arrays),\n//   then writes the merged outputs back to the top-k buffer. This avoids aliasing hazards.\n// - The requested k is a power-of-two between 32 and 1024 inclusive, and data_count >= k.\n// - The final result for each query is k pairs (index, squared distance), sorted in ascending order of distance.\n//\n// Performance considerations:\n// - The per-tile sort and merge are cooperative across the 32 threads of the warp.\n// - Using tile size equal to k keeps shared memory requirements predictable and modest, and guarantees correctness\n//   of merges without losing candidates.\n// - Block-shared tiles of data are reused by all warps in the block to reduce global memory traffic.\n// - This kernel uses only dynamic shared memory and no additional global allocations.\n//\n// Shared memory usage per block (for WARPS_PER_BLOCK warps):\n//   - Data tile: k * sizeof(float2) bytes.\n//   - Per-warp buffers: two sets (top-k and candidates) of (k floats + k ints) each.\n//     That is 2 * (k * 4 + k * 4) = 16k bytes per warp. For 4 warps: 64k.\n//   - Total per block: 8k + 16k * WARPS_PER_BLOCK bytes. For k=1024 and 4 warps: 8KB + 64KB = 72KB.\n//\n// Notes:\n// - Distances are squared L2, no sqrt, tie-breaking is arbitrary.\n// - The bitonic sort is implemented cooperatively by 32 threads over up to 1024 elements in shared memory.\n// - The merge of two sorted arrays uses the merge-path partitioning for parallelism.\n// - We assume std::pair<int, float> is allocated in device memory and accessible for device writes.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Compute squared Euclidean distance between two float2 points.\nstatic __device__ __forceinline__ float squared_distance(const float2 a, const float2 b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    return dx * dx + dy * dy;\n}\n\n// Cooperative in-place bitonic sort for pairs (distance, index) stored in shared memory arrays.\n// - dist: pointer to distances in shared memory, length 'len' (power of two).\n// - idx:  pointer to indices in shared memory, length 'len'.\n// - len:  number of elements to sort (power of two, 32 <= len <= 1024).\n// Sorting order: ascending by distance.\nstatic __device__ __forceinline__ void warp_bitonic_sort_pairs(float* dist, int* idx, int len) {\n    // Each compare-exchange step processes pairs (i, ixj) with ixj = i ^ j.\n    // We loop i across [0, len) with stride WARP_SIZE so 32 threads cooperate on len elements.\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n\n    // Bitonic sort network\n    for (int k = 2; k <= len; k <<= 1) {\n        for (int j = k >> 1; j > 0; j >>= 1) {\n            for (int i = lane; i < len; i += WARP_SIZE) {\n                int ixj = i ^ j;\n                if (ixj > i) {\n                    bool ascending = ((i & k) == 0);\n                    float ai = dist[i];\n                    float aj = dist[ixj];\n                    int aidx = idx[i];\n                    int ajdx = idx[ixj];\n                    // If ascending, ensure dist[i] <= dist[ixj]; if descending, ensure dist[i] >= dist[ixj].\n                    bool do_swap = (ai > aj) == ascending;\n                    if (do_swap) {\n                        dist[i]  = aj;\n                        idx[i]   = ajdx;\n                        dist[ixj]= ai;\n                        idx[ixj] = aidx;\n                    }\n                }\n            }\n            __syncwarp(); // Synchronize within warp to ensure all compare-swaps at this 'j' are visible before next stage.\n        }\n    }\n}\n\n// Merge-path binary search to locate partition on diagonal 'diag' for merging sorted arrays A and B.\n// Returns the number of elements taken from A (i), where j = diag - i are taken from B.\n// Preconditions:\n// - Arrays A[0..lenA) and B[0..lenB) are sorted ascending.\n// - 0 <= diag <= lenA + lenB.\n// - Works with sentinels: we use -INF and +INF for bounds.\nstatic __device__ __forceinline__ int merge_path_search(const float* __restrict__ A, int lenA,\n                                                        const float* __restrict__ B, int lenB,\n                                                        int diag)\n{\n    int low  = max(0, diag - lenB);\n    int high = min(diag, lenA);\n    while (low <= high) {\n        int i = (low + high) >> 1;\n        int j = diag - i;\n\n        float Ai_1 = (i > 0)     ? A[i - 1] : -CUDART_INF_F;\n        float Bj_1 = (j > 0)     ? B[j - 1] : -CUDART_INF_F;\n        float Ai   = (i < lenA)  ? A[i]     :  CUDART_INF_F;\n        float Bj   = (j < lenB)  ? B[j]     :  CUDART_INF_F;\n\n        // If A[i-1] > B[j], then i is too big.\n        if (Ai_1 > Bj) {\n            high = i - 1;\n        }\n        // Else if B[j-1] > A[i], then i is too small.\n        else if (Bj_1 > Ai) {\n            low = i + 1;\n        }\n        // Else we found valid partition.\n        else {\n            return i;\n        }\n    }\n    return low;\n}\n\n// Cooperative merge of the first 'k' elements from two sorted arrays A and B into Out.\n// Partition the first k outputs into WARP_SIZE segments; each lane merges its segment.\n// Arrays are in shared memory; to avoid read-write aliasing, each lane first copies its needed\n// input segment into registers, then writes the merged segment back to Out.\n// - A, idxA: sorted ascending by A (length lenA = k).\n// - B, idxB: sorted ascending by B (length lenB = k; entries beyond valid tile are +INF).\n// - Out, idxOut: output arrays (length k), may alias A if inputs are fully read to registers first.\n// - k: number of outputs to produce (multiple of 32).\nstatic __device__ __forceinline__ void warp_merge_first_k(const float* __restrict__ A, const int* __restrict__ idxA,\n                                                          const float* __restrict__ B, const int* __restrict__ idxB,\n                                                          float* __restrict__ Out, int* __restrict__ idxOut,\n                                                          int k)\n{\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n    const int seg  = k >> 5; // k / 32, guaranteed integer since k is power-of-two >= 32.\n\n    const int diag0 = lane * seg;\n    const int diag1 = diag0 + seg;\n\n    // Find start and end partitions for this lane's output segment [diag0, diag1)\n    int i0 = merge_path_search(A, k, B, k, diag0);\n    int j0 = diag0 - i0;\n    int i1 = merge_path_search(A, k, B, k, diag1);\n    int j1 = diag1 - i1;\n\n    const int cntA = i1 - i0;\n    const int cntB = j1 - j0;\n\n    // Copy needed input ranges to registers to avoid aliasing with output writes.\n    float regA[32]; int regAi[32];\n    float regB[32]; int regBi[32];\n\n    for (int u = 0; u < cntA; ++u) {\n        regA[u]  = A[i0 + u];\n        regAi[u] = idxA[i0 + u];\n    }\n    for (int v = 0; v < cntB; ++v) {\n        regB[v]  = B[j0 + v];\n        regBi[v] = idxB[j0 + v];\n    }\n\n    __syncwarp(); // Ensure all lanes have finished reading inputs before any lane writes to Out.\n\n    // Merge to produce seg outputs for this lane.\n    int a = 0, b = 0;\n    int outPos = diag0;\n    while (outPos < diag1) {\n        float va = (a < cntA) ? regA[a] : CUDART_INF_F;\n        float vb = (b < cntB) ? regB[b] : CUDART_INF_F;\n        if (va <= vb) {\n            Out[outPos]   = va;\n            idxOut[outPos]= regAi[a];\n            ++a;\n        } else {\n            Out[outPos]   = vb;\n            idxOut[outPos]= regBi[b];\n            ++b;\n        }\n        ++outPos;\n    }\n\n    __syncwarp(); // Make sure all outputs are visible before next tile iteration.\n}\n\n// Kernel: one warp processes one query. All warps in a block share the same data tile cached in shared memory.\nstatic __global__ void knn_kernel(const float2* __restrict__ query, int query_count,\n                                  const float2* __restrict__ data,  int data_count,\n                                  std::pair<int, float>* __restrict__ result, int k)\n{\n    // Thread/wrap identification\n    const int lane          = threadIdx.x & (WARP_SIZE - 1);\n    const int warp_in_block = threadIdx.x >> 5;\n    const int warps_per_blk = blockDim.x >> 5;\n    const int warp_global   = blockIdx.x * warps_per_blk + warp_in_block;\n    const bool warp_active  = (warp_global < query_count);\n\n    // Dynamic shared memory layout:\n    // [0] tile points (k float2s)\n    // Then per-warp regions, each of size:\n    //   topk_dist[k], topk_idx[k], cand_dist[k], cand_idx[k]\n    extern __shared__ unsigned char smem_raw[];\n    unsigned char* smem_ptr = smem_raw;\n\n    // Data tile cached in shared memory\n    float2* smem_tile = reinterpret_cast<float2*>(smem_ptr);\n    smem_ptr += sizeof(float2) * k;\n\n    // Per-warp top-k and candidate buffers\n    float* smem_topk_dist = reinterpret_cast<float*>(smem_ptr);\n    smem_ptr += sizeof(float) * (size_t)warps_per_blk * (size_t)k;\n\n    int* smem_topk_idx = reinterpret_cast<int*>(smem_ptr);\n    smem_ptr += sizeof(int) * (size_t)warps_per_blk * (size_t)k;\n\n    float* smem_cand_dist = reinterpret_cast<float*>(smem_ptr);\n    smem_ptr += sizeof(float) * (size_t)warps_per_blk * (size_t)k;\n\n    int* smem_cand_idx = reinterpret_cast<int*>(smem_ptr);\n    // smem_ptr += sizeof(int) * (size_t)warps_per_blk * (size_t)k; // not needed further\n\n    // Slice per-warp regions\n    float* myTopDist = smem_topk_dist + warp_in_block * k;\n    int*   myTopIdx  = smem_topk_idx  + warp_in_block * k;\n    float* myCandDist= smem_cand_dist + warp_in_block * k;\n    int*   myCandIdx = smem_cand_idx  + warp_in_block * k;\n\n    // Prepare query point in registers, broadcasted across warp.\n    float2 q;\n    if (lane == 0 && warp_active) {\n        q = query[warp_global];\n    }\n    // Broadcast q to all lanes of the warp; inactive warp broadcasts undefined but won't be used.\n    q.x = __shfl_sync(0xFFFFFFFFu, q.x, 0);\n    q.y = __shfl_sync(0xFFFFFFFFu, q.y, 0);\n\n    // Initialize top-k buffers: distances to +INF and indices to -1 (cooperative across lanes).\n    // Even if warp is inactive, we can harmlessly initialize its private region.\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        myTopDist[i] = CUDART_INF_F;\n        myTopIdx[i]  = -1;\n    }\n\n    // Process the dataset in tiles of size k.\n    for (int tile_start = 0; tile_start < data_count; tile_start += k) {\n        const int tile_count = min(k, data_count - tile_start);\n\n        // Block-cooperative load of data tile into shared memory.\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            smem_tile[i] = data[tile_start + i];\n        }\n        __syncthreads(); // Ensure tile fully loaded before warps read.\n\n        if (warp_active) {\n            // Each warp computes squared distances for the tile and stores them in its candidate buffer.\n            // Entries beyond actual tile_count are filled with +INF to simplify sorting/merging.\n            for (int t = lane; t < tile_count; t += WARP_SIZE) {\n                const float2 p = smem_tile[t];\n                const float d = squared_distance(p, q);\n                myCandDist[t] = d;\n                myCandIdx[t]  = tile_start + t;\n            }\n            for (int t = lane + tile_count; t < k; t += WARP_SIZE) {\n                myCandDist[t] = CUDART_INF_F;\n                myCandIdx[t]  = -1;\n            }\n            __syncwarp(); // Make sure candidate buffer is fully populated.\n\n            // Sort candidate distances (and indices) ascending.\n            warp_bitonic_sort_pairs(myCandDist, myCandIdx, k);\n\n            // Merge the first k elements from (myTopDist,myTopIdx) and (myCandDist,myCandIdx) into myTop*.\n            // Note: warp_merge_first_k reads inputs into registers first, then writes to myTop*, avoiding aliasing hazards.\n            warp_merge_first_k(myTopDist, myTopIdx, myCandDist, myCandIdx, myTopDist, myTopIdx, k);\n        }\n\n        __syncthreads(); // Ensure all warps are done using the tile before it is overwritten.\n    }\n\n    // Write results to global memory: k nearest neighbors for this query, sorted ascending.\n    if (warp_active) {\n        for (int j = lane; j < k; j += WARP_SIZE) {\n            const int out_idx = warp_global * k + j;\n            result[out_idx].first  = myTopIdx[j];\n            result[out_idx].second = myTopDist[j];\n        }\n    }\n}\n\n// Host launcher. Chooses launch parameters, configures dynamic shared memory, and launches the kernel.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data,  int data_count,\n             std::pair<int, float> *result, int k)\n{\n    // Hyperparameters: 4 warps per block (128 threads). This balances shared memory usage and occupancy.\n    const int WARPS_PER_BLOCK   = 4;\n    const int THREADS_PER_BLOCK = WARPS_PER_BLOCK * WARP_SIZE;\n\n    // Grid: one warp per query.\n    const int num_blocks = (query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK;\n\n    // Dynamic shared memory size per block:\n    //   tile: k * sizeof(float2)\n    //   per-warp: two sets (topk and candidates) of (k floats + k ints)\n    // total = 8*k + WARPS_PER_BLOCK * ( (k*4 + k*4) * 2 ) = 8k + 16*k*WARPS_PER_BLOCK\n    size_t smem_bytes = (size_t)k * sizeof(float2)\n                      + (size_t)WARPS_PER_BLOCK * ( (size_t)k * sizeof(float) + (size_t)k * sizeof(int) ) * 2;\n\n    // Optionally opt-in to larger dynamic shared memory if necessary.\n    cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)smem_bytes);\n\n    // Launch kernel\n    knn_kernel<<<num_blocks, THREADS_PER_BLOCK, smem_bytes>>>(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 641, "completion_tokens": 23860, "total_tokens": 24501, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 20032, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db707688190a5d7df46272ddc18", "custom_id": "knn02-05", "response": {"status_code": 200, "request_id": "84b99c6094e2cbfc90e56046d866b0a2", "body": {"id": "chatcmpl-C4tFpPMOzYHOGDwyl5nPXEdl32ebY", "object": "chat.completion", "created": 1755281745, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#include <utility>\n#include <limits>\n\n// This implementation assigns exactly one warp (32 threads) to each query point.\n// Each warp maintains a per-query global top-k (sorted ascending by distance)\n// entirely in shared memory. The input data points are processed in large tiles\n// that are cached in shared memory. For each tile, the warp iterates through the\n// points in chunks of 32: every thread computes the distance between the query\n// and one data point, the warp cooperatively sorts these 32 distances via a\n// warp-level bitonic sort, and then merges the sorted 32-candidate list into\n// the warp's top-k list using a parallel \"merge path\" algorithm. The merge\n// produces the updated top-k in shared memory. This approach guarantees exact\n// k-NN results and uses no device-side dynamic allocations beyond shared memory.\n//\n// Key characteristics and optimizations:\n// - Warp-per-query to fully utilize warp-level collectives (shuffle, sync).\n// - Data processed in shared-memory tiles to reduce global memory traffic.\n// - Warp-level bitonic sort turns 32 per-lane candidates into a sorted sequence.\n// - Parallel merge-path merges the 32-sorted candidates into a per-warp\n//   top-k array in O(k) work per 32 candidates, split across threads.\n// - The per-warp top-k arrays are kept sorted (ascending distances), ensuring\n//   the final writeback is in the required order.\n//\n// Assumptions:\n// - k is a power of two in [32, 1024]; query_count and data_count are valid.\n// - data_count >= k.\n// - Pointers query, data, result refer to device memory allocated with cudaMalloc.\n// - Target is a modern datacenter GPU (e.g., A100/H100), with large shared memory.\n//\n// Tunables chosen for a good trade-off on A100/H100:\n// - WARPS_PER_BLOCK = 4 (128 threads per block).\n// - DATA_TILE_POINTS = 8192 (64 KB for tile cache).\n// These choices keep shared memory usage under ~132 KB per block for the worst\n// case k=1024, allowing at least one resident block per SM and good throughput.\n\n#ifndef KNN_WARPS_PER_BLOCK\n#define KNN_WARPS_PER_BLOCK 4\n#endif\n\n#ifndef DATA_TILE_POINTS\n#define DATA_TILE_POINTS 8192\n#endif\n\n// Full warp mask for sync/shuffle\n#ifndef FULL_MASK\n#define FULL_MASK 0xFFFFFFFFu\n#endif\n\n// Lane ID within the warp\n__device__ __forceinline__ int lane_id() {\n    return threadIdx.x & 31;\n}\n\n// Warp ID within the block\n__device__ __forceinline__ int warp_id() {\n    return threadIdx.x >> 5;\n}\n\n// Warp-level bitonic sort of 32 (distance, index) pairs in ascending order by distance.\n// After the call, lane i holds the i-th smallest pair among the 32 inputs.\n__device__ __forceinline__ void warp_bitonic_sort_32_asc(float &key, int &val) {\n    const unsigned mask = FULL_MASK;\n    int lane = lane_id();\n    // Standard XOR-based bitonic sort network for 32 elements (warp size).\n    for (int k = 2; k <= 32; k <<= 1) {\n        for (int j = k >> 1; j > 0; j >>= 1) {\n            float other_key = __shfl_xor_sync(mask, key, j);\n            int   other_val = __shfl_xor_sync(mask, val, j);\n            bool up = ((lane & k) == 0);\n            // Swap to enforce bitonic sorting in 'up' direction.\n            if ((key > other_key) == up) {\n                float tmpk = key; key = other_key; other_key = tmpk;\n                int tmpv = val;  val = other_val;  other_val = tmpv;\n            }\n        }\n    }\n}\n\n// Merge-path partition search: find how many elements to take from A for a given 'diag'\n// (number of elements to output in total from A and B).\n// A and B must be sorted ascending by distance.\n// Returns ai such that ai in [max(0, diag - Blen), min(diag, Alen)] and\n// the merge prefix (of length 'diag') consists of ai elements from A and (diag - ai) from B.\n__device__ __forceinline__ int merge_path_search(const float* __restrict__ A, int Alen,\n                                                 const float* __restrict__ B, int Blen,\n                                                 int diag) {\n    int a_low = max(0, diag - Blen);\n    int a_high = min(diag, Alen);\n    // Binary search to find the split point\n    while (a_low < a_high) {\n        int a_mid = (a_low + a_high) >> 1;\n        int b_mid = diag - a_mid;\n\n        float a_mid_val = (a_mid < Alen) ? A[a_mid] : CUDART_INF_F;\n        float b_prev_val = (b_mid > 0) ? B[b_mid - 1] : -CUDART_INF_F;\n\n        if (a_mid_val < b_prev_val) {\n            a_low = a_mid + 1;\n        } else {\n            a_high = a_mid;\n        }\n    }\n    return a_low;\n}\n\n// Merge B (length M) into top-k array A (length K), both sorted ascending,\n// producing the smallest K elements of the union into C (length K).\n// Work is split among the 32 threads in the warp using merge-path partitioning.\n__device__ __forceinline__ void warp_merge_topk(const float* __restrict__ A_dist,\n                                                const int*   __restrict__ A_idx,\n                                                const float* __restrict__ B_dist,\n                                                const int*   __restrict__ B_idx,\n                                                int K, int M,\n                                                float* __restrict__ C_dist,\n                                                int*   __restrict__ C_idx) {\n    int lane = lane_id();\n\n    // We only need the first K elements of the merge of A (K) and B (M)\n    const int TOT = K;\n    const int seg = (TOT + 31) >> 5; // ceil(TOT / 32)\n\n    int d0 = min(TOT, lane * seg);\n    int d1 = min(TOT, d0 + seg);\n\n    int a0 = merge_path_search(A_dist, K, B_dist, M, d0);\n    int b0 = d0 - a0;\n\n    int a1 = merge_path_search(A_dist, K, B_dist, M, d1);\n    int b1 = d1 - a1;\n\n    int ai = a0;\n    int bi = b0;\n    int out = d0;\n\n    while (out < d1) {\n        float av = (ai < K) ? A_dist[ai] : CUDART_INF_F;\n        float bv = (bi < M) ? B_dist[bi] : CUDART_INF_F;\n        bool takeA = (av <= bv);\n        if (takeA) {\n            C_dist[out] = av;\n            C_idx[out]  = A_idx[ai];\n            ++ai;\n        } else {\n            C_dist[out] = bv;\n            C_idx[out]  = B_idx[bi];\n            ++bi;\n        }\n        ++out;\n    }\n}\n\n// Main kernel: one warp per query.\n__global__ void knn_kernel(const float2* __restrict__ query,\n                           int query_count,\n                           const float2* __restrict__ data,\n                           int data_count,\n                           std::pair<int, float>* __restrict__ result,\n                           int k) {\n    // Shared memory layout:\n    // [0]  float2 s_data[DATA_TILE_POINTS];\n    // [1]  float  s_topDist[WARPS_PER_BLOCK * k];\n    // [2]  int    s_topIdx [WARPS_PER_BLOCK * k];\n    // [3]  float  s_outDist[WARPS_PER_BLOCK * k];\n    // [4]  int    s_outIdx [WARPS_PER_BLOCK * k];\n    // [5]  float  s_candDist[WARPS_PER_BLOCK * 32];\n    // [6]  int    s_candIdx [WARPS_PER_BLOCK * 32];\n\n    extern __shared__ unsigned char smem[];\n    unsigned char* sm_ptr = smem;\n\n    float2* s_data = reinterpret_cast<float2*>(sm_ptr);\n    sm_ptr += DATA_TILE_POINTS * sizeof(float2);\n\n    float* s_topDist_all = reinterpret_cast<float*>(sm_ptr);\n    sm_ptr += (size_t)KNN_WARPS_PER_BLOCK * (size_t)k * sizeof(float);\n\n    int*   s_topIdx_all  = reinterpret_cast<int*>(sm_ptr);\n    sm_ptr += (size_t)KNN_WARPS_PER_BLOCK * (size_t)k * sizeof(int);\n\n    float* s_outDist_all = reinterpret_cast<float*>(sm_ptr);\n    sm_ptr += (size_t)KNN_WARPS_PER_BLOCK * (size_t)k * sizeof(float);\n\n    int*   s_outIdx_all  = reinterpret_cast<int*>(sm_ptr);\n    sm_ptr += (size_t)KNN_WARPS_PER_BLOCK * (size_t)k * sizeof(int);\n\n    float* s_candDist_all = reinterpret_cast<float*>(sm_ptr);\n    sm_ptr += (size_t)KNN_WARPS_PER_BLOCK * 32 * sizeof(float);\n\n    int*   s_candIdx_all  = reinterpret_cast<int*>(sm_ptr);\n    // sm_ptr final not used further\n\n    const int warp = warp_id();\n    const int lane = lane_id();\n    const int warps_per_block = KNN_WARPS_PER_BLOCK;\n\n    const int query_idx = blockIdx.x * warps_per_block + warp;\n\n    // Pointers to this warp's private regions in shared memory\n    float* s_topDist  = s_topDist_all  + (size_t)warp * (size_t)k;\n    int*   s_topIdx   = s_topIdx_all   + (size_t)warp * (size_t)k;\n    float* s_outDist  = s_outDist_all  + (size_t)warp * (size_t)k;\n    int*   s_outIdx   = s_outIdx_all   + (size_t)warp * (size_t)k;\n    float* s_candDist = s_candDist_all + (size_t)warp * 32;\n    int*   s_candIdx  = s_candIdx_all  + (size_t)warp * 32;\n\n    // Load query point into registers and broadcast within the warp\n    float qx = 0.0f, qy = 0.0f;\n    if (lane == 0 && query_idx < query_count) {\n        float2 q = query[query_idx];\n        qx = q.x; qy = q.y;\n    }\n    qx = __shfl_sync(FULL_MASK, qx, 0);\n    qy = __shfl_sync(FULL_MASK, qy, 0);\n\n    // Initialize per-warp top-k buffer (ascending, initialized to +inf, idx -1)\n    if (query_idx < query_count) {\n        for (int i = lane; i < k; i += 32) {\n            s_topDist[i] = CUDART_INF_F;\n            s_topIdx[i]  = -1;\n        }\n    }\n    __syncwarp();\n\n    // Process data in shared-memory tiles\n    for (int data_base = 0; data_base < data_count; data_base += DATA_TILE_POINTS) {\n        int tile_count = data_count - data_base;\n        if (tile_count > DATA_TILE_POINTS) tile_count = DATA_TILE_POINTS;\n\n        // Cooperative load of the tile into shared memory by the whole block\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            s_data[i] = data[data_base + i];\n        }\n        __syncthreads();\n\n        if (query_idx < query_count) {\n            // Iterate through the tile in chunks of 32 points\n            const int steps = (tile_count + 31) >> 5;\n            for (int step = 0; step < steps; ++step) {\n                // Each lane picks one point from the tile (or INF if out-of-range)\n                int idx_in_tile = (step << 5) + lane;\n                float cand_dist;\n                int cand_idx = -1;\n                if (idx_in_tile < tile_count) {\n                    float2 p = s_data[idx_in_tile];\n                    float dx = p.x - qx;\n                    float dy = p.y - qy;\n                    // Squared Euclidean distance (no sqrt)\n                    cand_dist = fmaf(dx, dx, dy * dy);\n                    cand_idx = data_base + idx_in_tile;\n                } else {\n                    cand_dist = CUDART_INF_F;\n                    cand_idx = -1;\n                }\n\n                // Sort the 32 candidates in ascending order across the warp\n                warp_bitonic_sort_32_asc(cand_dist, cand_idx);\n\n                // Store sorted candidates to shared memory (per-warp region)\n                s_candDist[lane] = cand_dist;\n                s_candIdx [lane] = cand_idx;\n                __syncwarp();\n\n                // Number of valid candidates in this chunk (last chunk may be partial)\n                int m = tile_count - (step << 5);\n                if (m > 32) m = 32;\n                if (m < 0)  m = 0;\n\n                // Merge the 32 sorted candidates into the warp's top-k\n                warp_merge_topk(s_topDist, s_topIdx, s_candDist, s_candIdx, k, m, s_outDist, s_outIdx);\n                __syncwarp();\n\n                // Copy merged result back to the per-warp top-k buffer\n                for (int i = lane; i < k; i += 32) {\n                    s_topDist[i] = s_outDist[i];\n                    s_topIdx[i]  = s_outIdx[i];\n                }\n                __syncwarp();\n            }\n        }\n\n        __syncthreads(); // ensure tile is no longer in use before overwriting\n    }\n\n    // Write final top-k results for this query back to global memory in ascending order\n    if (query_idx < query_count) {\n        const int base = query_idx * k;\n        for (int i = lane; i < k; i += 32) {\n            // std::pair<int,float>: write members directly\n            result[base + i].first  = s_topIdx[i];\n            result[base + i].second = s_topDist[i];\n        }\n    }\n}\n\n// Host-side entry point\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data, int data_count,\n             std::pair<int, float> *result, int k) {\n    // Grid/block configuration\n    const int warps_per_block = KNN_WARPS_PER_BLOCK;\n    const int threads_per_block = warps_per_block * 32;\n    const int grid_blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Dynamic shared memory size calculation.\n    // Components:\n    // - DATA_TILE_POINTS * sizeof(float2)\n    // - For each warp:\n    //     * topDist[k] + topIdx[k] + outDist[k] + outIdx[k]\n    //     * candDist[32] + candIdx[32]\n    size_t smem_bytes = 0;\n    smem_bytes += (size_t)DATA_TILE_POINTS * sizeof(float2);\n    smem_bytes += (size_t)warps_per_block * ( (size_t)k * sizeof(float)   // topDist\n                                            + (size_t)k * sizeof(int)     // topIdx\n                                            + (size_t)k * sizeof(float)   // outDist\n                                            + (size_t)k * sizeof(int)     // outIdx\n                                            + 32 * sizeof(float)          // candDist\n                                            + 32 * sizeof(int) );         // candIdx\n\n    // Ensure kernel is allowed to use the necessary dynamic shared memory on capable GPUs\n    int device = 0;\n    cudaGetDevice(&device);\n    int maxDynSmem = 0;\n    cudaDeviceGetAttribute(&maxDynSmem, cudaDevAttrMaxDynamicSharedMemorySize, device);\n    if ((int)smem_bytes <= maxDynSmem) {\n        cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)smem_bytes);\n    }\n\n    // Launch kernel\n    dim3 block(threads_per_block, 1, 1);\n    dim3 grid(grid_blocks, 1, 1);\n    knn_kernel<<<grid, block, smem_bytes>>>(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 641, "completion_tokens": 23478, "total_tokens": 24119, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 19840, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db751988190a3202bf2e74999bf", "custom_id": "knn02-06", "response": {"status_code": 200, "request_id": "c758a848fc9559dd80ef97012a15b6ee", "body": {"id": "chatcmpl-C4tIwH6cNvNLFizcBibcrdUkj2zZV", "object": "chat.completion", "created": 1755281938, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <algorithm>\n#include <cstdint>\n#include <utility>\n\n// Device-side POD equivalent of std::pair<int, float> to avoid depending on libstdc++ device annotations.\n// We will reinterpret_cast the std::pair<int,float>* pointer to this type inside run_knn.\nstruct PairIF {\n    int first;\n    float second;\n};\n\n// Utility: check for CUDA runtime errors in host code paths. This is used only in the host function below.\nstatic inline void cuda_check(cudaError_t e, const char* file, int line) {\n    if (e != cudaSuccess) {\n        fprintf(stderr, \"CUDA error %s at %s:%d\\n\", cudaGetErrorString(e), file, line);\n        abort();\n    }\n}\n#define CUDA_CHECK(call) cuda_check((call), __FILE__, __LINE__)\n\n// Warp-level constants\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Max threads per block we plan to use. We adapt the actual warps per block at runtime,\n// but never launch more than MAX_WARPS_PER_BLOCK warps.\n#ifndef MAX_WARPS_PER_BLOCK\n#define MAX_WARPS_PER_BLOCK 8\n#endif\n\n// Device utility: swap two elements in the heap arrays.\n__device__ __forceinline__ void heap_swap(float* __restrict__ hd, int* __restrict__ hi, int a, int b) {\n    float td = hd[a]; hd[a] = hd[b]; hd[b] = td;\n    int ti = hi[a]; hi[a] = hi[b]; hi[b] = ti;\n}\n\n// Device utility: sift-up operation for a max-heap.\n// Moves the element at position 'pos' up until the max-heap property is restored.\n__device__ __forceinline__ void heap_sift_up(float* __restrict__ hd, int* __restrict__ hi, int pos) {\n    while (pos > 0) {\n        int parent = (pos - 1) >> 1;\n        if (hd[parent] >= hd[pos]) break;\n        heap_swap(hd, hi, parent, pos);\n        pos = parent;\n    }\n}\n\n// Device utility: sift-down operation for a max-heap of size 'heap_size'.\n// Restores the max-heap property starting from the root at position 0.\n__device__ __forceinline__ void heap_sift_down(float* __restrict__ hd, int* __restrict__ hi, int heap_size) {\n    int pos = 0;\n    while (true) {\n        int left = (pos << 1) + 1;\n        if (left >= heap_size) break;\n        int right = left + 1;\n        int largest = left;\n        if (right < heap_size && hd[right] > hd[left]) {\n            largest = right;\n        }\n        if (hd[pos] >= hd[largest]) break;\n        heap_swap(hd, hi, pos, largest);\n        pos = largest;\n    }\n}\n\n// Device utility: push a new element (d, id) into the max-heap if heap_size < k.\n// Post-condition: heap_size is incremented and heap property maintained.\n__device__ __forceinline__ void heap_push(float* __restrict__ hd, int* __restrict__ hi, int& heap_size, float d, int id) {\n    int pos = heap_size;\n    hd[pos] = d;\n    hi[pos] = id;\n    heap_size++;\n    heap_sift_up(hd, hi, pos);\n}\n\n// Device utility: replace root with (d, id) and sift-down to maintain heap property.\n__device__ __forceinline__ void heap_replace_root(float* __restrict__ hd, int* __restrict__ hi, int heap_size, float d, int id) {\n    hd[0] = d;\n    hi[0] = id;\n    heap_sift_down(hd, hi, heap_size);\n}\n\n// Device utility: pop the root from the heap (max element) and write it to (out_d, out_i).\n// Decreases heap_size and maintains heap property.\n__device__ __forceinline__ void heap_pop(float* __restrict__ hd, int* __restrict__ hi, int& heap_size, float& out_d, int& out_i) {\n    out_d = hd[0];\n    out_i = hi[0];\n    int last = heap_size - 1;\n    hd[0] = hd[last];\n    hi[0] = hi[last];\n    heap_size = last;\n    if (heap_size > 0) {\n        heap_sift_down(hd, hi, heap_size);\n    }\n}\n\n// Each query is processed by a single warp (32 threads). Data points are processed in tiles cached in shared memory.\n// Per-warp, we maintain a private max-heap of size k in shared memory (distances + indices).\n// This kernel is designed for large data_count and many queries, with k in [32, 1024] (power of two).\n__global__ void knn_kernel(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data, int data_count,\n    PairIF* __restrict__ result, int k,\n    int warps_per_block, int tile_points)\n{\n    extern __shared__ unsigned char smem[];\n    // Shared memory layout:\n    // [tile_points * sizeof(float2)] + [warps_per_block * k * sizeof(float)] + [warps_per_block * k * sizeof(int)] + [warps_per_block * sizeof(int)]\n    float2* tile = reinterpret_cast<float2*>(smem);\n    size_t tile_bytes = size_t(tile_points) * sizeof(float2);\n    unsigned char* ptr = smem + tile_bytes;\n\n    float* all_heap_d = reinterpret_cast<float*>(ptr);\n    ptr += size_t(warps_per_block) * size_t(k) * sizeof(float);\n\n    int* all_heap_i = reinterpret_cast<int*>(ptr);\n    ptr += size_t(warps_per_block) * size_t(k) * sizeof(int);\n\n    int* heap_sizes = reinterpret_cast<int*>(ptr);\n    // ptr += size_t(warps_per_block) * sizeof(int); // Not needed further\n\n    int tid = threadIdx.x;\n    int lane = tid & (WARP_SIZE - 1);\n    int warp_in_block = tid >> 5;\n    int global_warp = blockIdx.x * warps_per_block + warp_in_block;\n\n    bool warp_active = (global_warp < query_count);\n\n    // Per-warp heap pointers into shared memory\n    float* heap_d = all_heap_d + size_t(warp_in_block) * size_t(k);\n    int*   heap_i = all_heap_i + size_t(warp_in_block) * size_t(k);\n\n    // Initialize heap size to 0 for active warps\n    if (lane == 0) {\n        heap_sizes[warp_in_block] = 0;\n    }\n    __syncwarp();\n\n    // Broadcast query point to all lanes in this warp\n    float qx = 0.0f, qy = 0.0f;\n    if (warp_active) {\n        if (lane == 0) {\n            float2 q = query[global_warp];\n            qx = q.x;\n            qy = q.y;\n        }\n        // Broadcast qx, qy from lane 0 to all lanes of the warp\n        unsigned mask = 0xffffffffu;\n        qx = __shfl_sync(mask, qx, 0);\n        qy = __shfl_sync(mask, qy, 0);\n    }\n\n    // Process data in tiles cached in shared memory\n    for (int tile_start = 0; tile_start < data_count; tile_start += tile_points) {\n        int count = data_count - tile_start;\n        if (count > tile_points) count = tile_points;\n\n        // Block-level cooperative load of the tile into shared memory.\n        // Use a contiguous, coalesced access pattern.\n        for (int i = threadIdx.x; i < count; i += blockDim.x) {\n            tile[i] = data[tile_start + i];\n        }\n        __syncthreads();\n\n        // Each active warp processes the tile; inactive warps just participate in barriers.\n        if (warp_active) {\n            // Stride over tile indices by warp size so each lane handles a separate subset.\n            for (int t = lane; t < count; t += WARP_SIZE) {\n                // Load point from shared memory\n                float2 p = tile[t];\n                float dx = qx - p.x;\n                float dy = qy - p.y;\n                // Squared Euclidean distance (use FMA for throughput)\n                float d = fmaf(dx, dx, dy * dy);\n                int id = tile_start + t;\n\n                // We will perform at most one insertion per lane per processed element.\n                // Identify candidates that might need an insertion into the heap.\n                // We snapshot heap size to avoid reading invalid heap root when heap is not full yet.\n                int local_hs = heap_sizes[warp_in_block];\n                bool want_insert = (local_hs < k);\n                if (!want_insert) {\n                    // Now it's safe to read heap root (heap_d[0]) because heap_size >= 1\n                    float worst = heap_d[0];\n                    want_insert = (d < worst);\n                }\n\n                // Ballot lanes that want to insert; then handle them one-by-one in lane order.\n                unsigned active = __ballot_sync(0xffffffffu, want_insert);\n                while (active) {\n                    int leader = __ffs(active) - 1; // index [0..31] of first set bit\n                    if (lane == leader) {\n                        // Re-check against up-to-date heap state\n                        int& hs = heap_sizes[warp_in_block];\n                        if (hs < k) {\n                            // Heap not full yet: push the candidate\n                            heap_push(heap_d, heap_i, hs, d, id);\n                        } else {\n                            // Heap full: replace root if this candidate is better (smaller distance)\n                            if (d < heap_d[0]) {\n                                heap_replace_root(heap_d, heap_i, hs, d, id);\n                            }\n                        }\n                    }\n                    __syncwarp();\n                    // Clear processed bit and continue\n                    active &= active - 1;\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    // After processing all tiles, lane 0 of each active warp extracts the k nearest neighbors\n    // from the max-heap into the result array in ascending order of distance.\n    if (warp_active && lane == 0) {\n        int hs = heap_sizes[warp_in_block];\n        // It is possible (only theoretically) that data_count < k; the problem statement guarantees data_count >= k.\n        // But to be robust, clamp hs to k.\n        if (hs > k) hs = k;\n\n        // Extract from heap: popping gives elements in non-increasing order (largest first).\n        // We store them into result in reverse so that result[j] is j-th nearest neighbor (ascending distances).\n        PairIF* out = result + size_t(global_warp) * size_t(k);\n        for (int pos = hs - 1; pos >= 0; --pos) {\n            float d;\n            int id;\n            heap_pop(heap_d, heap_i, hs, d, id);\n            out[pos].first = id;\n            out[pos].second = d;\n        }\n        // If for any reason hs < k (should not happen as data_count >= k), pad the remaining\n        // entries with invalid indices and +INF distances to maintain invariants.\n        for (int pos = 0; pos < k - hs; ++pos) {\n            out[pos].first = -1;\n            out[pos].second = CUDART_INF_F;\n        }\n    }\n}\n\n// Host helper: compute an efficient launch configuration based on available shared memory.\n// We adapt warps per block and tile size to fit within the device's max dynamic shared memory limit.\n// - warps_per_block in {8, 4, 2, 1}\n// - tile_points chosen to use the remaining shared memory after accounting for per-warp top-k storage\nstatic inline void choose_launch_config(int k, int query_count, int data_count,\n                                        int& warps_per_block, dim3& blocks, dim3& threads,\n                                        int& tile_points, size_t& smem_bytes) {\n    // Query device properties\n    int device = 0;\n    CUDA_CHECK(cudaGetDevice(&device));\n    cudaDeviceProp prop{};\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, device));\n\n    // Maximum dynamic shared memory per block available (opt-in for Ampere/Hopper)\n    int max_smem_optin = 0;\n    CUDA_CHECK(cudaDeviceGetAttribute(&max_smem_optin, cudaDevAttrMaxSharedMemoryPerBlockOptin, device));\n    int max_smem_default = prop.sharedMemPerBlock; // typically 48/64/96 KB\n    int max_smem = (max_smem_optin > 0 ? max_smem_optin : max_smem_default);\n\n    // Start with a preferred number of warps per block and reduce if necessary to fit smem.\n    int candidate_warps[] = { MAX_WARPS_PER_BLOCK, 4, 2, 1 };\n    warps_per_block = 1;\n    tile_points = 0;\n    smem_bytes = 0;\n\n    for (int w : candidate_warps) {\n        if (w <= 0) continue;\n        // Per-warp top-k storage: distances (float) + indices (int) + heap_size (int per warp)\n        size_t bytes_topk = size_t(w) * size_t(k) * (sizeof(float) + sizeof(int)) + size_t(w) * sizeof(int);\n        // We need at least one blockDim.x worth of points in the tile to get a perfectly coalesced load.\n        int block_threads = w * WARP_SIZE;\n        size_t bytes_min_tile = size_t(block_threads) * sizeof(float2); // at least one per thread\n        if (bytes_topk + bytes_min_tile > size_t(max_smem)) {\n            // Not enough shared memory for this number of warps per block; try fewer warps.\n            continue;\n        }\n        // Use as many points as remaining shared memory allows.\n        size_t bytes_tile = size_t(max_smem) - bytes_topk;\n        int tp = int(bytes_tile / sizeof(float2));\n        // Round tile_points down to a multiple of block_threads for coalesced loads\n        tp = (tp / block_threads) * block_threads;\n        // Ensure at least one stride\n        if (tp < block_threads) tp = block_threads;\n        // Upper bound tile_points by data_count to avoid overprovisioning dynamic shared memory (not necessary, but reasonable)\n        if (tp > data_count) tp = ((data_count + block_threads - 1) / block_threads) * block_threads;\n        // Compute shared memory bytes for this configuration\n        size_t smem_needed = bytes_topk + size_t(tp) * sizeof(float2);\n        if (smem_needed <= size_t(max_smem)) {\n            warps_per_block = w;\n            tile_points = tp;\n            smem_bytes = smem_needed;\n            break;\n        }\n    }\n\n    // Fallback safety: if the above loop fails (should not), pick 1 warp/block with minimal tile.\n    if (tile_points <= 0 || smem_bytes == 0) {\n        warps_per_block = 1;\n        int block_threads = warps_per_block * WARP_SIZE;\n        size_t bytes_topk = size_t(warps_per_block) * size_t(k) * (sizeof(float) + sizeof(int)) + size_t(warps_per_block) * sizeof(int);\n        size_t bytes_tile = size_t(max_smem) - bytes_topk;\n        int tp = int(bytes_tile / sizeof(float2));\n        tp = (tp / block_threads) * block_threads;\n        if (tp < block_threads) tp = block_threads;\n        tile_points = tp;\n        smem_bytes = bytes_topk + size_t(tp) * sizeof(float2);\n    }\n\n    // Compute launch grid/block dimensions\n    threads = dim3(warps_per_block * WARP_SIZE);\n    int blocks_needed = (query_count + warps_per_block - 1) / warps_per_block;\n    blocks = dim3(blocks_needed, 1, 1);\n}\n\n// Entry point: launch the optimized CUDA kernel.\n// - query: pointer to device memory array of float2, size query_count\n// - data: pointer to device memory array of float2, size data_count\n// - result: pointer to device memory array of std::pair<int,float>, size query_count * k\n// - k: number of neighbors (power of two between 32 and 1024), with data_count >= k\nvoid run_knn(const float2* query, int query_count, const float2* data, int data_count, std::pair<int, float>* result, int k) {\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Choose launch configuration based on device shared memory capacity and inputs\n    int warps_per_block = 0;\n    int tile_points = 0;\n    size_t smem_bytes = 0;\n    dim3 blocks, threads;\n    choose_launch_config(k, query_count, data_count, warps_per_block, blocks, threads, tile_points, smem_bytes);\n\n    // Opt-in to use the maximum dynamic shared memory available on the device for this kernel\n    int device = 0;\n    CUDA_CHECK(cudaGetDevice(&device));\n    int max_smem_optin = 0;\n    CUDA_CHECK(cudaDeviceGetAttribute(&max_smem_optin, cudaDevAttrMaxSharedMemoryPerBlockOptin, device));\n    if (max_smem_optin > 0) {\n        CUDA_CHECK(cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, max_smem_optin));\n    }\n\n    // Launch kernel. We pass result as POD PairIF* to be writable by device code.\n    knn_kernel<<<blocks, threads, smem_bytes>>>(\n        query, query_count, data, data_count,\n        reinterpret_cast<PairIF*>(result), k,\n        warps_per_block, tile_points\n    );\n    CUDA_CHECK(cudaGetLastError());\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 641, "completion_tokens": 17812, "total_tokens": 18453, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 13888, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db70d048190bbe53aa20964e78d", "custom_id": "knn02-07", "response": {"status_code": 200, "request_id": "7b8583f253ee9ff6bad87bd4c4bde137", "body": {"id": "chatcmpl-C4tKy2jPQiK5mJgN622TQqdPloDm2", "object": "chat.completion", "created": 1755282064, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <math_constants.h>\n#include <utility>\n\n// Kernel configuration tuned for A100/H100-class GPUs.\n// - 256 threads per block (8 warps), one warp processes one query.\n// - Data points are processed in tiles cached in shared memory.\n// - Each thread keeps a small per-lane buffer of size k/32 in registers.\n// - The final k results per query are merged and sorted cooperatively in shared memory.\n#ifndef THREADS_PER_BLOCK\n#define THREADS_PER_BLOCK 256\n#endif\n#define WARPS_PER_BLOCK (THREADS_PER_BLOCK / 32)\n#define WARP_SIZE 32\n// Maximum per-lane buffer size: k is in [32, 1024] and power of two, so k/32 in [1, 32].\n#define KLANE_MAX 32\n\n// Utility: warp lane id and warp id within block\nstatic __device__ __forceinline__ int lane_id() { return threadIdx.x & (WARP_SIZE - 1); }\nstatic __device__ __forceinline__ int warp_id() { return threadIdx.x >> 5; }\n\n// In-warp swap helper for shared-memory arrays of pairs (distance, index)\nstatic __device__ __forceinline__\nvoid swap_pair(float &a_d, int &a_i, float &b_d, int &b_i) {\n    float td = a_d; int ti = a_i;\n    a_d = b_d; a_i = b_i;\n    b_d = td; b_i = ti;\n}\n\n// Bitonic sort (ascending by distance) over a shared-memory array of length K.\n// Threads within a warp cooperatively sort sdist[0..K-1] and sidx[0..K-1] in-place.\n// K is guaranteed to be a power of two between 32 and 1024.\n// Requires: all 32 threads in the warp participate; synchronization via __syncwarp().\nstatic __device__ void warp_bitonic_sort(float* sdist, int* sidx, int K) {\n    const unsigned mask = 0xffffffffu;\n    // Outer loop controls the sequence length of bitonic merges.\n    for (int size = 2; size <= K; size <<= 1) {\n        // Inner loop controls the compare distance within the current sequence.\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            // Each lane handles a strided set of indices.\n            for (int i = lane_id(); i < K; i += WARP_SIZE) {\n                int ixj = i ^ stride;\n                if (ixj > i) {\n                    bool up = ((i & size) == 0);\n                    float a = sdist[i];\n                    float b = sdist[ixj];\n                    // Note: For ties (a == b) we leave as-is; stability is not required.\n                    if ((a > b) == up) {\n                        // Swap both distance and index\n                        swap_pair(sdist[i], sidx[i], sdist[ixj], sidx[ixj]);\n                    }\n                }\n            }\n            __syncwarp(mask);\n        }\n    }\n}\n\n// CUDA kernel: one warp processes one query.\n__global__ void knn_kernel(const float2* __restrict__ query,\n                           int query_count,\n                           const float2* __restrict__ data,\n                           int data_count,\n                           std::pair<int, float>* __restrict__ result,\n                           int K,\n                           int tile_elems)\n{\n    extern __shared__ unsigned char smem[];\n    // Shared memory layout per block:\n    // [0 .. tile_elems*sizeof(float2))               -> cached data tile as float2[]\n    // [tile_elems*sizeof(float2) .. + warps*K*4)     -> per-warp distances (float[])\n    // [.. + warps*K*4 .. + warps*K*8)                -> per-warp indices (int[])\n    float2* tile = reinterpret_cast<float2*>(smem);\n    float*  sdist_base = reinterpret_cast<float*>(tile + tile_elems);\n    int*    sidx_base  = reinterpret_cast<int*>(sdist_base + WARPS_PER_BLOCK * K);\n\n    const int lane = lane_id();\n    const int wid  = warp_id();\n    const int warp_global_query = blockIdx.x * WARPS_PER_BLOCK + wid;\n\n    // If no query in this block at all, we can exit early.\n    if (blockIdx.x * WARPS_PER_BLOCK >= query_count) return;\n\n    const bool warp_active = (warp_global_query < query_count);\n    float* sdist = sdist_base + wid * K;\n    int*   sidx  = sidx_base  + wid * K;\n\n    // Each thread keeps a small local buffer of size Klane = K / 32 (power of two, in [1, 32]).\n    const int Klane = K >> 5;\n    float local_dist[KLANE_MAX];\n    int   local_idx[KLANE_MAX];\n\n    // Initialize per-lane local buffers to +inf distance and invalid index.\n    if (warp_active) {\n#pragma unroll\n        for (int i = 0; i < KLANE_MAX; ++i) {\n            if (i < Klane) {\n                local_dist[i] = CUDART_INF_F;\n                local_idx[i]  = -1;\n            }\n        }\n    }\n\n    // Load the query point once per warp and broadcast to all lanes.\n    float qx = 0.f, qy = 0.f;\n    if (warp_active) {\n        if (lane == 0) {\n            float2 q = query[warp_global_query];\n            qx = q.x; qy = q.y;\n        }\n        const unsigned mask = 0xffffffffu;\n        qx = __shfl_sync(mask, qx, 0);\n        qy = __shfl_sync(mask, qy, 0);\n    }\n\n    // Per-lane tracking of current worst (maximum) distance in the local buffer,\n    // used as a quick rejection threshold.\n    float local_worst = CUDART_INF_F;\n    int   local_worst_pos = 0;\n\n    // Iterate over the dataset in tiles cached in shared memory.\n    for (int base = 0; base < data_count; base += tile_elems) {\n        int tile_n = data_count - base;\n        if (tile_n > tile_elems) tile_n = tile_elems;\n\n        // Cooperative load of the current tile into shared memory.\n        // All threads in the block participate in the load and in the barrier.\n        for (int t = threadIdx.x; t < tile_n; t += blockDim.x) {\n            // Use __ldg to hint read-only caching; data[] is declared const.\n            tile[t] = __ldg(&data[base + t]);\n        }\n        __syncthreads();\n\n        // Each active warp processes the cached tile for its query.\n        if (warp_active) {\n            // Strided loop over tile elements: each lane processes tile indices t = lane, lane+32, ...\n            for (int t = lane; t < tile_n; t += WARP_SIZE) {\n                float2 p = tile[t];\n                float dx = p.x - qx;\n                float dy = p.y - qy;\n                // Squared Euclidean distance\n                float d = fmaf(dx, dx, dy * dy);\n                const int gidx = base + t;\n\n                // If candidate is better than current worst in local buffer, insert it.\n                if (d < local_worst) {\n                    // Replace the current worst element.\n                    local_dist[local_worst_pos] = d;\n                    local_idx[local_worst_pos]  = gidx;\n\n                    // Recompute the worst element and its position among the Klane entries.\n                    // This is O(Klane) and happens only on successful insert.\n                    float w = local_dist[0];\n                    int   wp = 0;\n#pragma unroll\n                    for (int i = 1; i < KLANE_MAX; ++i) {\n                        if (i < Klane) {\n                            float vi = local_dist[i];\n                            if (vi > w) { w = vi; wp = i; }\n                        }\n                    }\n                    local_worst = w;\n                    local_worst_pos = wp;\n                }\n            }\n        }\n\n        __syncthreads(); // Ensure the tile region can be safely overwritten in the next iteration.\n    }\n\n    // Active warps now cooperatively merge their per-lane buffers into a per-warp buffer in shared memory.\n    if (warp_active) {\n        // Gather: each lane writes its Klane elements contiguously into the per-warp segment.\n#pragma unroll\n        for (int i = 0; i < KLANE_MAX; ++i) {\n            if (i < Klane) {\n                int pos = lane * Klane + i;  // unique in [0, K)\n                sdist[pos] = local_dist[i];\n                sidx[pos]  = local_idx[i];\n            }\n        }\n        __syncwarp();\n\n        // Final cooperative sort across the K entries to produce ordered k-NN for this query.\n        warp_bitonic_sort(sdist, sidx, K);\n\n        // Write back the results in sorted order. Each lane writes a strided subset.\n        std::pair<int, float>* out = result + (size_t)warp_global_query * (size_t)K;\n        for (int i = lane; i < K; i += WARP_SIZE) {\n            out[i].first  = sidx[i];\n            out[i].second = sdist[i];\n        }\n    }\n}\n\n// Host interface: launches the kernel.\n// - query: device pointer to float2 query points\n// - query_count: number of queries\n// - data: device pointer to float2 data points\n// - data_count: number of data points\n// - result: device pointer to std::pair<int,float> output buffer of size query_count * k\n// - k: number of neighbors; must be power of two in [32, 1024]\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data,  int data_count,\n             std::pair<int, float> *result, int k)\n{\n    // Launch configuration\n    dim3 block(THREADS_PER_BLOCK);\n    dim3 grid((query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK);\n\n    // Determine dynamic shared memory requirements and choose a tile size.\n    // Target a tile of 4096 points (32 KiB) when possible.\n    int tile_elems = 4096;\n\n    // Compute top-k shared memory bytes per block.\n    size_t topk_bytes = size_t(WARPS_PER_BLOCK) * size_t(k) * (sizeof(float) + sizeof(int));\n    size_t tile_bytes = size_t(tile_elems) * sizeof(float2);\n    size_t smem_bytes = topk_bytes + tile_bytes;\n\n    // Query the device limit for opt-in dynamic shared memory and adjust tile size if needed.\n    int device = 0;\n    cudaGetDevice(&device);\n    int max_optin_smem = 0;\n    cudaDeviceGetAttribute(&max_optin_smem, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n\n    if (smem_bytes > (size_t)max_optin_smem) {\n        // Reduce tile_elems to fit within the available shared memory.\n        size_t max_tile_bytes = (size_t)max_optin_smem > topk_bytes ? (size_t)max_optin_smem - topk_bytes : 0;\n        size_t feasible_tile_elems = max_tile_bytes / sizeof(float2);\n        if (feasible_tile_elems == 0) {\n            // As a last resort (should not happen on A100/H100 with given k range),\n            // clamp to the minimal tile size of 32 to keep the kernel functional.\n            feasible_tile_elems = 32;\n        } else if (feasible_tile_elems > (size_t)tile_elems) {\n            feasible_tile_elems = tile_elems;\n        }\n        tile_elems = (int)feasible_tile_elems;\n        tile_bytes = size_t(tile_elems) * sizeof(float2);\n        smem_bytes = topk_bytes + tile_bytes;\n    }\n\n    // Request the required dynamic shared memory size for this kernel.\n    // This enables usage beyond the default per-block limit on A100/H100.\n    cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)smem_bytes);\n\n    // Launch the kernel\n    knn_kernel<<<grid, block, smem_bytes>>>(query, query_count, data, data_count, result, k, tile_elems);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 641, "completion_tokens": 13341, "total_tokens": 13982, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 10624, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db951648190ad4e4e21aa47a0ec", "custom_id": "knn02-08", "response": {"status_code": 200, "request_id": "9778f6ebe0c92177fabaff401522cc65", "body": {"id": "chatcmpl-C4tMZ8cg5WxFKCjSVUSI2Z9Nyrlzq", "object": "chat.completion", "created": 1755282163, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n#include <cfloat>\n#include <cstdio>\n\n/*\nOptimized k-NN (2D, squared L2) with one warp (32 threads) per query.\n\nKey design:\n- Each warp processes one query end-to-end.\n- The block loads the dataset in tiles into shared memory; all warps in the block reuse it.\n- Each lane maintains a small local top-L list per tile (L = k/32). After each tile, all lanes pack\n  only the promising candidates (those that improve the warp\u2019s current top-k threshold) into a per-warp\n  shared buffer of size k. The union of previous top-k and new candidates (<= 2k elements) is then\n  sorted via an in-warp bitonic sort network to retain the new top-k.\n- Per-warp intermediate results (indices and distances) are kept in shared memory throughout and\n  are written to global memory at the end.\n\nImplementation notes:\n- k is assumed to be a power-of-two between 32 and 1024 inclusive.\n- We allocate per-warp candidate buffers of size 2k (distances + indices) in shared memory to support merging.\n- Shared memory layout per block:\n    [tile of float2 data points] + [warps_per_block * 2k floats] + [warps_per_block * 2k ints]\n- We choose a default of 256 threads/block (8 warps). If shared memory is tight for a given k, we reduce\n  warps per block to fit.\n- Warp-scope synchronization uses __syncwarp(mask) with a mask that includes only lanes belonging to\n  warps assigned a valid query. Block-wide synchronization (__syncthreads) wraps tile loads.\n\nThis kernel targets modern data center GPUs (A100/H100) and uses dynamic shared memory sized at launch.\n*/\n\n// Constants\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Device utility: Warp inclusive sum for int with mask.\n// Returns the sum across lanes in 'mask'; each participating lane gets the same final sum if desired via shuffles.\nstatic __device__ __forceinline__ int warp_inclusive_sum(int v, unsigned mask) {\n    // Butterfly reduction using shfl with mask\n    for (int offset = 1; offset < WARP_SIZE; offset <<= 1) {\n        int n = __shfl_up_sync(mask, v, offset);\n        if ((threadIdx.x & (WARP_SIZE - 1)) >= offset) v += n;\n    }\n    return v;\n}\n\n// Device utility: Warp exclusive prefix sum for int with mask.\n// Returns the exclusive prefix sum (sum of values from lanes lower than the current lane within 'mask').\nstatic __device__ __forceinline__ int warp_exclusive_prefix_sum(int v, unsigned mask) {\n    int inclusive = warp_inclusive_sum(v, mask);\n    return inclusive - v;\n}\n\n// Device utility: Warp-wide sum reduction (returns total sum to all lanes in mask)\nstatic __device__ __forceinline__ int warp_sum_all(int v, unsigned mask) {\n    // Convert inclusive scan at last active lane to broadcast: reduce with shfl_down\n    for (int offset = WARP_SIZE >> 1; offset > 0; offset >>= 1) {\n        int n = __shfl_down_sync(mask, v, offset);\n        v += n;\n    }\n    // After this, the value is valid only in lane 0 of the active mask; broadcast it\n    int total = __shfl_sync(mask, v, 0);\n    return total;\n}\n\n// In-warp bitonic sort of N pairs (dist, idx) stored in shared memory.\n// N must be a power of two. The sort order is ascending by 'dist'.\n// All lanes in 'mask' must call this with the same arguments. Lanes not in mask may call it too (they will not wait).\nstatic __device__ __forceinline__ void warp_bitonic_sort_pairs(float* dist, int* idx, int N, unsigned mask) {\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n    // Standard bitonic sort network using XOR based indexing.\n    for (int k = 2; k <= N; k <<= 1) {\n        for (int j = k >> 1; j > 0; j >>= 1) {\n            // Each lane processes multiple indices strided by warp size.\n            for (int i = lane; i < N; i += WARP_SIZE) {\n                int ixj = i ^ j;\n                if (ixj > i) {\n                    bool ascending = ((i & k) == 0);\n                    float di = dist[i];\n                    float dj = dist[ixj];\n                    int   ii = idx[i];\n                    int   ij = idx[ixj];\n                    // If ascending, swap if di > dj; if descending, swap if di < dj\n                    bool do_swap = ascending ? (di > dj) : (di < dj);\n                    if (do_swap) {\n                        dist[i]   = dj; dist[ixj] = di;\n                        idx[i]    = ij; idx[ixj]  = ii;\n                    }\n                }\n            }\n            __syncwarp(mask);\n        }\n    }\n}\n\nstatic __device__ __forceinline__ float sqr_distance2(const float2 a, const float2 b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    return dx * dx + dy * dy;\n}\n\n// Kernel implementing the warp-per-query k-NN with tiled shared-memory caching.\n__global__ void knn_kernel_warp_per_query(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data,  int data_count,\n    std::pair<int, float>* __restrict__ result,\n    int k, int tile_points)\n{\n    extern __shared__ unsigned char smem_raw[];\n    // Shared memory layout:\n    // [tile float2 array] [warps_per_block*2k floats] [warps_per_block*2k ints]\n    float2* shTile = reinterpret_cast<float2*>(smem_raw);\n\n    // Compute warps per block and per-warp offsets\n    const int warps_per_block = blockDim.x / WARP_SIZE;\n    // Offsets to per-warp candidate arrays start after tile\n    unsigned char* after_tile = smem_raw + static_cast<size_t>(tile_points) * sizeof(float2);\n\n    float* shWarpDists = reinterpret_cast<float*>(after_tile);\n    int*   shWarpIdx   = reinterpret_cast<int*>(shWarpDists + static_cast<size_t>(warps_per_block) * (2 * k));\n\n    const int warp_id       = threadIdx.x / WARP_SIZE;\n    const int lane          = threadIdx.x & (WARP_SIZE - 1);\n    const int warp_global   = blockIdx.x * warps_per_block + warp_id;\n    const bool has_query    = (warp_global < query_count);\n    // Mask of all lanes belonging to warps with valid queries in this block\n    unsigned full_mask = __activemask();\n    // Build a mask containing only lanes in the current warp that have a query\n    unsigned query_mask = __ballot_sync(full_mask, has_query);\n\n    // Per-warp base pointers for the 2k candidate arrays in shared memory\n    float* wdist = shWarpDists + static_cast<size_t>(warp_id) * (2 * k);\n    int*   widx  = shWarpIdx   + static_cast<size_t>(warp_id) * (2 * k);\n\n    // Each warp loads its query point, broadcast to all lanes in the warp\n    float2 q = make_float2(0.f, 0.f);\n    if (lane == 0 && has_query) {\n        q = query[warp_global];\n    }\n    // Broadcast q from lane 0; lanes not in query_mask will get undefined, but they won't use it\n    q.x = __shfl_sync(query_mask, q.x, 0);\n    q.y = __shfl_sync(query_mask, q.y, 0);\n\n    // Initialize per-warp top-k arrays to +inf and invalid index (-1).\n    // Only warps with queries need to initialize their regions.\n    if (has_query) {\n        const float INF = FLT_MAX;\n        // Initialize first k\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            wdist[i] = INF;\n            widx[i]  = -1;\n        }\n        // Initialize second half (k..2k-1) to INF/-1 once (will be overwritten per tile as needed, but zero-cost safety)\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            wdist[k + i] = INF;\n            widx[k + i]  = -1;\n        }\n    }\n    __syncwarp(query_mask);\n\n    // Number of local slots per lane\n    const int L = k / WARP_SIZE;\n\n    // Iterate over the dataset in tiles cached in shared memory\n    for (int tile_start = 0; tile_start < data_count; tile_start += tile_points) {\n        const int count = min(tile_points, data_count - tile_start);\n\n        // Block-cooperative load of the current tile into shared memory\n        for (int i = threadIdx.x; i < count; i += blockDim.x) {\n            shTile[i] = data[tile_start + i];\n        }\n        __syncthreads();\n\n        if (has_query) {\n            // Local per-lane top-L lists (kept unsorted). We keep only entries with distance < warpWorst.\n            float localDist[32]; // MAX_L = 1024/32 = 32\n            int   localIdx[32];\n#pragma unroll\n            for (int i = 0; i < 32; ++i) {\n                if (i < L) { localDist[i] = FLT_MAX; localIdx[i] = -1; }\n            }\n            float localWorst = FLT_MAX;\n            int   localWorstPos = 0;\n\n            // Read current warp threshold (worst top-k distance). The top-k is kept sorted after each merge, so worst is at k-1.\n            float threshold = wdist[k - 1];\n\n            // Each lane processes a strided subset of points in the tile\n            for (int t = lane; t < count; t += WARP_SIZE) {\n                float d = sqr_distance2(q, shTile[t]);\n                if (d >= threshold) continue; // Prune if not improving current top-k\n                // Accept into local top-L if better than current local worst\n                if (d < localWorst) {\n                    localDist[localWorstPos] = d;\n                    localIdx[localWorstPos]  = tile_start + t;\n                    // Recompute local worst among L slots (only when a replacement happens)\n                    float wv = -1.0f;\n                    int   wp = 0;\n#pragma unroll\n                    for (int i = 0; i < 32; ++i) {\n                        if (i < L) {\n                            float vi = localDist[i];\n                            if (vi > wv) { wv = vi; wp = i; }\n                        }\n                    }\n                    localWorst = wv;\n                    localWorstPos = wp;\n                }\n            }\n\n            // Pack only the local entries that beat the (possibly old) threshold\n            int packCount = 0;\n            // First, count how many valid improvements we have to compute prefix sum positions\n#pragma unroll\n            for (int i = 0; i < 32; ++i) {\n                if (i < L) {\n                    if (localIdx[i] >= 0 && localDist[i] < threshold) {\n                        ++packCount;\n                    }\n                }\n            }\n            // Compute exclusive prefix sum to place packed items contiguously in [k .. k + totalNew)\n            int myOffset = warp_exclusive_prefix_sum(packCount, query_mask);\n            int totalNew = warp_sum_all(packCount, query_mask);\n\n            // Pre-fill the second half with INF/-1 (to ensure unused entries are clean)\n            for (int i = lane; i < k; i += WARP_SIZE) {\n                wdist[k + i] = FLT_MAX;\n                widx[k + i]  = -1;\n            }\n            __syncwarp(query_mask);\n\n            // Write our packed items into shared memory at positions [k + myOffset, ...)\n            int written = 0;\n#pragma unroll\n            for (int i = 0; i < 32; ++i) {\n                if (i < L) {\n                    if (localIdx[i] >= 0 && localDist[i] < threshold) {\n                        int pos = k + myOffset + written;\n                        if (pos < 2 * k) { // safety\n                            wdist[pos] = localDist[i];\n                            widx[pos]  = localIdx[i];\n                        }\n                        ++written;\n                    }\n                }\n            }\n            __syncwarp(query_mask);\n\n            // If we have any improvements, sort the union of previous top-k and new candidates (size 2k), keep the best k\n            if (totalNew > 0) {\n                warp_bitonic_sort_pairs(wdist, widx, 2 * k, query_mask);\n                __syncwarp(query_mask);\n                // After sorting ascending, wdist[0..k-1] holds the current best k in order; wdist[k-1] is the new threshold.\n            }\n        }\n\n        __syncthreads(); // Ensure all warps are done before loading the next tile\n    }\n\n    // Write out the final top-k for each query\n    if (has_query) {\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            int out_idx = warp_global * k + i;\n            // Assign to std::pair members directly\n            result[out_idx].first  = widx[i];\n            result[out_idx].second = wdist[i];\n        }\n    }\n}\n\n// Host-side launcher matching the required interface.\n// Chooses block/grid sizes and dynamic shared memory layout to balance per-warp candidate storage (2k entries)\n// and the shared-memory tile of data points.\n// Uses up to the device's opt-in maximum shared memory per block to maximize tile size.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data,  int data_count,\n             std::pair<int, float> *result, int k)\n{\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Default: 256 threads per block (8 warps). Adjust downward if shared memory is tight.\n    int desired_threads_per_block = 256;\n    int desired_warps_per_block   = desired_threads_per_block / WARP_SIZE;\n\n    // Query device attributes for dynamic shared memory limits\n    int device = 0;\n    cudaGetDevice(&device);\n    int max_smem_optin = 0;\n    int max_smem_default = 0;\n    cudaDeviceGetAttribute(&max_smem_optin,   cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    cudaDeviceGetAttribute(&max_smem_default, cudaDevAttrMaxSharedMemoryPerBlock, device);\n    // Use the larger of default and opt-in as a target budget\n    int max_dynamic_smem = max(max_smem_optin, max_smem_default);\n    if (max_dynamic_smem <= 0) {\n        // Sensible fallback for very old devices (should not happen on A100/H100)\n        max_dynamic_smem = 96 * 1024;\n    }\n\n    // Per-warp candidate storage (2k pairs: float + int)\n    size_t bytes_per_warp_candidates = static_cast<size_t>(2) * static_cast<size_t>(k) * (sizeof(float) + sizeof(int));\n\n    // Find the maximum number of warps per block that fits into shared memory (keeping at least a small tile)\n    // We'll require at least 1 point in the tile; tile needs sizeof(float2) bytes.\n    int max_warps_by_smem = max(1, (max_dynamic_smem - static_cast<int>(sizeof(float2))) / static_cast<int>(bytes_per_warp_candidates));\n    int warps_per_block = min(desired_warps_per_block, max_warps_by_smem);\n    warps_per_block = max(1, warps_per_block); // ensure at least one warp\n\n    int threads_per_block = warps_per_block * WARP_SIZE;\n\n    // With chosen warps_per_block, compute remaining shared mem for tile\n    size_t cand_bytes_per_block = static_cast<size_t>(warps_per_block) * bytes_per_warp_candidates;\n    size_t remaining_for_tile = (cand_bytes_per_block < static_cast<size_t>(max_dynamic_smem))\n                                ? (static_cast<size_t>(max_dynamic_smem) - cand_bytes_per_block)\n                                : 0;\n\n    // Tile size in points (float2)\n    int tile_points = 0;\n    if (remaining_for_tile >= sizeof(float2)) {\n        tile_points = static_cast<int>(remaining_for_tile / sizeof(float2));\n    } else {\n        tile_points = 1; // minimal fallback; performance will be reduced but correctness preserved\n    }\n\n    // Limit tile_points to a reasonable upper bound to avoid extremely long per-tile loops;\n    // A large tile is beneficial but does not need to exceed the dataset size.\n    if (tile_points > data_count) tile_points = data_count;\n\n    // Compute the actual dynamic shared memory size for the kernel launch\n    size_t dynamic_smem_bytes = static_cast<size_t>(tile_points) * sizeof(float2) + cand_bytes_per_block;\n\n    // Opt-in to using the required amount of dynamic shared memory per block if the device supports it\n    // (On A100/H100 this enables using up to ~164KB).\n    cudaFuncSetAttribute(knn_kernel_warp_per_query,\n                         cudaFuncAttributeMaxDynamicSharedMemorySize,\n                         static_cast<int>(dynamic_smem_bytes));\n\n    // Grid size: one warp per query\n    int warps_needed = (query_count + 0) / 1; // one query per warp\n    int blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Launch the kernel\n    dim3 block(threads_per_block);\n    dim3 grid(blocks);\n    knn_kernel_warp_per_query<<<grid, block, dynamic_smem_bytes>>>(\n        query, query_count, data, data_count, result, k, tile_points\n    );\n    // Optional: consider synchronizing or checking errors outside this function as needed by the caller.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 641, "completion_tokens": 18917, "total_tokens": 19558, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 14912, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db7ca948190b8ee727df6019888", "custom_id": "knn02-09", "response": {"status_code": 200, "request_id": "f287df0b3dd6e476ec3d23f3b2899cc0", "body": {"id": "chatcmpl-C4tPMPOcxUtLddV3lBQWLIeAOw1Yi", "object": "chat.completion", "created": 1755282336, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#include <utility>\n#include <algorithm>\n#include <cfloat>\n\n// This implementation assigns one warp (32 threads) per query.\n// Each warp maintains its K-best list in shared memory (indices and distances).\n// The dataset is processed in tiles cached in shared memory. Each tile is loaded\n// cooperatively by the entire block, then each warp scans the tile against its query and\n// updates its private K-best list. Updates are done warp-synchronously:\n//   - For each group of 32 candidates (one per lane), we repeatedly insert the globally\n//     smallest candidate among the warp into the K-best list while it improves the list,\n//     recomputing the current worst element (\"tau\") cooperatively after each insertion.\n// The K-best list is maintained unsorted during scanning; after all tiles are processed,\n// each warp performs a parallel bitonic sort on its K elements (ascending by distance)\n// and writes results to the output array.\n// The code uses only registers and shared memory (no extra global allocations).\n// It targets large data_count (up to millions) and large query_count (thousands).\n// K is a power of two in [32, 1024].\n\nstatic __device__ __forceinline__ float sq_l2_distance(const float2 a, const float2 b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    // Use FMA to compute dx*dx + dy*dy\n    return fmaf(dx, dx, dy * dy);\n}\n\n// Warp-wide argmin over 32 lanes: input 'val' and implicit lane id.\n// Outputs the minimum value and the lane that owns it, broadcast to all lanes.\nstatic __device__ __forceinline__ void warp_argmin32(float val, int& minLaneOut, float& minValOut) {\n    unsigned mask = 0xFFFFFFFFu;\n    float v = val;\n    int lane = threadIdx.x & 31;\n    int id = lane;\n    // Tree reduction for minimum value; tie-break with smaller lane id.\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        float v2 = __shfl_down_sync(mask, v, offset);\n        int id2 = __shfl_down_sync(mask, id, offset);\n        if (v2 < v || (v2 == v && id2 < id)) {\n            v = v2;\n            id = id2;\n        }\n    }\n    minValOut = __shfl_sync(mask, v, 0);\n    minLaneOut = __shfl_sync(mask, id, 0);\n}\n\n// Recompute the current maximum distance (tau) and its position within the per-warp K-best list.\n// All 32 lanes of the warp cooperate: each lane scans a consecutive chunk of size (k/32).\nstatic __device__ __forceinline__ void warp_recompute_tau_max(const float* __restrict__ top_dist,\n                                                              int k, float& tau_out, int& max_pos_out) {\n    const int lane = threadIdx.x & 31;\n    const int chunk = k >> 5; // k/32, valid because k is power-of-two >= 32\n    const int start = lane * chunk;\n\n    float local_max = -CUDART_INF_F;\n    int local_pos = start;\n    // Scan this lane's chunk\n    #pragma unroll\n    for (int i = 0; i < chunk; ++i) {\n        float v = top_dist[start + i];\n        if (v > local_max) {\n            local_max = v;\n            local_pos = start + i;\n        }\n    }\n\n    // Warp-wide argmax over the 32 local maxima; tie-break with larger index.\n    unsigned mask = 0xFFFFFFFFu;\n    float val = local_max;\n    int pos = local_pos;\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        float v2 = __shfl_down_sync(mask, val, offset);\n        int p2 = __shfl_down_sync(mask, pos, offset);\n        if (v2 > val || (v2 == val && p2 > pos)) {\n            val = v2;\n            pos = p2;\n        }\n    }\n    tau_out = __shfl_sync(mask, val, 0);\n    max_pos_out = __shfl_sync(mask, pos, 0);\n}\n\n// Parallel bitonic sort (ascending by distance) over the per-warp K elements stored in shared memory.\n// The 32 lanes cooperatively perform compare-exchange passes on K elements.\n// For each stride, pairs (i, i^stride) form disjoint matches; to avoid races, only the owner with i < ixj updates both elements.\nstatic __device__ __forceinline__ void warp_bitonic_sort_topk(float* __restrict__ top_dist,\n                                                              int* __restrict__ top_idx, int k) {\n    const int lane = threadIdx.x & 31;\n    const int chunk = k >> 5; // k/32\n    // Outer stage size: 2,4,8,...,k\n    for (int size = 2; size <= k; size <<= 1) {\n        // Inner stride: size/2, size/4, ..., 1\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            int i_begin = lane * chunk;\n            int i_end = i_begin + chunk;\n            for (int i = i_begin; i < i_end; ++i) {\n                int ixj = i ^ stride;\n                if (ixj > i && ixj < k) {\n                    bool up = ((i & size) == 0); // desired order for this subsequence\n                    float di = top_dist[i];\n                    float dj = top_dist[ixj];\n                    int ii = top_idx[i];\n                    int ij = top_idx[ixj];\n                    // If up==true: ensure di <= dj; else ensure di >= dj\n                    bool doSwap = (di > dj) == up;\n                    if (doSwap) {\n                        top_dist[i] = dj;\n                        top_dist[ixj] = di;\n                        top_idx[i] = ij;\n                        top_idx[ixj] = ii;\n                    }\n                }\n            }\n            __syncwarp();\n        }\n    }\n}\n\n__global__ void knn_kernel(const float2* __restrict__ query, int query_count,\n                           const float2* __restrict__ data, int data_count,\n                           std::pair<int, float>* __restrict__ result,\n                           int k, int tile_size)\n{\n    // Thread and warp identifiers\n    const int lane = threadIdx.x & 31;\n    const int warp_in_block = threadIdx.x >> 5;           // 0..warps_per_block-1\n    const int warps_per_block = blockDim.x >> 5;\n    const int query_idx = blockIdx.x * warps_per_block + warp_in_block;\n    const bool warp_active = (query_idx < query_count);\n\n    // Shared memory layout:\n    // [ warps_per_block * k ints (indices) ][ warps_per_block * k floats (distances) ][ tile_size float2 points ]\n    extern __shared__ unsigned char smem_raw[];\n    int* sm_idx = reinterpret_cast<int*>(smem_raw);\n    float* sm_dist = reinterpret_cast<float*>(sm_idx + warps_per_block * k);\n    float2* sm_tile = reinterpret_cast<float2*>(sm_dist + warps_per_block * k);\n\n    // Pointers to this warp's private K-best buffers in shared memory\n    int* top_idx = sm_idx + warp_in_block * k;\n    float* top_dist = sm_dist + warp_in_block * k;\n\n    // Initialize K-best buffers for active warps\n    if (warp_active) {\n        const int chunk = k >> 5; // k/32\n        const int start = lane * chunk;\n        #pragma unroll\n        for (int i = 0; i < chunk; ++i) {\n            top_dist[start + i] = CUDART_INF_F;\n            top_idx[start + i] = -1;\n        }\n        __syncwarp();\n    }\n\n    // Load the query point and broadcast to all lanes in the warp\n    float2 qpt = make_float2(0.f, 0.f);\n    if (warp_active) {\n        if (lane == 0) qpt = query[query_idx];\n        unsigned mask = 0xFFFFFFFFu;\n        qpt.x = __shfl_sync(mask, qpt.x, 0);\n        qpt.y = __shfl_sync(mask, qpt.y, 0);\n    }\n\n    // Current worst distance (tau) and its position in top_k\n    float tau = CUDART_INF_F;\n    int max_pos = 0;\n    if (warp_active) {\n        // Compute initial tau and max_pos (will be +inf, but gives a valid position for updates)\n        warp_recompute_tau_max(top_dist, k, tau, max_pos);\n    }\n\n    // Process the dataset in tiles cached in shared memory\n    for (int tile_start = 0; tile_start < data_count; tile_start += tile_size) {\n        const int tile_count = min(tile_size, data_count - tile_start);\n\n        // All threads in the block cooperatively load the tile\n        for (int t = threadIdx.x; t < tile_count; t += blockDim.x) {\n            sm_tile[t] = data[tile_start + t];\n        }\n        __syncthreads();\n\n        // Each active warp scans the tile with 32 candidates at a time\n        if (warp_active) {\n            for (int base = 0; base < tile_count; base += 32) {\n                const int idx_in_tile = base + lane;\n                // Compute candidate distance for this lane (or INF if out-of-range)\n                float cand_dist = CUDART_INF_F;\n                int cand_gidx = -1;\n                if (idx_in_tile < tile_count) {\n                    float2 p = sm_tile[idx_in_tile];\n                    cand_dist = sq_l2_distance(qpt, p);\n                    cand_gidx = tile_start + idx_in_tile;\n                }\n\n                // Repeatedly select and insert the smallest candidate among the 32 lanes\n                while (true) {\n                    float dmin;\n                    int minLane;\n                    warp_argmin32(cand_dist, minLane, dmin);\n                    // If the smallest candidate is not better than tau, none of the 32 are\n                    if (!(dmin < tau)) break;\n\n                    // The owning lane inserts its candidate into the K-best at position 'max_pos'\n                    if (lane == minLane) {\n                        top_dist[max_pos] = dmin;\n                        top_idx[max_pos] = cand_gidx;\n                        // Mark this lane's candidate as consumed so it won't be selected again\n                        cand_dist = CUDART_INF_F;\n                    }\n                    __syncwarp();\n\n                    // Recompute tau (the current worst in top_k) cooperatively\n                    warp_recompute_tau_max(top_dist, k, tau, max_pos);\n                    __syncwarp();\n                } // while there are improving candidates in this group\n            } // for base\n            __syncwarp();\n        } // if warp_active\n\n        __syncthreads(); // Ensure all warps completed before reusing sm_tile\n    } // tile loop\n\n    // Sort the K-best list (ascending by distance) and write results\n    if (warp_active) {\n        warp_bitonic_sort_topk(top_dist, top_idx, k);\n        __syncwarp();\n\n        // Write out sorted results: each lane writes strided positions\n        const int out_base = query_idx * k;\n        for (int j = lane; j < k; j += 32) {\n            // Store (index, distance) pair\n            result[out_base + j].first = top_idx[j];\n            result[out_base + j].second = top_dist[j];\n        }\n    }\n}\n\n// Host-side launcher. It computes a suitable configuration (warps per block and tile size),\n// sets the kernel's dynamic shared memory attribute as needed, and launches.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data, int data_count,\n             std::pair<int, float> *result, int k)\n{\n    // Prefer 8 warps per block (256 threads), fallback to fewer warps if shared memory is tight.\n    int device = 0;\n    cudaGetDevice(&device);\n    cudaDeviceProp prop{};\n    cudaGetDeviceProperties(&prop, device);\n\n    int max_dyn_shmem = 0;\n    // Hopper/Ampere support opt-in dynamic shared memory sizes; fall back to legacy if needed\n    cudaDeviceGetAttribute(&max_dyn_shmem, cudaDevAttrMaxDynamicSharedMemoryPerBlockOptin, device);\n    if (max_dyn_shmem == 0) {\n        max_dyn_shmem = prop.sharedMemPerBlockOptin ? prop.sharedMemPerBlockOptin : prop.sharedMemPerBlock;\n    }\n\n    // Choose warps per block and tile size to fit into available dynamic shared memory.\n    int warps_per_block = 0;\n    int threads_per_block = 0;\n    int tile_size = 0;\n\n    for (int w = 8; w >= 1; w >>= 1) {\n        const size_t knn_bytes = static_cast<size_t>(w) * static_cast<size_t>(k) * (sizeof(int) + sizeof(float));\n        int max_tile_points = static_cast<int>((max_dyn_shmem - (int)knn_bytes) / sizeof(float2));\n        if (max_tile_points <= 0) continue;\n\n        // Cap the tile size for good cache behavior; enforce a multiple of 32\n        int candidate_tile = max_tile_points;\n        if (candidate_tile > 4096) candidate_tile = 4096;\n        candidate_tile = (candidate_tile / 32) * 32;\n        if (candidate_tile <= 0) continue;\n\n        warps_per_block = w;\n        threads_per_block = w * 32;\n        tile_size = candidate_tile;\n        break;\n    }\n\n    if (warps_per_block == 0) {\n        // Extremely unlikely fallback\n        warps_per_block = 1;\n        threads_per_block = 32;\n        tile_size = 32;\n    }\n\n    // Compute required dynamic shared memory size\n    const size_t smem_bytes = static_cast<size_t>(warps_per_block) * static_cast<size_t>(k) * (sizeof(int) + sizeof(float))\n                            + static_cast<size_t>(tile_size) * sizeof(float2);\n\n    // Allow the kernel to use the requested dynamic shared memory size (up to the device maximum)\n    int optin_bytes = static_cast<int>(std::min(smem_bytes, static_cast<size_t>(max_dyn_shmem)));\n    cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, optin_bytes);\n\n    // Launch configuration\n    dim3 block(threads_per_block, 1, 1);\n    dim3 grid((query_count + warps_per_block - 1) / warps_per_block, 1, 1);\n\n    // Launch the kernel\n    knn_kernel<<<grid, block, smem_bytes>>>(query, query_count, data, data_count, result, k, tile_size);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 641, "completion_tokens": 22601, "total_tokens": 23242, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 19328, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db8d7408190b1fa04729cd7747d", "custom_id": "knn02-10", "response": {"status_code": 200, "request_id": "3cf94038ef085791fbe9f40dac8d1fac", "body": {"id": "chatcmpl-C4tSYcuIXM2fE21mPj5PavQBdvO6t", "object": "chat.completion", "created": 1755282534, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// This implementation assigns one warp (32 threads) to each query point.\n// Each warp maintains a private max-heap (size k) of the current best neighbors\n// in shared memory and updates it while iterating over the data points cached\n// in shared memory tiles. Warp-aggregated updates minimize contention: for each\n// batch of 32 candidates (one per lane), only the best candidates that beat the\n// current worst in the heap are inserted, one at a time, using warp-synchronous\n// operations. After processing all tiles, lane 0 of each warp heap-sorts its\n// heap to produce the k nearest neighbors in ascending order by distance and\n// writes the results to global memory.\n//\n// The code relies only on registers and shared memory; it does not allocate any\n// additional device memory. The number of threads per block is a multiple of 32,\n// and the number of warps per block is chosen on the host depending on k and the\n// available dynamic shared memory. Tiles of data points are cached in shared memory\n// and processed iteratively.\n\nnamespace {\n\n// Constants\nconstexpr int WARP_SIZE = 32;\n\n// Compute squared Euclidean distance between two float2 points.\n__device__ __forceinline__ float squared_distance(const float2 a, const float2 b) {\n    const float dx = a.x - b.x;\n    const float dy = a.y - b.y;\n    // fmaf can improve precision slightly and may map to FMA instruction.\n    return fmaf(dx, dx, dy * dy);\n}\n\n// Swap utility for paired arrays (distances and indices).\n__device__ __forceinline__ void swap_pair(float &ad, int &ai, float &bd, int &bi) {\n    float td = ad; ad = bd; bd = td;\n    int ti = ai; ai = bi; bi = ti;\n}\n\n// Sift-down operation for a max-heap stored in (dists, idxs) with length n.\n// Compares by distance; larger distances are \"greater\".\n__device__ __forceinline__ void heap_sift_down(float* dists, int* idxs, int i, int n) {\n    while (true) {\n        int l = (i << 1) + 1;\n        if (l >= n) break;\n        int r = l + 1;\n        int s = l;\n        if (r < n && dists[r] > dists[l]) s = r; // pick larger child\n        if (dists[i] >= dists[s]) break;\n        swap_pair(dists[i], idxs[i], dists[s], idxs[s]);\n        i = s;\n    }\n}\n\n// Build a max-heap from arrays (dists, idxs) of length n.\n__device__ __forceinline__ void heap_build(float* dists, int* idxs, int n) {\n    for (int i = (n >> 1) - 1; i >= 0; --i) {\n        heap_sift_down(dists, idxs, i, n);\n    }\n}\n\n// Heapsort in-place using max-heap to produce ascending order by distance.\n__device__ __forceinline__ void heap_sort_ascending(float* dists, int* idxs, int n) {\n    // Assume (dists, idxs) is already a valid max-heap of length n.\n    for (int end = n - 1; end > 0; --end) {\n        // Move current max to the end.\n        swap_pair(dists[0], idxs[0], dists[end], idxs[end]);\n        heap_sift_down(dists, idxs, 0, end);\n    }\n    // Now dists[0..n-1] is in ascending order.\n}\n\n// Warp-level argmin: among lanes with is_candidate and value 'val', find the lane\n// with the minimal value (ties broken by lower lane id). Lanes not participating\n// should pass val = +inf. Returns the winning lane id; broadcasts are performed via shuffles.\n__device__ __forceinline__ int warp_argmin_lane(float val, unsigned mask, int lane_id) {\n    float best_val = val;\n    int best_lane = lane_id;\n    // Reduce to lane 0\n    for (int offset = WARP_SIZE >> 1; offset > 0; offset >>= 1) {\n        float oth_val = __shfl_down_sync(mask, best_val, offset);\n        int oth_lane = __shfl_down_sync(mask, best_lane, offset);\n        // Prefer strictly smaller value; if equal, prefer lower lane id.\n        if (oth_val < best_val || (oth_val == best_val && oth_lane < best_lane)) {\n            best_val = oth_val;\n            best_lane = oth_lane;\n        }\n    }\n    // Broadcast the winning lane id from lane 0 to all lanes.\n    int win_lane = __shfl_sync(mask, best_lane, 0);\n    return win_lane;\n}\n\n} // anonymous namespace\n\n// Kernel: one warp processes one query. Each warp maintains a private max-heap\n// of size k in shared memory: heap_dists[warp_id_in_block][k], heap_idxs[warp_id_in_block][k].\n// The block cooperatively loads tiles of data points into shared memory.\n__global__ void knn_warp_kernel(const float2* __restrict__ query,\n                                int query_count,\n                                const float2* __restrict__ data,\n                                int data_count,\n                                std::pair<int, float>* __restrict__ result,\n                                int k,\n                                int tile_points)\n{\n    // Identify warp and lane.\n    const int lane_id = threadIdx.x & (WARP_SIZE - 1);\n    const int warp_id_in_block = threadIdx.x >> 5; // 0..warps_per_block-1\n    const int warps_per_block = blockDim.x >> 5;\n    const int warp_global_id = blockIdx.x * warps_per_block + warp_id_in_block;\n    if (warp_global_id >= query_count) return;\n\n    // Dynamic shared memory layout:\n    // [0 .. tile_points-1]: float2 tile of data points (shared across warps)\n    // next: warps_per_block * k floats for distances (heap storage per warp)\n    // next: warps_per_block * k ints for indices (heap storage per warp)\n    extern __shared__ __align__(16) unsigned char smem[];\n    float2* sh_data = reinterpret_cast<float2*>(smem);\n    unsigned char* ptr = smem + static_cast<size_t>(tile_points) * sizeof(float2);\n\n    float* all_heap_dists = reinterpret_cast<float*>(ptr);\n    ptr += static_cast<size_t>(warps_per_block) * static_cast<size_t>(k) * sizeof(float);\n\n    int* all_heap_idxs = reinterpret_cast<int*>(ptr);\n    // Pointers to this warp's private heap arrays\n    float* heap_dists = all_heap_dists + static_cast<size_t>(warp_id_in_block) * static_cast<size_t>(k);\n    int* heap_idxs = all_heap_idxs + static_cast<size_t>(warp_id_in_block) * static_cast<size_t>(k);\n\n    // Load the query point and broadcast to the warp.\n    float2 q = (lane_id == 0) ? query[warp_global_id] : make_float2(0.f, 0.f);\n    q.x = __shfl_sync(0xffffffffu, q.x, 0);\n    q.y = __shfl_sync(0xffffffffu, q.y, 0);\n\n    // Initialize the heap:\n    // Lane 0 computes distances to the first k data points and builds a max-heap.\n    if (lane_id == 0) {\n        // Seed with the first k data points (data_count >= k by assumption).\n        for (int i = 0; i < k; ++i) {\n            float2 p = data[i];\n            float d = squared_distance(p, q);\n            heap_dists[i] = d;\n            heap_idxs[i] = i;\n        }\n        // Build max-heap so that heap_dists[0] is the current worst (largest) distance in the top-k.\n        heap_build(heap_dists, heap_idxs, k);\n    }\n    __syncwarp();\n\n    // Process data in tiles cached in shared memory.\n    // All warps in the block cooperate to load the tile; each warp then iterates over the tile.\n    for (int tile_start = 0; tile_start < data_count; tile_start += tile_points) {\n        const int count = min(tile_points, data_count - tile_start);\n\n        // Block-wide load of 'count' points into shared memory tile.\n        for (int i = threadIdx.x; i < count; i += blockDim.x) {\n            sh_data[i] = data[tile_start + i];\n        }\n        __syncthreads();\n\n        // Each warp processes the tile in batches of 32 points (one per lane).\n        for (int base = 0; base < count; base += WARP_SIZE) {\n            const int j = base + lane_id;\n\n            // Compute candidate distance and global index for this lane (or +inf if out of bounds).\n            float cand_dist = (j < count) ? squared_distance(sh_data[j], q) : CUDART_INF_F;\n            int cand_idx = tile_start + j;\n\n            // Warp-aggregated heap update:\n            // Only candidates that beat the current worst (heap root) need to be inserted.\n            // Insert them one at a time in ascending order of distance among lanes.\n            while (true) {\n                // Read current heap worst distance (max).\n                const float heap_worst = heap_dists[0];\n                unsigned mask = __activemask();\n                // Identify lanes with candidates that beat the current worst.\n                unsigned active_mask = __ballot_sync(mask, cand_dist < heap_worst);\n                if (active_mask == 0u) {\n                    break; // No lane has a candidate that beats current worst.\n                }\n\n                // Compute the lane with the minimal candidate distance among the active lanes.\n                // Lanes not active contribute +inf and never win.\n                float val = (cand_dist < heap_worst) ? cand_dist : CUDART_INF_F;\n                int winner_lane = warp_argmin_lane(val, mask, lane_id);\n\n                // The winning lane updates the heap root and sifts down.\n                if (lane_id == winner_lane) {\n                    heap_dists[0] = cand_dist;\n                    heap_idxs[0] = cand_idx;\n                    heap_sift_down(heap_dists, heap_idxs, 0, k);\n                    // Mark this candidate as consumed to avoid re-inserting it.\n                    cand_dist = CUDART_INF_F;\n                }\n                __syncwarp(mask);\n                // Loop continues to see if other lanes still beat the new worst.\n            }\n        }\n\n        __syncthreads();\n    }\n\n    // Finalize: lane 0 of the warp sorts the heap ascending and writes out results.\n    if (lane_id == 0) {\n        // Heapsort to ascending order.\n        heap_sort_ascending(heap_dists, heap_idxs, k);\n\n        // Write results for this query in row-major order: result[q * k + j]\n        const int out_base = warp_global_id * k;\n        for (int j = 0; j < k; ++j) {\n            const int idx = heap_idxs[j];\n            const float d = heap_dists[j];\n            result[out_base + j].first = idx;\n            result[out_base + j].second = d;\n        }\n    }\n}\n\n// Host interface.\n// Determines a good launch configuration based on k and the device's dynamic shared memory,\n// launches the kernel, and ensures the shared memory budget is sufficient.\n//\n// - Each warp processes one query.\n// - The number of warps per block is chosen to keep the per-block shared memory within\n//   the opt-in maximum and leave room for a reasonably large tile.\n// - The tile size is chosen to maximize use of shared memory while leaving space for the\n//   per-warp heaps. It is also limited to a reasonable upper bound to avoid excessive\n//   per-block shared memory that could hurt occupancy.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data, int data_count,\n             std::pair<int, float> *result, int k)\n{\n    if (query_count <= 0) return;\n\n    int device = 0;\n    cudaGetDevice(&device);\n\n    int max_default_smem = 0;\n    int max_optin_smem = 0;\n    cudaDeviceGetAttribute(&max_default_smem, cudaDevAttrMaxSharedMemoryPerBlock, device);\n    cudaDeviceGetAttribute(&max_optin_smem, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    // Fall back to default if opt-in is unavailable (shouldn't happen on A100/H100).\n    if (max_optin_smem <= 0) max_optin_smem = max_default_smem;\n\n    // Choose warps per block. Start from 4 and adjust down if needed to fit shared memory.\n    int warps_per_block = 4; // 128 threads per block by default (good balance).\n    const int max_warps = 8; // Cap to 8 warps/block (256 threads).\n    // Increase warps per block if k is small to improve latency hiding, but stay within smem.\n    if (k <= 128) warps_per_block = 8;\n    else if (k <= 256) warps_per_block = 6;\n\n    if (warps_per_block > max_warps) warps_per_block = max_warps;\n    if (warps_per_block < 1) warps_per_block = 1;\n\n    // Compute per-block shared memory requirements: per-warp heaps + tile.\n    auto compute_smem = [&](int wpb, int tile_pts) -> size_t {\n        size_t heap_bytes = static_cast<size_t>(wpb) * static_cast<size_t>(k) * (sizeof(float) + sizeof(int));\n        size_t tile_bytes = static_cast<size_t>(tile_pts) * sizeof(float2);\n        return heap_bytes + tile_bytes;\n    };\n\n    // Target a reasonably large tile to amortize loads. We'll adjust to fit in shared memory.\n    int tile_points = 8192; // 8192 points -> 64 KiB tile\n    // Ensure at least some tile size.\n    if (tile_points < WARP_SIZE) tile_points = WARP_SIZE;\n\n    // Adjust warps_per_block and tile_points to fit within opt-in shared memory.\n    while (true) {\n        size_t heap_bytes = static_cast<size_t>(warps_per_block) * static_cast<size_t>(k) * (sizeof(float) + sizeof(int));\n        // Ensure tile points non-negative.\n        int max_tile_points_fit = 0;\n        if (max_optin_smem > static_cast<int>(heap_bytes)) {\n            max_tile_points_fit = (max_optin_smem - static_cast<int>(heap_bytes)) / static_cast<int>(sizeof(float2));\n        }\n        // Clamp tile_points to what fits, but try to keep it reasonably large.\n        int desired_tile_points = tile_points;\n        if (desired_tile_points > max_tile_points_fit) desired_tile_points = max_tile_points_fit;\n        if (desired_tile_points < WARP_SIZE) {\n            // Reduce warps per block if tile cannot fit reasonably.\n            if (warps_per_block > 1) {\n                warps_per_block >>= 1;\n                continue;\n            } else {\n                desired_tile_points = max_tile_points_fit > 0 ? max_tile_points_fit : WARP_SIZE;\n            }\n        }\n\n        tile_points = desired_tile_points;\n        break;\n    }\n\n    // Limit tile_points to avoid excessive per-block SMEM that might reduce occupancy too much,\n    // but ensure it's at least a few warps worth.\n    const int threads_per_block = warps_per_block * WARP_SIZE;\n    // Round tile_points down to a multiple of warp size to simplify batches (optional).\n    if (tile_points > WARP_SIZE) {\n        tile_points = (tile_points / WARP_SIZE) * WARP_SIZE;\n    } else {\n        tile_points = WARP_SIZE;\n    }\n\n    size_t smem_bytes = compute_smem(warps_per_block, tile_points);\n\n    // Opt-in to the requested dynamic shared memory size.\n    cudaFuncSetAttribute(knn_warp_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, static_cast<int>(smem_bytes));\n\n    // Launch configuration: one warp per query.\n    const int num_blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    knn_warp_kernel<<<num_blocks, threads_per_block, smem_bytes>>>(\n        query, query_count, data, data_count, result, k, tile_points\n    );\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 641, "completion_tokens": 19343, "total_tokens": 19984, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 15744, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db72b988190aa1846ef33a39d47", "custom_id": "knn03-01", "response": {"status_code": 200, "request_id": "bd0a5b9b3ad4c9561c1a668fa090639c", "body": {"id": "chatcmpl-C4sg78sCQ4v4uHVlqbpl8HyQx7ydz", "object": "chat.completion", "created": 1755279531, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n#include <math_constants.h>\n\n// Optimized k-NN for 2D points using one warp per query.\n// - Data is processed in tiles cached in shared memory by the whole block.\n// - Each warp maintains a private top-k (sorted ascending by distance) in shared memory.\n// - Candidates are inserted cooperatively by the warp to avoid races.\n// - k is assumed to be a power of two in [32, 1024].\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Tunable hyper-parameters chosen for modern data-center GPUs (A100/H100).\n// - WARPS_PER_BLOCK: number of queries processed concurrently per block.\n// - TILE_POINTS: number of data points cached per tile.\nconstexpr int WARPS_PER_BLOCK     = 8;                    // 8 warps per block = 256 threads\nconstexpr int THREADS_PER_BLOCK   = WARPS_PER_BLOCK * WARP_SIZE;\nconstexpr int TILE_POINTS         = 4096;                 // 4096 points per tile => 32 KiB tile (float2)\n\nstatic __device__ __forceinline__ float sq_distance_2d(const float2 a, const float2 b) {\n    // Squared Euclidean distance between 2D points.\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    return fmaf(dx, dx, dy * dy);\n}\n\n// Lower-bound position in ascending array 'arr' of length 'n' for value 'x'.\n// Returns the first index i such that arr[i] >= x.\n// For arrays filled with +inf initially, this will return 0 for any finite x.\nstatic __device__ __forceinline__ int lower_bound_asc(const float* arr, int n, float x) {\n    int l = 0, r = n;\n#pragma unroll 1\n    while (l < r) {\n        int m = (l + r) >> 1;\n        float v = arr[m];\n        if (x <= v) {\n            r = m;\n        } else {\n            l = m + 1;\n        }\n    }\n    return l;\n}\n\n// Cooperative warp insertion of multiple candidates into a shared, ascending top-k array.\n// Each lane provides at most one candidate (dist, index). The function will:\n// - Iteratively select one of the lanes whose candidate qualifies (dist < current worst),\n// - Compute the insertion position,\n// - Shift the tail [pos, k-2] to the right by one cooperatively across the warp,\n// - Insert the candidate at position 'pos',\n// - Repeat until no remaining qualifying candidates.\n// Notes:\n// - top_dists/top_idx: pointers to the warp's private top-k arrays in shared memory.\n// - k: size of the top-k.\n// - dist, idx: the calling lane's candidate. Lanes without candidate can pass dist=+inf or rely on the active flag.\nstatic __device__ __forceinline__ void warp_cooperative_insert(float* __restrict__ top_dists,\n                                                               int*   __restrict__ top_idx,\n                                                               int k,\n                                                               float dist,\n                                                               int idx,\n                                                               unsigned full_mask)\n{\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n    // 'active' indicates this lane still has a candidate to try inserting.\n    // First-level filter: must be better than the current worst.\n    bool active = (dist < top_dists[k - 1]);\n\n    // Process insertions one by one, prioritizing lower lane indices among qualifiers.\n#pragma unroll 1\n    while (true) {\n        // Re-evaluate qualification against potentially updated worst value.\n        bool qualifies = active && (dist < top_dists[k - 1]);\n        unsigned mask = __ballot_sync(full_mask, qualifies);\n        if (mask == 0)\n            break;\n\n        int leader_lane = __ffs(mask) - 1;\n\n        // Broadcast the leader's candidate (dist, idx) to the whole warp.\n        float cand_dist = __shfl_sync(full_mask, dist, leader_lane);\n        int   cand_idx  = __shfl_sync(full_mask, idx,  leader_lane);\n\n        // Leader computes insertion position (lower_bound) in the ascending array.\n        int pos = 0;\n        if (lane == leader_lane) {\n            pos = lower_bound_asc(top_dists, k, cand_dist);\n        }\n        pos = __shfl_sync(full_mask, pos, leader_lane);\n\n        // Cooperative right-shift by 1: move [pos .. k-2] -> [pos+1 .. k-1].\n        // Process from high to low indices to avoid overwrite hazards.\n        __syncwarp(full_mask);\n        for (int j = (k - 2) - lane; j >= pos; j -= WARP_SIZE) {\n            top_dists[j + 1] = top_dists[j];\n            top_idx[j + 1]   = top_idx[j];\n        }\n        __syncwarp(full_mask);\n\n        // Single lane writes the new element; any lane can do it, choose lane 0.\n        if (lane == 0) {\n            top_dists[pos] = cand_dist;\n            top_idx[pos]   = cand_idx;\n        }\n        __syncwarp(full_mask);\n\n        // The leader's candidate has been inserted; mark it consumed.\n        if (lane == leader_lane) {\n            active = false;\n        }\n    }\n}\n\n// CUDA kernel: one warp (32 threads) computes the k nearest neighbors for one query point.\n__global__ void knn_kernel_warp_per_query(const float2* __restrict__ query,\n                                          int query_count,\n                                          const float2* __restrict__ data,\n                                          int data_count,\n                                          std::pair<int, float>* __restrict__ result,\n                                          int k)\n{\n    extern __shared__ unsigned char smem_raw[];\n\n    // Shared memory layout:\n    // [0 .. TILE_POINTS*sizeof(float2)) : shared cache of data tile\n    // Next: WARPS_PER_BLOCK*k floats for top-k distances of each warp\n    // Next: WARPS_PER_BLOCK*k ints for top-k indices of each warp\n    float2* tile_points = reinterpret_cast<float2*>(smem_raw);\n    float*  all_top_dists = reinterpret_cast<float*>(smem_raw + TILE_POINTS * sizeof(float2));\n    int*    all_top_idx   = reinterpret_cast<int*>(reinterpret_cast<unsigned char*>(all_top_dists) + WARPS_PER_BLOCK * (size_t)k * sizeof(float));\n\n    const int tid   = threadIdx.x;\n    const int lane  = tid & (WARP_SIZE - 1);\n    const int warp  = tid >> 5; // warp id within block [0 .. WARPS_PER_BLOCK-1]\n    const unsigned FULL_MASK = 0xffffffffu;\n\n    // Determine which query this warp is responsible for.\n    const int warp_global = blockIdx.x * WARPS_PER_BLOCK + warp;\n    const bool warp_active = (warp_global < query_count);\n\n    // Pointers to the warp's private top-k buffers in shared memory.\n    float* warp_top_dists = all_top_dists + warp * (size_t)k;\n    int*   warp_top_idx   = all_top_idx   + warp * (size_t)k;\n\n    // Load query point for this warp and broadcast to all lanes.\n    float2 q = make_float2(0.0f, 0.0f);\n    if (warp_active && lane == 0) {\n        q = query[warp_global];\n    }\n    q.x = __shfl_sync(FULL_MASK, q.x, 0);\n    q.y = __shfl_sync(FULL_MASK, q.y, 0);\n\n    // Initialize the warp's top-k arrays (ascending order; fill with +inf distances and -1 indices).\n    if (warp_active) {\n#pragma unroll 1\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            warp_top_dists[i] = CUDART_INF_F;\n            warp_top_idx[i]   = -1;\n        }\n    }\n    __syncwarp(FULL_MASK); // ensure a consistent initial state per warp\n\n    // Iterate over data in tiles cached into shared memory.\n    for (int tile_base = 0; tile_base < data_count; tile_base += TILE_POINTS) {\n        const int tile_count = min(TILE_POINTS, data_count - tile_base);\n\n        // Cooperative load of the tile into shared memory by the whole block.\n        for (int i = tid; i < tile_count; i += blockDim.x) {\n            tile_points[i] = data[tile_base + i];\n        }\n        __syncthreads();\n\n        // Each active warp processes the tile, each lane strides by 32 within the tile.\n        if (warp_active) {\n#pragma unroll 1\n            for (int j = lane; j < tile_count; j += WARP_SIZE) {\n                float2 p = tile_points[j];\n                float dist = sq_distance_2d(q, p);\n                int   idx  = tile_base + j;\n\n                // Cooperative insertion across the warp for candidates that beat the current worst.\n                // Each lane participates to avoid divergence around warp-level synchronization.\n                warp_cooperative_insert(warp_top_dists, warp_top_idx, k, dist, idx, FULL_MASK);\n            }\n        }\n\n        __syncthreads(); // ensure all warps finished using the current tile before reloading\n    }\n\n    // Write results for this warp's query as (index, distance) pairs in ascending order of distance.\n    if (warp_active) {\n        const int out_base = warp_global * k;\n#pragma unroll 1\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            // std::pair is a simple POD; directly assign to its fields.\n            result[out_base + i].first  = warp_top_idx[i];\n            result[out_base + i].second = warp_top_dists[i];\n        }\n    }\n}\n\n// Public C++ interface. Allocations are assumed to be done via cudaMalloc.\n// This function configures and launches the kernel with carefully chosen parameters.\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k)\n{\n    if (query_count <= 0 || data_count <= 0 || k <= 0) {\n        return;\n    }\n\n    // Kernel launch configuration.\n    dim3 block(THREADS_PER_BLOCK, 1, 1);\n    // One warp per query. Each block processes WARPS_PER_BLOCK queries.\n    int blocks = (query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK;\n    if (blocks <= 0) blocks = 1;\n    dim3 grid(blocks, 1, 1);\n\n    // Dynamic shared memory size per block:\n    // - TILE_POINTS float2 for the cached tile\n    // - WARPS_PER_BLOCK * k floats for distances\n    // - WARPS_PER_BLOCK * k ints for indices\n    size_t shmem_bytes = TILE_POINTS * sizeof(float2)\n                       + WARPS_PER_BLOCK * (size_t)k * sizeof(float)\n                       + WARPS_PER_BLOCK * (size_t)k * sizeof(int);\n\n    // Opt-in larger shared memory carveout and dynamic shared memory limit where supported.\n    // These calls are best-effort; ignore errors if unsupported (e.g., on older devices).\n    cudaFuncSetAttribute(knn_kernel_warp_per_query,\n                         cudaFuncAttributePreferredSharedMemoryCarveout,\n                         100);\n    cudaFuncSetAttribute(knn_kernel_warp_per_query,\n                         cudaFuncAttributeMaxDynamicSharedMemorySize,\n                         (int)shmem_bytes);\n\n    // Launch the kernel.\n    knn_kernel_warp_per_query<<<grid, block, shmem_bytes>>>(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 686, "completion_tokens": 18518, "total_tokens": 19204, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 16000, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db7389c819083f4b6a5f834f4ff", "custom_id": "knn03-02", "response": {"status_code": 200, "request_id": "3bf2096962448a88e6a49cbbb10db2da", "body": {"id": "chatcmpl-C4sg6DIYPR3gH8OyzcDsKuGUX3EVT", "object": "chat.completion", "created": 1755279530, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// This file implements a CUDA kernel for k-nearest neighbors (k-NN) of 2D points,\n// optimized for modern NVIDIA data center GPUs (A100/H100).\n//\n// Key design points:\n// - One warp (32 threads) processes one query point.\n// - Data points are processed in tiles cached in shared memory to minimize global memory traffic.\n// - Each warp maintains a private max-heap of size k (in shared memory) holding the current k nearest neighbors.\n//   The heap keeps the largest (worst) distance at the root so new candidates only trigger updates when they beat the current worst.\n// - Warp threads cooperatively compute distances; new candidates are inserted into the heap by lane 0 using warp shuffles.\n// - After processing all data, the heap is popped to produce k nearest neighbors in ascending order of distance.\n// - The kernel uses dynamic shared memory sized at launch based on k and the chosen tile size.\n// - The result is written as std::pair<int, float> but we avoid using std facilities in device code by reinterpreting it as a POD struct.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// POD pair with the same field layout as std::pair<int, float> (first, second).\n// We avoid using std::pair operations in device code and only write raw fields.\nstruct PairIF {\n    int first;\n    float second;\n};\n\nstatic_assert(sizeof(PairIF) == sizeof(std::pair<int, float>), \"PairIF must match std::pair<int, float> size\");\n\n// Max-heap helper functions (lane 0 only).\n// The heap stores distances (float) and their corresponding indices (int) in parallel arrays.\n// Invariants: For a max-heap, parent >= children. The root at index 0 is the largest distance.\n\n// Sift-up during insertion to maintain max-heap property.\n__device__ __forceinline__\nvoid heap_sift_up(float* __restrict__ dist, int* __restrict__ idx, int pos, float val, int id)\n{\n    while (pos > 0) {\n        int parent = (pos - 1) >> 1;\n        float parentVal = dist[parent];\n        if (parentVal >= val) break;\n        dist[pos] = parentVal;\n        idx[pos] = idx[parent];\n        pos = parent;\n    }\n    dist[pos] = val;\n    idx[pos] = id;\n}\n\n// Insert a (distance, index) pair into the heap of current size 'size' (size < k).\n__device__ __forceinline__\nvoid heap_insert_max(float* __restrict__ dist, int* __restrict__ idx, int size, float val, int id)\n{\n    heap_sift_up(dist, idx, size, val, id);\n}\n\n// Sift-down during root replacement to maintain max-heap property.\n__device__ __forceinline__\nvoid heap_sift_down(float* __restrict__ dist, int* __restrict__ idx, int size, float val, int id)\n{\n    int pos = 0;\n    int child = 1;\n    while (child < size) {\n        int right = child + 1;\n        int largestChild = (right < size && dist[right] > dist[child]) ? right : child;\n        float lcVal = dist[largestChild];\n        if (lcVal <= val) break;\n        dist[pos] = lcVal;\n        idx[pos] = idx[largestChild];\n        pos = largestChild;\n        child = (pos << 1) + 1;\n    }\n    dist[pos] = val;\n    idx[pos] = id;\n}\n\n// Replace the root (largest) value with a new smaller value and restore heap property.\n__device__ __forceinline__\nvoid heap_replace_root(float* __restrict__ dist, int* __restrict__ idx, int size, float val, int id)\n{\n    // Precondition: val < dist[0] and size > 0\n    heap_sift_down(dist, idx, size, val, id);\n}\n\n__global__ void knn2d_kernel(\n    const float2* __restrict__ query,\n    int query_count,\n    const float2* __restrict__ data,\n    int data_count,\n    int k,\n    PairIF* __restrict__ result,\n    int tile_points)\n{\n    // Dynamic shared memory layout:\n    // [ float2 s_data[tile_points] ][ float s_heapDist[WARPS_PER_BLOCK * k] ][ int s_heapIdx[WARPS_PER_BLOCK * k] ]\n    extern __shared__ unsigned char smem[];\n    float2* s_data = reinterpret_cast<float2*>(smem);\n\n    const int threads_per_block = blockDim.x;\n    const int warps_per_block = threads_per_block / WARP_SIZE;\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n    const int warp = threadIdx.x >> 5;\n    const int warp_global = blockIdx.x * warps_per_block + warp;\n\n    // Compute shared memory offsets for per-warp heaps.\n    size_t off = static_cast<size_t>(tile_points) * sizeof(float2);\n    // Align to 8-byte boundary to avoid misaligned accesses.\n    off = (off + 7) & ~size_t(7);\n\n    float* s_heapDist_base = reinterpret_cast<float*>(smem + off);\n    int*   s_heapIdx_base  = reinterpret_cast<int*>(s_heapDist_base + warps_per_block * k);\n\n    float* s_heapDist = s_heapDist_base + warp * k;\n    int*   s_heapIdx  = s_heapIdx_base  + warp * k;\n\n    // All warps must participate in __syncthreads, even if they have no query to process.\n    // So we guard computations but not barriers.\n    const bool warp_has_query = (warp_global < query_count);\n\n    // Initialize per-warp heap size and worst distance (tau).\n    int heapSize = 0;\n    float tau = __int_as_float(0x7f800000); // +inf\n\n    // Each warp's query point loaded once.\n    float2 q = make_float2(0.0f, 0.0f);\n    if (warp_has_query) {\n        q = query[warp_global];\n    }\n\n    // Process data in tiles cached in shared memory.\n    for (int tile_start = 0; tile_start < data_count; tile_start += tile_points) {\n        int tile_count = data_count - tile_start;\n        if (tile_count > tile_points) tile_count = tile_points;\n\n        // Load tile into shared memory cooperatively by the block.\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            s_data[i] = data[tile_start + i];\n        }\n        __syncthreads();\n\n        // Each warp processes the tile for its query.\n        if (warp_has_query) {\n            // Iterate over points in the tile, each lane handles a strided subset.\n            for (int i = lane; i < tile_count; i += WARP_SIZE) {\n                float2 p = s_data[i];\n                float dx = p.x - q.x;\n                float dy = p.y - q.y;\n                // Fused multiply-adds to compute squared distance.\n                float dist = fmaf(dx, dx, dy * dy);\n                int gidx = tile_start + i;\n\n                // Broadcast current tau and heapSize from lane 0 to all lanes so that each lane can do a quick precheck.\n                unsigned mask_full = __activemask();\n                float tau_bcast = __shfl_sync(mask_full, tau, 0);\n                int heapSize_bcast = __shfl_sync(mask_full, heapSize, 0);\n\n                // Precheck to reduce number of candidates reaching the heap:\n                // - If heap is not yet full, we must insert.\n                // - Else insert only if dist < current worst (tau).\n                int needs_insert = (heapSize_bcast < k) || (dist < tau_bcast);\n\n                unsigned insert_mask = __ballot_sync(mask_full, needs_insert);\n\n                // Process all lanes that need insertion. Lane 0 performs heap updates.\n                while (insert_mask) {\n                    int lead_lane = __ffs(insert_mask) - 1;\n                    float dist_lead = __shfl_sync(mask_full, dist, lead_lane);\n                    int   idx_lead  = __shfl_sync(mask_full, gidx, lead_lane);\n\n                    if (lane == 0) {\n                        if (heapSize < k) {\n                            // Insert directly until heap is full.\n                            heap_insert_max(s_heapDist, s_heapIdx, heapSize, dist_lead, idx_lead);\n                            heapSize++;\n                            // Update tau (worst distance) when heap becomes full.\n                            if (heapSize == k) {\n                                tau = s_heapDist[0];\n                            }\n                        } else if (dist_lead < s_heapDist[0]) {\n                            // Replace root with better candidate and fix heap.\n                            heap_replace_root(s_heapDist, s_heapIdx, heapSize, dist_lead, idx_lead);\n                            tau = s_heapDist[0];\n                        }\n                    }\n                    insert_mask &= (insert_mask - 1);\n                }\n            }\n        }\n\n        __syncthreads(); // Ensure all warps finished reading s_data before the next tile is loaded.\n    }\n\n    // After processing all data, write out results sorted in ascending order of distance.\n    if (warp_has_query && lane == 0) {\n        // Pop max-heap repeatedly into output from the end to the beginning.\n        int qbase = warp_global * k;\n        int size = heapSize; // Should be equal to k (since data_count >= k).\n        // If for any reason heapSize < k (e.g., pathological), fill the rest with +inf/-1.\n        while (size > 0) {\n            // Extract max (largest distance).\n            float topd = s_heapDist[0];\n            int   topi = s_heapIdx[0];\n\n            // Move last to root and sift down.\n            int last = size - 1;\n            float lastd = s_heapDist[last];\n            int   lasti = s_heapIdx[last];\n            size--;\n            if (size > 0) {\n                heap_sift_down(s_heapDist, s_heapIdx, size, lastd, lasti);\n            }\n\n            // Place the extracted element at the end of the sorted range.\n            int out_pos = qbase + size; // as size decreases, out_pos goes from qbase + k - 1 down to qbase\n            // Writing as POD PairIF ensures device-side compatibility.\n            result[out_pos].first = topi;\n            result[out_pos].second = topd;\n        }\n\n        // If heapSize < k (shouldn't happen given data_count >= k), pad remaining entries with invalid index and +inf.\n        for (int r = heapSize; r < k; ++r) {\n            int out_pos = qbase + r;\n            result[out_pos].first = -1;\n            result[out_pos].second = __int_as_float(0x7f800000); // +inf\n        }\n    }\n}\n\n// Host function to launch the kernel.\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k)\n{\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    const int threads_per_block = 256; // 8 warps per block for good balance of occupancy and shared memory usage.\n    const int warps_per_block = threads_per_block / WARP_SIZE;\n    const int grid_blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Determine a tile size that fits into available dynamic shared memory along with per-warp heaps.\n    int device = 0;\n    cudaGetDevice(&device);\n\n    int max_smem_optin = 0;\n    cudaDeviceGetAttribute(&max_smem_optin, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    if (max_smem_optin == 0) {\n        // Fallback to default maximum if opt-in attribute is unavailable.\n        cudaDeviceGetAttribute(&max_smem_optin, cudaDevAttrMaxSharedMemoryPerBlock, device);\n    }\n\n    size_t per_warp_heap_bytes = static_cast<size_t>(k) * (sizeof(float) + sizeof(int));\n    size_t heaps_bytes = static_cast<size_t>(warps_per_block) * per_warp_heap_bytes;\n\n    // Choose the largest tile_points that fits in shared memory budget.\n    // Reserve some safety margin (e.g., 1024 bytes) for alignment overhead.\n    size_t safety = 1024;\n    int tile_points = 0;\n    if (max_smem_optin > static_cast<int>(heaps_bytes + safety)) {\n        size_t available = static_cast<size_t>(max_smem_optin) - heaps_bytes - safety;\n        tile_points = static_cast<int>(available / sizeof(float2));\n    }\n    // Ensure tile_points is sensible and aligned for coalesced loads.\n    if (tile_points <= 0) tile_points = 1024; // minimal fallback\n    // Align down to a multiple of threads_per_block to make loads coalesced across block.\n    tile_points = (tile_points / threads_per_block) * threads_per_block;\n    if (tile_points <= 0) tile_points = threads_per_block; // at least one block stride\n\n    // Compute dynamic shared memory size.\n    size_t smem_bytes = static_cast<size_t>(tile_points) * sizeof(float2) + heaps_bytes;\n    // Try to set the kernel attribute to allow this much dynamic shared memory.\n    cudaFuncSetAttribute(knn2d_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)smem_bytes);\n\n    // Reinterpret the result pointer as a POD to avoid std::pair device code usage.\n    PairIF* result_pod = reinterpret_cast<PairIF*>(result);\n\n    // Launch the kernel.\n    knn2d_kernel<<<grid_blocks, threads_per_block, smem_bytes>>>(query, query_count, data, data_count, k, result_pod, tile_points);\n\n    // Note: Error checking (cudaGetLastError/cudaDeviceSynchronize) is intentionally omitted as per instruction\n    // to keep the code focused on the core kernel logic and to avoid extra host-side behavior.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 686, "completion_tokens": 25090, "total_tokens": 25776, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 22016, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db73d548190b6c20cc3cfe7576b", "custom_id": "knn03-03", "response": {"status_code": 200, "request_id": "5773226862890a941df98cbec7632212", "body": {"id": "chatcmpl-C4sg8Y7aVX18WVGtyIzPh2znIJi2o", "object": "chat.completion", "created": 1755279532, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// Optimized k-NN for 2D points using warp-per-query, shared-memory tiling, and a per-warp max-heap.\n// - Each warp (32 threads) processes one query.\n// - Data points are processed in shared-memory tiles to maximize cache reuse.\n// - Each warp maintains its private top-k as a max-heap in shared memory.\n// - Distance is squared Euclidean.\n// - At the end, each warp heap-sorts its heap into ascending order and writes results.\n//\n// Hyperparameters:\n// - WARPS_PER_BLOCK = 8 (block size 256 threads)\n// - TILE_POINTS = 4096 (32 KB tile for float2)\n// Shared memory usage per block:\n// - Tile: 32 KB\n// - Per-warp top-k buffers: WARPS_PER_BLOCK * k * (4 + 4) bytes\n// - Per-warp control ints: WARPS_PER_BLOCK * 2 * 4 bytes\n// For k=1024, WARPS_PER_BLOCK=8 => 96 KB + 64 bytes control ~= 96 KB total, fits A100 (opt-in) and H100.\n\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 8\n#endif\n\n#ifndef TILE_POINTS\n#define TILE_POINTS 4096\n#endif\n\n// Lane id within a warp\nstatic __device__ __forceinline__ int lane_id() { return threadIdx.x & 31; }\n// Warp id within a block\nstatic __device__ __forceinline__ int warp_id_in_block() { return threadIdx.x >> 5; }\n\n// Squared L2 distance between q=(qx,qy) and p=(px,py)\nstatic __device__ __forceinline__ float sq_l2(const float qx, const float qy, const float2 p) {\n    float dx = qx - p.x;\n    float dy = qy - p.y;\n    return fmaf(dx, dx, dy * dy);\n}\n\n// Max-heap sift-down (heap stored in dist/idx arrays)\n// Maintains max-heap property for [0..length-1] starting at index 'start'\nstatic __device__ __forceinline__ void heap_sift_down(float* dist, int* idx, int start, int length) {\n    int root = start;\n    while (true) {\n        int left = (root << 1) + 1;\n        if (left >= length) break;\n        int right = left + 1;\n        int swap_idx = root;\n        if (dist[swap_idx] < dist[left]) swap_idx = left;\n        if (right < length && dist[swap_idx] < dist[right]) swap_idx = right;\n        if (swap_idx == root) break;\n        float td = dist[root];\n        dist[root] = dist[swap_idx];\n        dist[swap_idx] = td;\n        int ti = idx[root];\n        idx[root] = idx[swap_idx];\n        idx[swap_idx] = ti;\n        root = swap_idx;\n    }\n}\n\n// Build max-heap from arbitrary array contents\nstatic __device__ __forceinline__ void heap_build(float* dist, int* idx, int length) {\n    for (int i = (length >> 1) - 1; i >= 0; --i) {\n        heap_sift_down(dist, idx, i, length);\n    }\n}\n\n// Insert candidate (d, id) into max-heap of size k if better than current worst\nstatic __device__ __forceinline__ void heap_insert_if_better(float* dist, int* idx, int k, float d, int id) {\n    // Root holds current worst (maximum distance)\n    if (d < dist[0]) {\n        dist[0] = d;\n        idx[0] = id;\n        heap_sift_down(dist, idx, 0, k);\n    }\n}\n\n// In-place heapsort on max-heap (dist/idx), produces ascending order by distance\nstatic __device__ __forceinline__ void heap_sort_ascending(float* dist, int* idx, int length) {\n    for (int end = length - 1; end > 0; --end) {\n        // Move current max to the end\n        float td = dist[0];\n        dist[0] = dist[end];\n        dist[end] = td;\n        int ti = idx[0];\n        idx[0] = idx[end];\n        idx[end] = ti;\n        // Restore heap property on reduced heap\n        heap_sift_down(dist, idx, 0, end);\n    }\n    // After this, dist[0..length-1] is ascending\n}\n\nextern \"C\" __global__ void knn_kernel(const float2* __restrict__ query,\n                                      int query_count,\n                                      const float2* __restrict__ data,\n                                      int data_count,\n                                      std::pair<int, float>* __restrict__ result,\n                                      int k)\n{\n    // Dynamic shared memory layout:\n    // [ tile (TILE_POINTS float2) ]\n    // [ all_idx (WARPS_PER_BLOCK * k int) ]\n    // [ all_dist (WARPS_PER_BLOCK * k float) ]\n    // [ warp_fill (WARPS_PER_BLOCK int) ]\n    // [ warp_heap_built (WARPS_PER_BLOCK int) ]\n    extern __shared__ unsigned char smem_raw[];\n    size_t smem_off = 0;\n\n    // Tile buffer\n    float2* tile = reinterpret_cast<float2*>(smem_raw + smem_off);\n    smem_off += size_t(TILE_POINTS) * sizeof(float2);\n\n    // Align to int\n    smem_off = (smem_off + alignof(int) - 1) & ~(size_t(alignof(int)) - 1);\n    int* all_idx = reinterpret_cast<int*>(smem_raw + smem_off);\n    smem_off += size_t(WARPS_PER_BLOCK) * size_t(k) * sizeof(int);\n\n    // Align to float\n    smem_off = (smem_off + alignof(float) - 1) & ~(size_t(alignof(float)) - 1);\n    float* all_dist = reinterpret_cast<float*>(smem_raw + smem_off);\n    smem_off += size_t(WARPS_PER_BLOCK) * size_t(k) * sizeof(float);\n\n    // Per-warp control variables\n    smem_off = (smem_off + alignof(int) - 1) & ~(size_t(alignof(int)) - 1);\n    int* warp_fill = reinterpret_cast<int*>(smem_raw + smem_off);\n    smem_off += size_t(WARPS_PER_BLOCK) * sizeof(int);\n\n    smem_off = (smem_off + alignof(int) - 1) & ~(size_t(alignof(int)) - 1);\n    int* warp_heap_built = reinterpret_cast<int*>(smem_raw + smem_off);\n    smem_off += size_t(WARPS_PER_BLOCK) * sizeof(int);\n\n    const int lane = lane_id();\n    const int warp = warp_id_in_block();\n    const int warp_in_grid = blockIdx.x * WARPS_PER_BLOCK + warp;\n\n    // Pointers to this warp's private top-k buffers\n    int* w_idx = all_idx + warp * k;\n    float* w_dist = all_dist + warp * k;\n\n    // Initialize control variables\n    if (threadIdx.x < WARPS_PER_BLOCK) {\n        warp_fill[threadIdx.x] = 0;\n        warp_heap_built[threadIdx.x] = 0;\n    }\n    __syncthreads();\n\n    // Load query point and broadcast within warp\n    float qx = 0.0f, qy = 0.0f;\n    bool warp_active = (warp_in_grid < query_count);\n    if (warp_active && lane == 0) {\n        float2 q = query[warp_in_grid];\n        qx = q.x;\n        qy = q.y;\n    }\n    unsigned warp_mask = __activemask();\n    qx = __shfl_sync(warp_mask, qx, 0);\n    qy = __shfl_sync(warp_mask, qy, 0);\n\n    // Process data in tiles\n    for (int tile_base = 0; tile_base < data_count; tile_base += TILE_POINTS) {\n        int tile_count = data_count - tile_base;\n        if (tile_count > TILE_POINTS) tile_count = TILE_POINTS;\n\n        // Load tile cooperatively by the whole block\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            tile[i] = data[tile_base + i];\n        }\n        __syncthreads();\n\n        // Each warp processes the tile for its query\n        if (warp_active) {\n            for (int t = 0; t < tile_count; t += warpSize) {\n                int i = t + lane;\n                bool lane_active = (i < tile_count);\n                float cand_dist = 0.0f;\n                int cand_idx = tile_base + i;\n\n                if (lane_active) {\n                    float2 p = tile[i];\n                    cand_dist = sq_l2(qx, qy, p);\n                }\n\n                // Initialization phase: fill first k elements (unsorted), then build heap once\n                if (!warp_heap_built[warp]) {\n                    if (lane_active) {\n                        int pos = atomicAdd(&warp_fill[warp], 1);\n                        if (pos < k) {\n                            // Write directly into buffer\n                            w_dist[pos] = cand_dist;\n                            w_idx[pos] = cand_idx;\n                        }\n                        // Collect overflow candidates that arrive after k has been filled\n                        unsigned overflow_mask = __ballot_sync(warp_mask, pos >= k);\n                        __syncwarp(warp_mask);\n                        // Build heap once when enough items collected\n                        if (lane == 0 && !warp_heap_built[warp] && warp_fill[warp] >= k) {\n                            // Clamp fill to exactly k and build max-heap\n                            warp_fill[warp] = k;\n                            heap_build(w_dist, w_idx, k);\n                            warp_heap_built[warp] = 1;\n                        }\n                        __syncwarp(warp_mask);\n\n                        // After heap is built, process overflow candidates within this step\n                        if (warp_heap_built[warp]) {\n                            if (overflow_mask != 0 && lane == 0) {\n                                // For each overflowing lane, fetch its candidate via shuffles and insert\n                                unsigned mask_iter = overflow_mask;\n                                while (mask_iter) {\n                                    int src_lane = __ffs(mask_iter) - 1;\n                                    mask_iter &= (mask_iter - 1);\n                                    float d = __shfl_sync(warp_mask, cand_dist, src_lane);\n                                    int id = __shfl_sync(warp_mask, cand_idx, src_lane);\n                                    heap_insert_if_better(w_dist, w_idx, k, d, id);\n                                }\n                            }\n                            __syncwarp(warp_mask);\n                        }\n                    } else {\n                        // Lane inactive; still need to participate in syncs\n                        __syncwarp(warp_mask);\n                        if (lane == 0 && !warp_heap_built[warp] && warp_fill[warp] >= k) {\n                            warp_fill[warp] = k;\n                            heap_build(w_dist, w_idx, k);\n                            warp_heap_built[warp] = 1;\n                        }\n                        __syncwarp(warp_mask);\n                    }\n                } else {\n                    // Heap phase: threshold filter then sequential insert by lane 0\n                    float thresh = w_dist[0]; // current worst (max)\n                    unsigned accept_mask = __ballot_sync(warp_mask, lane_active && (cand_dist < thresh));\n                    if (lane == 0 && accept_mask) {\n                        unsigned mask_iter = accept_mask;\n                        while (mask_iter) {\n                            int src_lane = __ffs(mask_iter) - 1;\n                            mask_iter &= (mask_iter - 1);\n                            float d = __shfl_sync(warp_mask, cand_dist, src_lane);\n                            int id = __shfl_sync(warp_mask, cand_idx, src_lane);\n                            heap_insert_if_better(w_dist, w_idx, k, d, id);\n                        }\n                    }\n                    __syncwarp(warp_mask);\n                }\n            } // end tile scan\n        }\n\n        __syncthreads(); // Ensure tile is no longer in use before loading next\n    } // end tiles\n\n    // Finalize results: heap-sort to ascending and write out\n    if (warp_active && lane == 0) {\n        // Ensure heap built (data_count >= k by assumption, but guard anyway)\n        if (!warp_heap_built[warp]) {\n            int fill = warp_fill[warp];\n            // If fill < k (shouldn't happen), pad with +inf to size k, then build heap\n            for (int i = fill; i < k; ++i) {\n                w_dist[i] = CUDART_INF_F;\n                w_idx[i] = -1;\n            }\n            heap_build(w_dist, w_idx, k);\n            warp_heap_built[warp] = 1;\n        }\n        heap_sort_ascending(w_dist, w_idx, k);\n\n        // Write results for this query\n        size_t base = size_t(warp_in_grid) * size_t(k);\n        for (int j = 0; j < k; ++j) {\n            result[base + j].first = w_idx[j];\n            result[base + j].second = w_dist[j];\n        }\n    }\n}\n\n// Host interface: launches the kernel\nextern \"C\" void run_knn(const float2 *query, int query_count,\n                        const float2 *data, int data_count,\n                        std::pair<int, float> *result, int k)\n{\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Configure launch\n    dim3 block(WARPS_PER_BLOCK * 32, 1, 1);\n    dim3 grid((query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK, 1, 1);\n\n    // Dynamic shared memory size\n    size_t tile_bytes = size_t(TILE_POINTS) * sizeof(float2);\n    size_t perwarp_topk_bytes = size_t(k) * (sizeof(int) + sizeof(float));\n    size_t topk_bytes = size_t(WARPS_PER_BLOCK) * perwarp_topk_bytes;\n    size_t control_bytes = size_t(WARPS_PER_BLOCK) * 2 * sizeof(int);\n    size_t shmem_bytes = tile_bytes + topk_bytes + control_bytes;\n\n    // Opt-in larger dynamic shared memory if needed\n    cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)shmem_bytes);\n\n    // Launch\n    knn_kernel<<<grid, block, shmem_bytes>>>(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 686, "completion_tokens": 19676, "total_tokens": 20362, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 16512, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db7331c81909029f2c0800b615c", "custom_id": "knn03-04", "response": {"status_code": 200, "request_id": "afc689706ecc6196d0b5e3effc76b16f", "body": {"id": "chatcmpl-C4sg8DaJWNDx8qmmWZm9CYo1IdCSZ", "object": "chat.completion", "created": 1755279532, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cfloat>\n#include <utility>\n\n// This implementation computes k-NN for 2D points using one warp (32 threads) per query.\n// It processes the data points in batches loaded into shared memory by the entire block.\n// Each warp maintains a distributed top-k (across its 32 threads), with k guaranteed to be\n// a power of two between 32 and 1024. Thus, each lane owns S = k / 32 slots. We store\n// each lane's S candidates in registers. The warp cooperatively maintains a global \"gate\"\n// equal to the current worst (largest) distance among the distributed top-k to reject\n// non-improving candidates quickly. When a replacement happens, the owning lane updates\n// its local maximum and the warp recomputes the global gate via a warp reduction.\n// After scanning all data, each lane sorts its local S candidates (ascending). The warp\n// then performs a 32-way merge to produce the final sorted top-k list for the query and\n// writes it to the result array.\n//\n// Notes:\n// - Distances are squared L2 norm (no sqrt).\n// - No extra global memory is allocated; only shared memory for data tiles is used.\n// - The kernel assumes a modern NVIDIA GPU (A100/H100). It uses warp shuffles for\n//   intra-warp communication and __syncthreads to guard shared memory tiles.\n// - The block size is chosen as 4 warps (128 threads). The data tile size is 4096 points,\n//   which consumes 32 KB of shared memory for float2 points. You can tune these constants\n//   for different GPUs if desired.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Tune these for the target GPU if needed.\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 4   // 4 warps = 128 threads per block\n#endif\n\n#ifndef TILE_POINTS\n#define TILE_POINTS 4096    // Number of float2 points per shared-memory tile (32 KB)\n#endif\n\n// Maximum per-lane capacity: k <= 1024 and warp size is 32 => S_max = 32\n#define MAX_S (1024 / WARP_SIZE)\n\nstatic __device__ __forceinline__ unsigned full_warp_mask() {\n    // All 32 lanes active\n    return 0xFFFFFFFFu;\n}\n\n// Warp-wide argmax reduction with broadcast of the winning value and lane.\n// Ties are broken in favor of the larger lane index to ensure deterministic behavior.\nstatic __device__ __forceinline__ void warp_argmax_broadcast(float val_in, int lane_id, float &max_val_out, int &max_lane_out, unsigned mask) {\n    float v = val_in;\n    int   l = lane_id;\n    // Tree reduction using shuffle down\n    for (int offset = WARP_SIZE >> 1; offset > 0; offset >>= 1) {\n        float v_other = __shfl_down_sync(mask, v, offset);\n        int   l_other = __shfl_down_sync(mask, l, offset);\n        if (v_other > v || (v_other == v && l_other > l)) {\n            v = v_other;\n            l = l_other;\n        }\n    }\n    // Broadcast final max value and lane from lane 0 of the warp\n    max_val_out  = __shfl_sync(mask, v, 0);\n    max_lane_out = __shfl_sync(mask, l, 0);\n}\n\n// Warp-wide argmin reduction with broadcast of the winning value and lane.\n// Ties are broken in favor of the smaller lane index for stable ordering in outputs.\nstatic __device__ __forceinline__ void warp_argmin_broadcast(float val_in, int lane_id, float &min_val_out, int &min_lane_out, unsigned mask) {\n    float v = val_in;\n    int   l = lane_id;\n    for (int offset = WARP_SIZE >> 1; offset > 0; offset >>= 1) {\n        float v_other = __shfl_down_sync(mask, v, offset);\n        int   l_other = __shfl_down_sync(mask, l, offset);\n        if (v_other < v || (v_other == v && l_other < l)) {\n            v = v_other;\n            l = l_other;\n        }\n    }\n    min_val_out  = __shfl_sync(mask, v, 0);\n    min_lane_out = __shfl_sync(mask, l, 0);\n}\n\n// Insert a candidate (dist, idx) into the warp-distributed top-k if it improves (dist < gate).\n// The global worst (gate) resides at (gate_owner_lane, that lane's local_max_slot).\n// After replacement, the owner lane recomputes its local maximum and the warp recomputes the gate.\nstatic __device__ __forceinline__ void warp_topk_try_insert(\n    float cand_dist, int cand_idx,\n    float &gate, int &gate_owner_lane, int &gate_owner_slot,\n    float local_dist[MAX_S], int local_idx[MAX_S], int S,\n    float &local_max_val, int &local_max_slot,\n    int lane_id, unsigned mask)\n{\n    if (cand_dist >= gate) return; // Not improving the current top-k\n    // Replace the current global worst in the owning lane\n    if (lane_id == gate_owner_lane) {\n        // Replace at the local max slot\n        local_dist[local_max_slot] = cand_dist;\n        local_idx[local_max_slot]  = cand_idx;\n\n        // Recompute local maximum (worst in this lane's S candidates)\n        // S is at most 32; a linear scan is fine.\n        float mx_val = local_dist[0];\n        int   mx_pos = 0;\n#pragma unroll\n        for (int i = 1; i < MAX_S; ++i) {\n            if (i >= S) break;\n            float v = local_dist[i];\n            if (v > mx_val) {\n                mx_val = v;\n                mx_pos = i;\n            }\n        }\n        local_max_val  = mx_val;\n        local_max_slot = mx_pos;\n    }\n    __syncwarp(mask);\n\n    // Recompute the global worst (gate) among all lanes' local maxima.\n    float new_gate;\n    int   new_owner_lane;\n    warp_argmax_broadcast(local_max_val, lane_id, new_gate, new_owner_lane, mask);\n    int new_owner_slot = __shfl_sync(mask, local_max_slot, new_owner_lane);\n\n    gate            = new_gate;\n    gate_owner_lane = new_owner_lane;\n    gate_owner_slot = new_owner_slot;\n    __syncwarp(mask);\n}\n\n// Insertion sort of a small array (size S <= 32) in ascending order.\nstatic __device__ __forceinline__ void small_insertion_sort(float dist_arr[MAX_S], int idx_arr[MAX_S], int S) {\n#pragma unroll\n    for (int i = 1; i < MAX_S; ++i) {\n        if (i >= S) break;\n        float key_d = dist_arr[i];\n        int   key_i = idx_arr[i];\n        int j = i - 1;\n        while (j >= 0 && dist_arr[j] > key_d) {\n            dist_arr[j + 1] = dist_arr[j];\n            idx_arr[j + 1]  = idx_arr[j];\n            --j;\n        }\n        dist_arr[j + 1] = key_d;\n        idx_arr[j + 1]  = key_i;\n    }\n}\n\n// Kernel implementing batched, warp-cooperative k-NN for 2D points.\n__global__ void knn2d_kernel(const float2 * __restrict__ query,\n                             int query_count,\n                             const float2 * __restrict__ data,\n                             int data_count,\n                             std::pair<int, float> * __restrict__ result,\n                             int k)\n{\n    extern __shared__ unsigned char smem_raw[];\n    // Shared memory tile for data points\n    float2 *s_points = reinterpret_cast<float2*>(smem_raw);\n\n    const int lane_id        = threadIdx.x & (WARP_SIZE - 1);\n    const int warp_in_block  = threadIdx.x >> 5; // threadIdx.x / 32\n    const int warps_per_block = WARPS_PER_BLOCK;\n    const int warp_global    = blockIdx.x * warps_per_block + warp_in_block;\n    const bool warp_active   = (warp_global < query_count);\n    const unsigned warp_mask = full_warp_mask();\n\n    // Compute per-lane capacity S = k / 32 (k guaranteed divisible by 32).\n    const int S = k >> 5;\n\n    // Load and broadcast query point for this warp.\n    float qx = 0.0f, qy = 0.0f;\n    if (warp_active && lane_id == 0) {\n        float2 q = query[warp_global];\n        qx = q.x;\n        qy = q.y;\n    }\n    qx = __shfl_sync(warp_mask, qx, 0);\n    qy = __shfl_sync(warp_mask, qy, 0);\n\n    // Per-lane local storage for distributed top-k\n    float local_dist[MAX_S];\n    int   local_idx[MAX_S];\n#pragma unroll\n    for (int i = 0; i < MAX_S; ++i) {\n        local_dist[i] = FLT_MAX;\n        local_idx[i]  = -1;\n    }\n\n    // Each lane tracks its local maximum (worst) among its S candidates.\n    float local_max_val  = FLT_MAX;\n    int   local_max_slot = 0;\n\n    // Initialize global gate (worst among all lanes).\n    float gate_val;\n    int   gate_owner_lane;\n    warp_argmax_broadcast(local_max_val, lane_id, gate_val, gate_owner_lane, warp_mask);\n    int   gate_owner_slot = __shfl_sync(warp_mask, local_max_slot, gate_owner_lane);\n\n    // Process data in shared-memory tiles\n    for (int base = 0; base < data_count; base += TILE_POINTS) {\n        int tile_count = data_count - base;\n        if (tile_count > TILE_POINTS) tile_count = TILE_POINTS;\n\n        // Load tile cooperatively by all threads in the block\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            s_points[i] = data[base + i];\n        }\n        __syncthreads();\n\n        // Each warp processes the tile for its query\n        if (warp_active) {\n            for (int offset = 0; offset < tile_count; offset += WARP_SIZE) {\n                int j = offset + lane_id;\n\n                // Compute candidate distance for this lane\n                float cand_dist = FLT_MAX;\n                int   cand_idx  = -1;\n                if (j < tile_count) {\n                    float2 p = s_points[j];\n                    float dx = qx - p.x;\n                    float dy = qy - p.y;\n                    cand_dist = dx * dx + dy * dy; // squared L2 distance\n                    cand_idx  = base + j;\n                }\n\n                // Sequentially attempt insertion for all 32 candidates in this group.\n#pragma unroll\n                for (int t = 0; t < WARP_SIZE; ++t) {\n                    float d2  = __shfl_sync(warp_mask, cand_dist, t);\n                    int   idx = __shfl_sync(warp_mask, cand_idx,  t);\n                    // Try inserting if better than current gate\n                    warp_topk_try_insert(d2, idx,\n                                         gate_val, gate_owner_lane, gate_owner_slot,\n                                         local_dist, local_idx, S,\n                                         local_max_val, local_max_slot,\n                                         lane_id, warp_mask);\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    // Finalization for active warps: sort per-lane local lists and produce globally sorted k-NN\n    if (warp_active) {\n        // Sort each lane's local list ascending (by distance)\n        small_insertion_sort(local_dist, local_idx, S);\n        __syncwarp(warp_mask);\n\n        // Multi-way merge of 32 sorted lists (each of size S) to produce the final k sorted neighbors.\n        int head = 0; // this lane's head pointer into its local list\n        for (int t = 0; t < k; ++t) {\n            float my_val = (head < S) ? local_dist[head] : FLT_MAX;\n            // Find minimum distance among lanes\n            float min_val;\n            int   min_lane;\n            warp_argmin_broadcast(my_val, lane_id, min_val, min_lane, warp_mask);\n            // Retrieve the index corresponding to the winning lane's current head\n            int min_idx = __shfl_sync(warp_mask, (head < S ? local_idx[head] : -1), min_lane);\n            // Lane 0 writes the output\n            if (lane_id == 0) {\n                // Store as j-th nearest neighbor for this query (sorted ascending by distance)\n                std::pair<int, float> &dst = result[warp_global * k + t];\n                dst.first  = min_idx;\n                dst.second = min_val;\n            }\n            // Winning lane advances its head\n            if (lane_id == min_lane) {\n                ++head;\n            }\n            __syncwarp(warp_mask);\n        }\n    }\n}\n\n// Host API entry point\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    // Choose grid/block dimensions\n    const int warps_per_block = WARPS_PER_BLOCK;\n    const int threads_per_block = warps_per_block * WARP_SIZE;\n    const int num_warps = (query_count + warps_per_block - 1) / warps_per_block;\n    dim3 grid(num_warps, 1, 1);\n    dim3 block(threads_per_block, 1, 1);\n\n    // Shared memory size: only the data tile is stored in shared memory (float2 * TILE_POINTS)\n    size_t shared_bytes = sizeof(float2) * TILE_POINTS;\n\n    // Launch kernel\n    knn2d_kernel<<<grid, block, shared_bytes>>>(query, query_count, data, data_count, result, k);\n\n    // Optionally, synchronize or check for errors here if desired by the caller.\n    // cudaDeviceSynchronize();\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 686, "completion_tokens": 21968, "total_tokens": 22654, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 18880, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db750e481909202d700ad90abb7", "custom_id": "knn03-05", "response": {"status_code": 200, "request_id": "0ee5f453b67c103778bff5c8bbbf9290", "body": {"id": "chatcmpl-C4sj5LnGXwOSOVtDqfsyaiyZZGtOi", "object": "chat.completion", "created": 1755279715, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cuda.h>\n#include <utility>\n\n// This implementation computes k-NN (squared Euclidean distances) for 2D points using a warp-per-query strategy.\n// - Each warp (32 threads) handles a single query point.\n// - The data points are processed in shared-memory tiles loaded cooperatively by the entire block.\n// - Each warp maintains its private top-k candidate set distributed across lanes (k/32 per lane).\n// - A warp-wide selection replaces the current global worst candidate when a better one is found.\n// - Finally, each warp writes its k candidates to shared memory and performs an in-place bitonic sort,\n//   then writes the sorted k nearest neighbors to the output.\n//\n// Notes on data layout:\n// - Input arrays 'query' and 'data' hold float2 points (x, y) in row-major order.\n// - Output array 'result' holds std::pair<int, float> where 'first' is index in 'data' and 'second' is squared distance.\n//\n// Performance considerations for modern NVIDIA GPUs (A100/H100):\n// - 256 threads per block (8 warps) by default provides good occupancy while leaving sufficient shared memory.\n// - Dynamic shared memory is sized at runtime as max(per-warp top-k buffer, preferred tile buffer).\n// - The kernel uses warp intrinsics (__reduce_max_sync, __ballot_sync, __shfl_sync) for efficient warp communication.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Device-side pair compatible with std::pair<int, float> (assumed sequential layout: int first; float second).\nstruct PairIF {\n    int   first;\n    float second;\n};\n\n// Validate that our device-side pair matches the host-side std::pair<int,float> layout.\nstatic_assert(sizeof(PairIF) == sizeof(std::pair<int, float>), \"PairIF must match std::pair<int,float> size\");\nstatic_assert(alignof(PairIF) == alignof(std::pair<int, float>), \"PairIF must match std::pair<int,float> alignment\");\n\n__device__ __forceinline__ float sq_l2_dist2d(const float2 a, const float2 b) {\n    // Compute squared Euclidean distance using FMA for better throughput.\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    return __fmaf_rn(dx, dx, dy * dy);\n}\n\n// Kernel implementing warp-per-query k-NN over 2D points.\n// - query:    [query_count] float2\n// - data:     [data_count]  float2\n// - result:   [query_count * k] PairIF\n// - k:        power of two in [32, 1024], and data_count >= k\n// - warpsPerBlock: number of warps per block (threads per block = 32 * warpsPerBlock)\n// - tilePoints: number of data points per shared-memory tile (block-wide)\n__global__ void knn_kernel_2d_warp(const float2* __restrict__ query,\n                                   int query_count,\n                                   const float2* __restrict__ data,\n                                   int data_count,\n                                   PairIF* __restrict__ result,\n                                   int k,\n                                   int warpsPerBlock,\n                                   int tilePoints)\n{\n    extern __shared__ unsigned char smem_bytes[];\n    // Shared memory region, first used as a tile buffer (float2), later as per-warp PairIF buffer for sorting.\n    float2* tile = reinterpret_cast<float2*>(smem_bytes);\n\n    const int lane           = threadIdx.x & (WARP_SIZE - 1);\n    const int warpIdInBlock  = threadIdx.x >> 5;\n    const int globalWarpId   = blockIdx.x * warpsPerBlock + warpIdInBlock;\n    const bool hasQuery      = (globalWarpId < query_count);\n\n    // Broadcast the query point to the whole warp (one load per warp).\n    float qx = 0.0f, qy = 0.0f;\n    if (hasQuery) {\n        float2 q0 = (lane == 0) ? query[globalWarpId] : make_float2(0.0f, 0.0f);\n        unsigned fullMask = 0xFFFFFFFFu;\n        qx = __shfl_sync(fullMask, q0.x, 0);\n        qy = __shfl_sync(fullMask, q0.y, 0);\n    }\n\n    // Each lane maintains k/32 entries.\n    const int perLane = k / WARP_SIZE; // k is guaranteed power of two and >= 32.\n    float topDist[WARP_SIZE];  // up to 32\n    int   topIdx [WARP_SIZE];  // up to 32\n\n    #pragma unroll\n    for (int i = 0; i < WARP_SIZE; ++i) {\n        if (i < perLane) {\n            topDist[i] = CUDART_INF_F;\n            topIdx[i]  = -1;\n        }\n    }\n\n    // Helper to recompute this lane's local worst among its perLane entries.\n    auto recompute_local_worst = [&](float& worstVal, int& worstPos) {\n        if (!hasQuery) {\n            worstVal = -CUDART_INF_F;\n            worstPos = -1;\n            return;\n        }\n        float wv = -CUDART_INF_F;\n        int wp   = 0;\n        #pragma unroll\n        for (int i = 0; i < WARP_SIZE; ++i) {\n            if (i < perLane) {\n                float d = topDist[i];\n                if (d > wv) { wv = d; wp = i; }\n            }\n        }\n        worstVal = wv;\n        worstPos = wp;\n    };\n\n    float localWorstVal;\n    int   localWorstPos;\n    recompute_local_worst(localWorstVal, localWorstPos);\n\n    // Process the data in tiles cached in shared memory by the entire block.\n    for (int base = 0; base < data_count; base += tilePoints) {\n        const int remaining = data_count - base;\n        const int tileCount = (remaining < tilePoints) ? remaining : tilePoints;\n\n        // Cooperative load of the tile by all threads in the block.\n        for (int i = threadIdx.x; i < tileCount; i += blockDim.x) {\n            tile[i] = data[base + i];\n        }\n        __syncthreads();\n\n        if (hasQuery) {\n            // Each warp loops through the tile, each lane visiting strided elements.\n            for (int j = lane; j < tileCount; j += WARP_SIZE) {\n                float2 p = tile[j];\n                // Compute squared distance without sqrt\n                float dx = p.x - qx;\n                float dy = p.y - qy;\n                float dist = __fmaf_rn(dx, dx, dy * dy);\n                unsigned fullMask = 0xFFFFFFFFu;\n\n                // Compute the current global worst value across the warp (max of lane-local worsts).\n                // Note: __reduce_max_sync returns the maximum across the active warp lanes.\n                float worstVal = __reduce_max_sync(fullMask, localWorstVal);\n\n                // Quickly reject if the candidate is not better than the global worst.\n                if (dist < worstVal) {\n                    // Which lane holds that global worst? Use ballot on equality to worstVal and pick highest lane id.\n                    unsigned eqMask = __ballot_sync(fullMask, localWorstVal == worstVal);\n                    int worstLane = 31 - __clz(eqMask); // choose the highest set bit (tie-breaker)\n                    // Get the position (index within that lane) to replace.\n                    int worstPos = __shfl_sync(fullMask, localWorstPos, worstLane);\n\n                    if (lane == worstLane) {\n                        topDist[worstPos] = dist;\n                        topIdx[worstPos]  = base + j;\n                        // Update this lane's local worst for subsequent iterations.\n                        recompute_local_worst(localWorstVal, localWorstPos);\n                    }\n                    // Other lanes keep their local worst cached; it's still valid.\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    // Reuse shared memory as per-warp PairIF buffer, then sort using bitonic sort (ascending by distance).\n    PairIF* pairbuf = reinterpret_cast<PairIF*>(smem_bytes);\n\n    if (hasQuery) {\n        PairIF* myBuf = pairbuf + warpIdInBlock * k;\n\n        // Scatter the per-lane entries into a contiguous buffer [0..k).\n        // Map lane-local index i in [0..perLane) to position pos = i * WARP_SIZE + lane.\n        #pragma unroll\n        for (int i = 0; i < WARP_SIZE; ++i) {\n            if (i < perLane) {\n                int pos = i * WARP_SIZE + lane;\n                myBuf[pos].first  = topIdx[i];\n                myBuf[pos].second = topDist[i];\n            }\n        }\n        __syncwarp();\n\n        // Bitonic sort network for length k (k is power-of-two).\n        // Each thread processes multiple indices strided by warp size.\n        for (int size = 2; size <= k; size <<= 1) {\n            for (int stride = size >> 1; stride > 0; stride >>= 1) {\n                for (int i = lane; i < k; i += WARP_SIZE) {\n                    int partner = i ^ stride;\n                    if (partner > i) {\n                        bool up = ((i & size) == 0);\n                        PairIF a = myBuf[i];\n                        PairIF b = myBuf[partner];\n                        // Compare by distance (second)\n                        bool swap = up ? (a.second > b.second) : (a.second < b.second);\n                        if (swap) {\n                            myBuf[i]       = b;\n                            myBuf[partner] = a;\n                        }\n                    }\n                }\n                __syncwarp();\n            }\n        }\n\n        // Write sorted results to global memory.\n        PairIF* out = result + static_cast<size_t>(globalWarpId) * static_cast<size_t>(k);\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            out[i] = myBuf[i];\n        }\n    }\n}\n\n// Host interface.\n// The arrays query, data, and result are assumed to be device allocations (cudaMalloc).\n// - query:  device pointer to query_count float2 points\n// - data:   device pointer to data_count float2 points\n// - result: device pointer to query_count * k pairs (std::pair<int,float>)\n// - k:      power of two in [32, 1024], and data_count >= k\nvoid run_knn(const float2 *query,\n             int query_count,\n             const float2 *data,\n             int data_count,\n             std::pair<int, float> *result,\n             int k)\n{\n    // Choose the number of warps per block based on shared memory availability and k.\n    int device = 0;\n    cudaGetDevice(&device);\n\n    int maxOptinShared = 0;\n    cudaDeviceGetAttribute(&maxOptinShared, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    if (maxOptinShared <= 0) {\n        // Fallback to default per-block shared memory size if opt-in is unavailable.\n        cudaDeviceProp prop{};\n        cudaGetDeviceProperties(&prop, device);\n        maxOptinShared = prop.sharedMemPerBlock; // Typically 48KB or 64KB on many devices\n    }\n\n    // Start with a preferred number of warps per block.\n    int warpsPerBlock = 8; // 8 warps => 256 threads per block (good default on A100/H100)\n    // Ensure the per-warp top-k buffer fits into available dynamic shared memory.\n    size_t perBlockTopKBytes = static_cast<size_t>(warpsPerBlock) * static_cast<size_t>(k) * sizeof(PairIF);\n    if (perBlockTopKBytes > static_cast<size_t>(maxOptinShared)) {\n        warpsPerBlock = static_cast<int>(maxOptinShared / (static_cast<size_t>(k) * sizeof(PairIF)));\n        if (warpsPerBlock < 1) warpsPerBlock = 1;\n        perBlockTopKBytes = static_cast<size_t>(warpsPerBlock) * static_cast<size_t>(k) * sizeof(PairIF);\n    }\n\n    // Preferred tile size: aim for ~64 KiB of float2 points if possible.\n    size_t preferredTileBytes = 64 * 1024; // 64KB\n    // Total dynamic shared memory is the maximum of per-warp top-k buffer and preferred tile buffer.\n    size_t dynSmemBytes = perBlockTopKBytes > preferredTileBytes ? perBlockTopKBytes : preferredTileBytes;\n    // Cap to the device's opt-in limit.\n    if (dynSmemBytes > static_cast<size_t>(maxOptinShared)) {\n        dynSmemBytes = static_cast<size_t>(maxOptinShared);\n    }\n\n    // Configure kernel attribute to allow dynamic shared memory up to dynSmemBytes.\n    cudaFuncSetAttribute(knn_kernel_2d_warp, cudaFuncAttributeMaxDynamicSharedMemorySize, static_cast<int>(dynSmemBytes));\n\n    // Compute tilePoints from the chosen dynamic shared memory size (shared memory is reused).\n    int tilePoints = static_cast<int>(dynSmemBytes / sizeof(float2));\n    if (tilePoints < WARP_SIZE) tilePoints = WARP_SIZE; // Ensure minimal useful tile size.\n\n    // Configure grid and block.\n    const int threadsPerBlock = warpsPerBlock * WARP_SIZE;\n    const int blocks = (query_count + warpsPerBlock - 1) / warpsPerBlock;\n\n    // Launch the kernel.\n    PairIF* result_if = reinterpret_cast<PairIF*>(result);\n    knn_kernel_2d_warp<<<blocks, threadsPerBlock, dynSmemBytes>>>(\n        query, query_count, data, data_count, result_if, k, warpsPerBlock, tilePoints\n    );\n\n    // Optional: synchronize or leave to caller. Here we ensure completion.\n    cudaDeviceSynchronize();\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 686, "completion_tokens": 18161, "total_tokens": 18847, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 15104, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db7a6b08190bbfd18e5353720b8", "custom_id": "knn03-06", "response": {"status_code": 200, "request_id": "fd7347cfc245d35ca986c731d0721af4", "body": {"id": "chatcmpl-C4sjYcTATGu2I6yvzVp8lrZLZaIVV", "object": "chat.completion", "created": 1755279744, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <math_constants.h>\n#include <utility>\n\n// This implementation assigns one CUDA warp (32 threads) per query point.\n// The data points are processed in tiles that are cooperatively loaded into shared memory by the entire block.\n// Each warp maintains a private intermediate top-k structure split evenly across its 32 lanes.\n// Specifically, each lane keeps the best L = k/32 candidates it has seen (sorted ascending by distance),\n// and at the end, the warp performs a multiway merge (via warp-level shuffles) to produce the final k neighbors.\n//\n// Key properties:\n// - k is a power of two in [32, 1024], so L = k / 32 is an integer in [1, 32].\n// - Distances are squared Euclidean (no sqrt), computed with an FMA for precision and speed.\n// - No additional device memory is allocated (uses only shared memory and registers).\n// - Threads in a warp communicate via __shfl_sync. Block-wide __syncthreads() is used around shared-memory tiles.\n// - The per-lane top-L is kept sorted ascending to make insertion O(L) and the final merge simple.\n// - Result is written as contiguous std::pair<int, float> per query, ordered by increasing distance.\n\nstruct PairIF {\n    int   first;\n    float second;\n};\n\nstatic inline __device__ float sqdist2D(const float2 a, const float qx, const float qy) {\n    float dx = a.x - qx;\n    float dy = a.y - qy;\n    // dx*dx + dy*dy using one FMA\n    return fmaf(dy, dy, dx * dx);\n}\n\nstatic inline __device__ unsigned full_warp_mask() {\n#if __CUDACC_VER_MAJOR__ >= 9\n    return 0xFFFFFFFFu;\n#else\n    return 0xFFFFFFFFu;\n#endif\n}\n\n// Warp-level argmin on a float value; returns pair {min_value, min_lane}.\nstruct MinValLane {\n    float val;\n    int   lane;\n};\n\nstatic inline __device__ MinValLane warp_argmin(float val) {\n    unsigned mask = full_warp_mask();\n    int lane = threadIdx.x & 31;\n    float v = val;\n    int   l = lane;\n\n    // Tree reduction to find min value and corresponding lane (tie-break by smaller lane id)\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        float ov = __shfl_down_sync(mask, v, offset);\n        int   ol = __shfl_down_sync(mask, l, offset);\n        // Choose the smaller value; in case of tie, choose the smaller lane id\n        if ((ov < v) || ((ov == v) && (ol < l))) {\n            v = ov;\n            l = ol;\n        }\n    }\n    // Broadcast the winner to all lanes\n    MinValLane res;\n    res.val  = __shfl_sync(mask, v, 0);\n    res.lane = __shfl_sync(mask, l, 0);\n    return res;\n}\n\n__global__ void knn2d_kernel(\n    const float2* __restrict__ query,\n    int query_count,\n    const float2* __restrict__ data,\n    int data_count,\n    PairIF* __restrict__ result,\n    int k,\n    int tile_points // number of data points per shared-memory tile\n) {\n    extern __shared__ float2 smem[]; // shared-memory cache for data tiles\n\n    const int warpSize_ = 32;\n    const int lane      = threadIdx.x & (warpSize_ - 1);\n    const int warp      = threadIdx.x >> 5; // warp index within the block\n    const int warpsPerBlock = blockDim.x >> 5;\n\n    const int qid = blockIdx.x * warpsPerBlock + warp; // global query index served by this warp\n    const bool active = (qid < query_count);\n\n    // Load and broadcast the query point to the warp's lanes\n    float qx = 0.0f, qy = 0.0f;\n    if (active) {\n        float2 qv;\n        if (lane == 0) qv = query[qid];\n        unsigned mask = full_warp_mask();\n        qx = __shfl_sync(mask, qv.x, 0);\n        qy = __shfl_sync(mask, qv.y, 0);\n    }\n    __syncwarp(); // Ensure all lanes have qx, qy (mainly for readability; warps execute in lock-step)\n\n    // Per-lane top-L container (ascending order: best (smallest) at [0], worst at [L-1])\n    const int L = k >> 5; // k / 32, guaranteed >= 1 by problem constraints\n    float bestDists[32];\n    int   bestIdx[32];\n#pragma unroll\n    for (int i = 0; i < 32; ++i) {\n        if (i < L) {\n            bestDists[i] = CUDART_INF_F;\n            bestIdx[i]   = -1;\n        }\n    }\n\n    // Process the data in tiles loaded into shared memory by the entire block\n    for (int base = 0; base < data_count; base += tile_points) {\n        int tileCount = tile_points;\n        if (base + tileCount > data_count) tileCount = data_count - base;\n\n        // Cooperative load: each thread in the block loads some float2s\n        for (int i = threadIdx.x; i < tileCount; i += blockDim.x) {\n            smem[i] = data[base + i];\n        }\n        __syncthreads();\n\n        // Each warp computes distances from its query to all points in the tile\n        if (active) {\n            for (int i = lane; i < tileCount; i += warpSize_) {\n                float d = sqdist2D(smem[i], qx, qy);\n\n                // Fast reject if not better than current worst\n                if (d < bestDists[L - 1]) {\n                    // Insert d into ascending-sorted list bestDists[0..L-1], shifting larger elements right.\n                    int gidx = base + i;\n                    int j = L - 1;\n                    // Move larger elements one position to the right\n                    while (j > 0 && d < bestDists[j - 1]) {\n                        bestDists[j] = bestDists[j - 1];\n                        bestIdx[j]   = bestIdx[j - 1];\n                        --j;\n                    }\n                    bestDists[j] = d;\n                    bestIdx[j]   = gidx;\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    // Final multiway merge across lanes of the warp:\n    // Each lane provides its next smallest candidate from its local sorted list.\n    // We repeat k times to select the global top-k in ascending order.\n    if (active) {\n        int outBase = qid * k;\n        int ptr = 0; // index into this lane's bestDists/bestIdx\n\n        unsigned mask = full_warp_mask();\n        for (int out = 0; out < k; ++out) {\n            float myCand = (ptr < L) ? bestDists[ptr] : CUDART_INF_F;\n            // Find the winning lane with the smallest candidate\n            MinValLane m = warp_argmin(myCand);\n            int winLane = m.lane;\n            float winVal = m.val;\n\n            // Broadcast the corresponding index from the winning lane\n            int myCandIdx = (ptr < L) ? bestIdx[ptr] : -1;\n            int winIdx = __shfl_sync(mask, myCandIdx, winLane);\n\n            if (lane == 0) {\n                result[outBase + out].first  = winIdx;\n                result[outBase + out].second = winVal;\n            }\n            // The lane that contributed the winner advances its pointer\n            if (lane == winLane) {\n                ++ptr;\n            }\n        }\n    }\n}\n\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    // Tunable parameters: threads per block and shared-memory tile size (in points).\n    //  - Use 256 threads per block (8 warps / block) to balance occupancy and shared memory usage.\n    //  - Use a tile of 4096 points (32 KB at 8 bytes per float2) to keep SMEM within typical limits.\n    const int threads_per_block = 256;\n    const int warps_per_block   = threads_per_block / 32;\n    const int tile_points       = 4096;\n\n    // Grid size: one warp per query\n    const int total_warps = (query_count + warps_per_block - 1) / warps_per_block;\n    dim3 grid(total_warps, 1, 1);\n    dim3 block(threads_per_block, 1, 1);\n\n    // Dynamic shared memory size: tile_points float2s\n    size_t shmem_bytes = static_cast<size_t>(tile_points) * sizeof(float2);\n\n    // Launch kernel. Reinterpret the result pointer to a simple POD with identical layout.\n    knn2d_kernel<<<grid, block, shmem_bytes>>>(\n        query, query_count,\n        data, data_count,\n        reinterpret_cast<PairIF*>(result),\n        k,\n        tile_points\n    );\n\n    // Note: Error checking is intentionally omitted; callers may insert cudaGetLastError()/cudaDeviceSynchronize() as needed.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 686, "completion_tokens": 14683, "total_tokens": 15369, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 12608, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db76a3081909cf36d0eac59a548", "custom_id": "knn03-07", "response": {"status_code": 200, "request_id": "b9be29c3cae33dbefc41947d6de7da09", "body": {"id": "chatcmpl-C4sjXkdZ84WRxq5ixdBx8G4jhcH01", "object": "chat.completion", "created": 1755279743, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <math_constants.h>\n#include <cfloat>\n#include <utility>\n\n// High-performance k-NN for 2D points using a warp-per-query approach, a shared-memory data tile per block,\n// and a per-warp max-heap of size k stored in shared memory for intermediate results.\n//\n// - Each warp (32 threads) handles one query point.\n// - The block cooperatively loads a tile of data points into shared memory.\n// - All warps in the block compute distances from their query to the cached tile points.\n// - For each warp, lane 0 maintains a max-heap (size k) of the current best neighbors (smallest distances).\n// - Threads in the warp propose candidates; lane 0 filters and inserts them via heap operations.\n// - After processing all tiles, lane 0 performs a heap sort to produce ascending order and writes results.\n//\n// Design choices:\n// - 8 warps per block (256 threads) is a good compromise on A100/H100: high parallelism while leaving shared memory\n//   for both the data tile and per-warp heaps (k up to 1024 => ~8KB/warp).\n// - The data tile size is chosen at runtime to fit within the available opt-in dynamic shared memory.\n// - Distances are squared Euclidean (no sqrt); k is power-of-two in [32, 1024].\n// - No device allocations; all temporary storage is in shared memory.\n// - Warp-level shuffles and ballots are used for intra-warp coordination, with warp barriers for clarity.\n\nnamespace {\n\n// POD type with the same memory layout as std::pair<int, float>.\nstruct PairIF {\n    int   first;\n    float second;\n};\n\n// Tunable constant: number of warps per block (must be >=1). 8 warps => 256 threads per block.\nconstexpr int WARPS_PER_BLOCK = 8;\nconstexpr int WARP_SIZE       = 32;\nconstexpr unsigned FULL_MASK  = 0xffffffffu;\n\n// Swap helper for heap operations (pairs of distance/index).\n__device__ __forceinline__ void swap_pair(float &ad, int &ai, float &bd, int &bi) {\n    float td = ad; ad = bd; bd = td;\n    int   ti = ai; ai = bi; bi = ti;\n}\n\n// Sift-up for max-heap on (distances, indices).\n__device__ __forceinline__ void heap_sift_up(float* __restrict__ dists,\n                                             int*   __restrict__ idxs,\n                                             int pos)\n{\n    while (pos > 0) {\n        int parent = (pos - 1) >> 1;\n        if (dists[parent] >= dists[pos]) break;\n        swap_pair(dists[parent], idxs[parent], dists[pos], idxs[pos]);\n        pos = parent;\n    }\n}\n\n// Sift-down for max-heap on (distances, indices).\n__device__ __forceinline__ void heap_sift_down(float* __restrict__ dists,\n                                               int*   __restrict__ idxs,\n                                               int size,\n                                               int pos = 0)\n{\n    int left = (pos << 1) + 1;\n    while (left < size) {\n        int right = left + 1;\n        int largest = (right < size && dists[right] > dists[left]) ? right : left;\n        if (dists[pos] >= dists[largest]) break;\n        swap_pair(dists[pos], idxs[pos], dists[largest], idxs[largest]);\n        pos = largest;\n        left = (pos << 1) + 1;\n    }\n}\n\n// Push a value/index into the max-heap (size increases by 1).\n__device__ __forceinline__ void heap_push_max(float* __restrict__ dists,\n                                              int*   __restrict__ idxs,\n                                              int& size,\n                                              int   capacity,\n                                              float val,\n                                              int   id)\n{\n    // Precondition: size < capacity\n    int pos = size;\n    dists[pos] = val;\n    idxs[pos]  = id;\n    size++;\n    heap_sift_up(dists, idxs, pos);\n}\n\n// Replace the root of the max-heap with (val, id) and restore heap property.\n__device__ __forceinline__ void heap_replace_root(float* __restrict__ dists,\n                                                  int*   __restrict__ idxs,\n                                                  int    size,\n                                                  float  val,\n                                                  int    id)\n{\n    dists[0] = val;\n    idxs[0]  = id;\n    heap_sift_down(dists, idxs, size, 0);\n}\n\n// Heapsort (ascending) in-place on max-heap arrays of length 'k'.\n// After this, dists[0..k-1] is ascending and idxs aligned accordingly.\n__device__ __forceinline__ void heap_sort_ascending(float* __restrict__ dists,\n                                                    int*   __restrict__ idxs,\n                                                    int k)\n{\n    // Standard in-place heapsort on max-heap: repeatedly move the max to the end,\n    // shrink the heap, and sift down to restore max-heap property.\n    for (int end = k - 1; end > 0; --end) {\n        // Move current max to end\n        swap_pair(dists[0], idxs[0], dists[end], idxs[end]);\n        // Restore heap property on the reduced heap [0, end)\n        heap_sift_down(dists, idxs, end, 0);\n    }\n    // Result: ascending order (smallest at dists[0], largest at dists[k-1])\n}\n\n// CUDA kernel: warp-per-query k-NN for 2D points.\ntemplate <int WarpsPerBlock>\n__global__ void knn_kernel(const float2* __restrict__ query,\n                           int query_count,\n                           const float2* __restrict__ data,\n                           int data_count,\n                           PairIF* __restrict__ result,\n                           int k,\n                           int tile_capacity)\n{\n    // Shared memory layout (dynamic):\n    // [ float2 tile_points[tile_capacity] ][ float dists[WarpsPerBlock*k] ][ int idxs[WarpsPerBlock*k] ]\n    extern __shared__ unsigned char smem_raw[];\n    float2* s_points = reinterpret_cast<float2*>(smem_raw);\n\n    size_t points_bytes = static_cast<size_t>(tile_capacity) * sizeof(float2);\n    float* s_dists = reinterpret_cast<float*>(smem_raw + points_bytes);\n    int*   s_idxs  = reinterpret_cast<int*>(s_dists + static_cast<size_t>(WarpsPerBlock) * k);\n\n    const int tid      = threadIdx.x;\n    const int lane     = tid & (WARP_SIZE - 1);\n    const int warp     = tid >> 5; // warp index within block\n    const int warp_id  = blockIdx.x * WarpsPerBlock + warp; // global warp id ==> query index\n    const bool active  = (warp_id < query_count);\n\n    float* warp_dists = s_dists + warp * k;\n    int*   warp_idxs  = s_idxs  + warp * k;\n\n    // Load query coordinates to lane 0 and broadcast\n    float qx = 0.0f, qy = 0.0f;\n    if (lane == 0 && active) {\n        float2 q = query[warp_id];\n        qx = q.x;\n        qy = q.y;\n    }\n    qx = __shfl_sync(FULL_MASK, qx, 0);\n    qy = __shfl_sync(FULL_MASK, qy, 0);\n\n    int heap_size = 0; // maintained by lane 0 only\n\n    // Iterate over data in tiles cached in shared memory\n    for (int tile_base = 0; tile_base < data_count; tile_base += tile_capacity) {\n        const int tile_count = min(tile_capacity, data_count - tile_base);\n\n        // Block-cooperative load of data tile into shared memory\n        for (int i = tid; i < tile_count; i += blockDim.x) {\n            s_points[i] = data[tile_base + i];\n        }\n        __syncthreads(); // ensure tile loaded before use\n\n        // Each active warp processes the cached tile\n        if (active) {\n            // Process candidates in groups of 32 (one per lane), then filter+insert into heap\n            for (int t = 0; t < tile_count; t += WARP_SIZE) {\n                int local_idx = t + lane;\n                int global_idx = tile_base + local_idx;\n\n                // Compute squared Euclidean distance for this lane's candidate in the tile\n                float cand_dist = FLT_MAX;\n                int   cand_idx  = -1;\n                if (local_idx < tile_count) {\n                    float2 p = s_points[local_idx]; // shared cached point\n                    float dx = qx - p.x;\n                    float dy = qy - p.y;\n                    cand_dist = fmaf(dx, dx, dy * dy); // squared L2 distance\n                    cand_idx  = global_idx;\n                }\n\n                // Snapshot heap state (lane 0) and broadcast to lanes\n                int filled = __shfl_sync(FULL_MASK, heap_size, 0);\n                float root_val = FLT_MAX;\n                if (lane == 0) {\n                    root_val = (heap_size >= k) ? warp_dists[0] : FLT_MAX; // current worst in top-k\n                }\n                root_val = __shfl_sync(FULL_MASK, root_val, 0);\n\n                // Lanes propose candidates if either heap not full OR cand < current worst\n                unsigned propose_mask = __ballot_sync(FULL_MASK,\n                                                      (local_idx < tile_count) &&\n                                                      ((filled < k) || (cand_dist < root_val)));\n\n                // Serialize insertions to the shared per-warp heap through lane 0\n                while (propose_mask) {\n                    int src_lane = __ffs(propose_mask) - 1; // next lane to serve\n                    float d = __shfl_sync(FULL_MASK, cand_dist, src_lane);\n                    int   id = __shfl_sync(FULL_MASK, cand_idx,  src_lane);\n\n                    if (lane == 0) {\n                        if (heap_size < k) {\n                            heap_push_max(warp_dists, warp_idxs, heap_size, k, d, id);\n                        } else if (d < warp_dists[0]) {\n                            heap_replace_root(warp_dists, warp_idxs, heap_size, d, id);\n                        }\n                    }\n                    // Clear served lane\n                    propose_mask &= (propose_mask - 1);\n                }\n                __syncwarp(); // warp-level barrier for clarity before next 32-candidate group\n            }\n        }\n\n        __syncthreads(); // allow tile buffer to be reused/overwritten in next iteration\n    }\n\n    // Finalize: for each active warp, sort its heap ascending and write to result\n    if (active && lane == 0) {\n        // In case data_count < k (shouldn't happen per problem statement), heap_size may be < k.\n        // We proceed with whatever is in the heap; if smaller than k, we will only write heap_size entries.\n        int out_k = min(heap_size, k);\n\n        // Convert max-heap into ascending order via in-place heapsort\n        heap_sort_ascending(warp_dists, warp_idxs, out_k);\n\n        // Write to global memory in ascending order of distance\n        PairIF* out = result + static_cast<size_t>(warp_id) * k;\n        for (int j = 0; j < out_k; ++j) {\n            out[j].first  = warp_idxs[j];\n            out[j].second = warp_dists[j];\n        }\n\n        // If out_k < k (unlikely here), fill remaining entries with sentinel values\n        for (int j = out_k; j < k; ++j) {\n            out[j].first  = -1;\n            out[j].second = FLT_MAX;\n        }\n    }\n}\n\n} // namespace\n\n// Host entry point. Launches the kernel with appropriate shared memory and tiling configuration.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data,  int data_count,\n             std::pair<int, float> *result, int k)\n{\n    // Constants based on the chosen block configuration\n    constexpr int warps_per_block = WARPS_PER_BLOCK;\n    constexpr int threads_per_block = warps_per_block * WARP_SIZE;\n\n    // Determine the number of blocks (one warp per query)\n    int num_blocks = (query_count + warps_per_block - 1) / warps_per_block;\n    if (num_blocks <= 0) return;\n\n    // Compute required shared memory for per-warp heaps\n    // Each warp needs k floats + k ints = k * 8 bytes.\n    size_t heap_bytes_per_block = static_cast<size_t>(warps_per_block) * static_cast<size_t>(k) * (sizeof(float) + sizeof(int));\n\n    // Query device to opt-in to maximum dynamic shared memory (A100/H100 support > 96KB)\n    int dev = 0;\n    cudaGetDevice(&dev);\n    int max_optin_bytes = 0;\n    cudaDeviceGetAttribute(&max_optin_bytes, cudaDevAttrMaxSharedMemoryPerBlockOptin, dev);\n\n    // Opt-in the kernel to use the maximum available dynamic shared memory\n    cudaFuncSetAttribute(knn_kernel<warps_per_block>,\n                         cudaFuncAttributeMaxDynamicSharedMemorySize,\n                         max_optin_bytes);\n\n    // Choose a tile capacity that fits into the remaining shared memory\n    // Total dynamic shared memory per block = heap_bytes_per_block + tile_capacity * sizeof(float2)\n    // Ensure at least 1 element in tile (practically, we want more, but this guarantees correctness).\n    size_t max_tile_bytes = (heap_bytes_per_block < static_cast<size_t>(max_optin_bytes))\n                          ? static_cast<size_t>(max_optin_bytes) - heap_bytes_per_block\n                          : size_t(0);\n\n    int tile_capacity = 0;\n    if (max_tile_bytes >= sizeof(float2)) {\n        // Limit to data_count to avoid over-requesting dynamic memory unnecessarily\n        size_t max_tile_elems = max_tile_bytes / sizeof(float2);\n        if (max_tile_elems > static_cast<size_t>(data_count)) {\n            max_tile_elems = static_cast<size_t>(data_count);\n        }\n        // For coalesced loads, using a multiple of blockDim.x can be beneficial\n        size_t preferred = (max_tile_elems / threads_per_block) * threads_per_block;\n        if (preferred == 0) preferred = max_tile_elems; // if data_count < threads_per_block\n        tile_capacity = static_cast<int>(preferred);\n        if (tile_capacity <= 0) tile_capacity = 1;\n    } else {\n        // Fallback minimal tiling if shared memory is extremely constrained (shouldn't happen on A100/H100)\n        tile_capacity = 1;\n    }\n\n    // Compute the actual dynamic shared memory size for the kernel launch\n    size_t dynamic_smem_bytes = heap_bytes_per_block + static_cast<size_t>(tile_capacity) * sizeof(float2);\n\n    // Launch the kernel\n    PairIF* result_cast = reinterpret_cast<PairIF*>(result);\n    knn_kernel<warps_per_block><<<num_blocks, threads_per_block, dynamic_smem_bytes>>>(\n        query, query_count,\n        data, data_count,\n        result_cast, k, tile_capacity\n    );\n\n    // Optional: synchronize or error-check can be added by the caller if desired.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 686, "completion_tokens": 17229, "total_tokens": 17915, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 13888, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db75ddc81909c4fc3177e3666ab", "custom_id": "knn03-08", "response": {"status_code": 200, "request_id": "682347620536acfd0b92bc9faeee6fa6", "body": {"id": "chatcmpl-C4sjvQp5LeKwnivXXmleCXt6tqGca", "object": "chat.completion", "created": 1755279767, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// Efficient k-NN in 2D using one warp per query, block-level shared-memory tiling of the dataset,\n// and a per-warp max-heap of size k maintained in shared memory.\n// - Each block cooperatively loads a tile of data points into shared memory.\n// - Each warp processes one query and iterates over the tile computing distances.\n// - Within a warp, each lane computes distances for a strided subset of tile points.\n// - Candidate insertions into the per-warp heap are handled warp-synchronously by lane 0.\n// - The heap is a max-heap to allow fast pruning using the current worst (root) distance.\n// - At the end, the heap is heap-sorted to ascending order and written to the output.\n// The implementation assumes modern NVIDIA GPUs (Ampere/Hopper) and uses warp shuffles/ballots.\n\nstatic inline __device__ void heap_swap(float &ad, int &ai, float &bd, int &bi) {\n    float td = ad; ad = bd; bd = td;\n    int   ti = ai; ai = bi; bi = ti;\n}\n\nstatic inline __device__ void heap_bubble_up_max(float* __restrict__ dist, int* __restrict__ idx, int pos) {\n    // Bubble-up to maintain max-heap property (parent >= child)\n    int curr = pos;\n    while (curr > 0) {\n        int parent = (curr - 1) >> 1;\n        if (dist[parent] >= dist[curr]) break;\n        heap_swap(dist[parent], idx[parent], dist[curr], idx[curr]);\n        curr = parent;\n    }\n}\n\nstatic inline __device__ void heapify_down_max(float* __restrict__ dist, int* __restrict__ idx, int size) {\n    // Heapify-down from root for a max-heap\n    int curr = 0;\n    while (true) {\n        int left  = (curr << 1) + 1;\n        int right = left + 1;\n        if (left >= size) break;\n        int largest = left;\n        if (right < size && dist[right] > dist[left]) largest = right;\n        if (dist[curr] >= dist[largest]) break;\n        heap_swap(dist[curr], idx[curr], dist[largest], idx[largest]);\n        curr = largest;\n    }\n}\n\n__global__ void knn_warp_kernel(const float2* __restrict__ query,\n                                int query_count,\n                                const float2* __restrict__ data,\n                                int data_count,\n                                int k,\n                                int tile_points,\n                                std::pair<int, float>* __restrict__ result)\n{\n    extern __shared__ unsigned char smem[];\n    // Layout of shared memory:\n    // [data tile: tile_points * sizeof(float2)] [per-warp heaps: W*k floats][per-warp heaps: W*k ints]\n    float2* s_data = reinterpret_cast<float2*>(smem);\n\n    const int warpsPerBlock = blockDim.x >> 5;\n    const int warpIdInBlock = threadIdx.x >> 5;\n    const int lane          = threadIdx.x & 31;\n    const unsigned fullMask = __activemask();\n\n    // Heaps base pointers after the tile region\n    size_t tile_bytes = static_cast<size_t>(tile_points) * sizeof(float2);\n    unsigned char* heaps_base = smem + tile_bytes;\n\n    float* s_heap_dist = reinterpret_cast<float*>(heaps_base);\n    int*   s_heap_idx  = reinterpret_cast<int*>(s_heap_dist + static_cast<size_t>(warpsPerBlock) * k);\n\n    // Pointers to this warp's heap region\n    float* heap_dist = s_heap_dist + static_cast<size_t>(warpIdInBlock) * k;\n    int*   heap_idx  = s_heap_idx  + static_cast<size_t>(warpIdInBlock) * k;\n\n    const int globalWarpId = blockIdx.x * warpsPerBlock + warpIdInBlock;\n    const bool warp_active = (globalWarpId < query_count);\n\n    float2 q;\n    if (warp_active) {\n        q = query[globalWarpId];\n    }\n\n    // Per-warp heap size tracked by lane 0\n    int heap_size = 0;\n\n    // Process dataset in tiles; the whole block loads tiles cooperatively.\n    for (int tileStart = 0; tileStart < data_count; tileStart += tile_points) {\n        const int nTile = min(tile_points, data_count - tileStart);\n\n        // Cooperative load of data tile into shared memory\n        for (int i = threadIdx.x; i < nTile; i += blockDim.x) {\n            s_data[i] = data[tileStart + i];\n        }\n        __syncthreads();\n\n        if (warp_active) {\n            // Iterate over tile entries, striding by warp size for coalesced reuse of the tile cache\n            for (int i = lane; i < nTile; i += warpSize) {\n                float2 p = s_data[i];\n                float dx = q.x - p.x;\n                float dy = q.y - p.y;\n                float dist = fmaf(dx, dx, dy * dy); // squared L2\n\n                int idx = tileStart + i;\n\n                // Lane 0 broadcasts current threshold (root of heap) to the warp.\n                // For heap_size < k, threshold is +inf so all candidates are considered.\n                float thr = CUDART_INF_F;\n                int hs = 0;\n                if (lane == 0) {\n                    hs = heap_size;\n                    if (hs >= k) {\n                        thr = heap_dist[0];\n                    }\n                }\n                thr = __shfl_sync(fullMask, thr, 0);\n                hs  = __shfl_sync(fullMask, hs, 0);\n\n                // Warp-wide ballot to find which lanes have candidates worth considering\n                unsigned mask = __ballot_sync(fullMask, (hs < k) || (dist < thr));\n\n                // Lane 0 sequentially processes the set of candidate lanes for this micro-batch.\n                // This reduces insertion work when many candidates are pruned by the current threshold.\n                while (mask) {\n                    int srcLane = __ffs(mask) - 1;\n                    float cand_d = __shfl_sync(fullMask, dist, srcLane);\n                    int   cand_i = __shfl_sync(fullMask, idx,  srcLane);\n\n                    if (lane == 0) {\n                        if (heap_size < k) {\n                            int pos = heap_size++;\n                            heap_dist[pos] = cand_d;\n                            heap_idx[pos]  = cand_i;\n                            heap_bubble_up_max(heap_dist, heap_idx, pos);\n                        } else if (cand_d < heap_dist[0]) {\n                            heap_dist[0] = cand_d;\n                            heap_idx[0]  = cand_i;\n                            heapify_down_max(heap_dist, heap_idx, k);\n                        }\n                    }\n\n                    // Clear processed bit\n                    mask &= (mask - 1);\n                }\n            }\n        }\n\n        __syncthreads(); // Ensure the tile buffer can be reused safely\n    }\n\n    // Finalize: per-warp heap sort to ascending order and store results\n    if (warp_active && lane == 0) {\n        // Ensure heap is fully built; since data_count >= k, heap_size should be k.\n        // But we guard anyway.\n        int n = heap_size;\n        if (n > 1) {\n            // Standard heapsort over a max-heap yields ascending order.\n            for (int i = n - 1; i > 0; --i) {\n                heap_swap(heap_dist[0], heap_idx[0], heap_dist[i], heap_idx[i]);\n                // Restore heap property on range [0, i)\n                int curr = 0;\n                while (true) {\n                    int left  = (curr << 1) + 1;\n                    int right = left + 1;\n                    if (left >= i) break;\n                    int largest = left;\n                    if (right < i && heap_dist[right] > heap_dist[left]) largest = right;\n                    if (heap_dist[curr] >= heap_dist[largest]) break;\n                    heap_swap(heap_dist[curr], heap_idx[curr], heap_dist[largest], heap_idx[largest]);\n                    curr = largest;\n                }\n            }\n        }\n\n        // If, for any reason, heap_size < k (shouldn't happen per assumptions), pad with infinities/invalid indices.\n        // Then write results.\n        std::pair<int, float>* out = result + static_cast<size_t>(globalWarpId) * k;\n        int outN = min(n, k);\n        for (int j = 0; j < outN; ++j) {\n            out[j].first  = heap_idx[j];\n            out[j].second = heap_dist[j];\n        }\n        for (int j = outN; j < k; ++j) {\n            out[j].first  = -1;\n            out[j].second = CUDART_INF_F;\n        }\n    }\n}\n\n// Host-side launcher.\n// This function configures the kernel launch parameters, selects the number of warps per block,\n// and the tile size for shared-memory caching. It also opts the kernel into using the maximum\n// available dynamic shared memory when supported (Ampere/Hopper).\nvoid run_knn(const float2 *query,\n             int query_count,\n             const float2 *data,\n             int data_count,\n             std::pair<int, float> *result,\n             int k)\n{\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    int dev = 0;\n    cudaGetDevice(&dev);\n\n    int maxSmemDefault = 0;\n    int maxSmemOptin   = 0;\n    cudaDeviceGetAttribute(&maxSmemDefault, cudaDevAttrMaxSharedMemoryPerBlock, dev);\n    cudaDeviceGetAttribute(&maxSmemOptin,   cudaDevAttrMaxSharedMemoryPerBlockOptin, dev);\n\n    // Attempt to allow the kernel to use the opt-in maximum dynamic shared memory.\n    if (maxSmemOptin > 0) {\n        cudaFuncSetAttribute(knn_warp_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, maxSmemOptin);\n    }\n\n    // Use the largest available shared memory capacity for dynamic allocation.\n    size_t smemLimit = (maxSmemOptin > 0) ? static_cast<size_t>(maxSmemOptin)\n                                          : static_cast<size_t>(maxSmemDefault);\n\n    // Per-warp heap bytes (dist + idx)\n    const size_t heapBytesPerWarp = static_cast<size_t>(k) * (sizeof(float) + sizeof(int));\n\n    // Choose warps per block (W). Try up to 8 warps/block. Reduce if shared memory is tight.\n    int chosenWarps = 0;\n    int tile_points = 0;\n\n    for (int W = 8; W >= 1; --W) {\n        size_t heapsBytes = heapBytesPerWarp * static_cast<size_t>(W);\n        if (heapsBytes >= smemLimit) continue; // need room for at least one data point tile entry\n\n        // Allocate the remaining shared memory to the data tile.\n        size_t remaining = smemLimit - heapsBytes;\n        int tp = static_cast<int>(remaining / sizeof(float2));\n        if (tp >= 1) {\n            // Prefer larger tiles but keep reasonable minimum; no explicit upper cap needed.\n            chosenWarps = W;\n            tile_points = tp;\n            break;\n        }\n    }\n\n    // Fallback safety (should not trigger on A100/H100); ensure at least one warp and tile point.\n    if (chosenWarps == 0) {\n        chosenWarps = 1;\n        tile_points = 1;\n    }\n\n    const int threadsPerBlock = chosenWarps * 32;\n    const dim3 block(threadsPerBlock, 1, 1);\n    const dim3 grid((query_count + chosenWarps - 1) / chosenWarps, 1, 1);\n\n    // Dynamic shared memory size: tile bytes + heap bytes for all warps in block\n    size_t dynamicSmemBytes = static_cast<size_t>(tile_points) * sizeof(float2)\n                            + heapBytesPerWarp * static_cast<size_t>(chosenWarps);\n\n    knn_warp_kernel<<<grid, block, dynamicSmemBytes>>>(query, query_count, data, data_count, k, tile_points, result);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 686, "completion_tokens": 16552, "total_tokens": 17238, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 13888, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db760b8819086323d0b0efd8a4e", "custom_id": "knn03-09", "response": {"status_code": 200, "request_id": "f202445b4a709a0483fdf33b6f42d751", "body": {"id": "chatcmpl-C4slNpaUB5qjdSt43c400t34uQnGl", "object": "chat.completion", "created": 1755279857, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// Optimized CUDA k-NN (squared L2) for 2D points.\n// - One warp (32 threads) processes one query.\n// - The block loads data points in tiles into shared memory.\n// - Each lane keeps L = k/32 local best candidates for the current tile in registers.\n// - Warps merge the 32 local lists into a sorted buffer in shared memory.\n// - Merge the sorted tile buffer with the warp's global best (size k) stored in shared memory.\n// - Repeat for all tiles, then write the final k-NN results for each query.\n//\n// Notes:\n// - k is a power of two in [32, 1024], so L is an integer in [1, 32].\n// - Shared memory usage per block:\n//     tile_points: TILE_POINTS * sizeof(float2)\n//     per-warp buffers (dist+idx for each entry): 3 buffers * WARPS_PER_BLOCK * k * (sizeof(float)+sizeof(int))\n//   With the parameters below (TILE_POINTS=4096, WARPS_PER_BLOCK=4), the maximum is ~128 KB for k=1024.\n//\n// Compile with a recent CUDA toolkit. Use cudaFuncSetAttribute to opt-in to large shared memory if necessary.\n\nnamespace {\nconstexpr int WARP_SIZE = 32;\nconstexpr int WARPS_PER_BLOCK = 4;       // 4 warps -> 128 threads per block\nconstexpr int BLOCK_THREADS = WARPS_PER_BLOCK * WARP_SIZE;\nconstexpr int TILE_POINTS = 4096;        // Number of data points cached per block iteration\nconstexpr unsigned FULL_MASK = 0xFFFFFFFFu;\n}\n\n// Insert a candidate (dist, idx) into an ascending-sorted fixed-size array (length L).\n// If dist is not smaller than the current worst (last entry), it is discarded.\n// Arrays top_dist[0..L-1], top_idx[0..L-1] are always kept ascending.\n__device__ __forceinline__\nvoid insert_into_top_ascending(float dist, int idx, float* top_dist, int* top_idx, int L)\n{\n    if (L <= 0) return;\n    if (dist >= top_dist[L - 1]) return;\n\n    // Find insertion position by shifting elements to the right.\n    int j = L - 1;\n    // Move larger elements up to make room for 'dist'\n    for (; j > 0 && top_dist[j - 1] > dist; --j) {\n        top_dist[j] = top_dist[j - 1];\n        top_idx[j]  = top_idx[j - 1];\n    }\n    top_dist[j] = dist;\n    top_idx[j]  = idx;\n}\n\n// Merge two ascending-sorted sequences from src into dst.\n// left:  src_dist[left_start .. left_start + left_len - 1]\n// right: src_dist[right_start .. right_start + right_len - 1]\n// Output ascending to dst_dist[dst_start .. dst_start + (left_len+right_len) - 1]\n__device__ __forceinline__\nvoid merge_two_sequences_ascending(\n    const float* __restrict__ src_dist, const int* __restrict__ src_idx,\n    float* __restrict__ dst_dist,       int* __restrict__ dst_idx,\n    int left_start, int left_len, int right_start, int right_len, int dst_start)\n{\n    int i = 0, j = 0, o = 0;\n    while (i < left_len && j < right_len) {\n        float dl = src_dist[left_start + i];\n        float dr = src_dist[right_start + j];\n        if (dl <= dr) {\n            dst_dist[dst_start + o] = dl;\n            dst_idx [dst_start + o] = src_idx[left_start + i];\n            ++i; ++o;\n        } else {\n            dst_dist[dst_start + o] = dr;\n            dst_idx [dst_start + o] = src_idx[right_start + j];\n            ++j; ++o;\n        }\n    }\n    // Copy remaining items (at most one of the loops executes)\n    while (i < left_len) {\n        dst_dist[dst_start + o] = src_dist[left_start + i];\n        dst_idx [dst_start + o] = src_idx[left_start + i];\n        ++i; ++o;\n    }\n    while (j < right_len) {\n        dst_dist[dst_start + o] = src_dist[right_start + j];\n        dst_idx [dst_start + o] = src_idx[right_start + j];\n        ++j; ++o;\n    }\n}\n\n// Kernel implementing k-NN for 2D points with squared L2 distances.\n__global__ void knn_kernel_2d(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data,  int data_count,\n    std::pair<int, float>* __restrict__ result,\n    int k)\n{\n    extern __shared__ unsigned char smem_raw[];\n    unsigned char* smem_ptr = smem_raw;\n\n    // Layout of shared memory:\n    // [ tile_points (float2) | gbest_dist (float) | gbest_idx (int) | tA_dist (float) | tA_idx (int) | tB_dist (float) | tB_idx (int) ]\n    float2* tile_points = reinterpret_cast<float2*>(smem_ptr);\n    smem_ptr += sizeof(float2) * TILE_POINTS;\n\n    float* gbest_dist_base = reinterpret_cast<float*>(smem_ptr);\n    smem_ptr += sizeof(float) * WARPS_PER_BLOCK * k;\n\n    int* gbest_idx_base = reinterpret_cast<int*>(smem_ptr);\n    smem_ptr += sizeof(int) * WARPS_PER_BLOCK * k;\n\n    float* tA_dist_base = reinterpret_cast<float*>(smem_ptr);\n    smem_ptr += sizeof(float) * WARPS_PER_BLOCK * k;\n\n    int* tA_idx_base = reinterpret_cast<int*>(smem_ptr);\n    smem_ptr += sizeof(int) * WARPS_PER_BLOCK * k;\n\n    float* tB_dist_base = reinterpret_cast<float*>(smem_ptr);\n    smem_ptr += sizeof(float) * WARPS_PER_BLOCK * k;\n\n    int* tB_idx_base = reinterpret_cast<int*>(smem_ptr);\n    // smem_ptr += sizeof(int) * WARPS_PER_BLOCK * k; // not used further\n\n    const int tid  = threadIdx.x;\n    const int lane = tid & (WARP_SIZE - 1);\n    const int warp_in_block = tid >> 5;                        // warp index within block [0..WARPS_PER_BLOCK-1]\n    const int warp_global   = blockIdx.x * WARPS_PER_BLOCK + warp_in_block;\n    const bool active       = (warp_global < query_count);\n    const int L = k / WARP_SIZE;                               // per-lane capacity\n\n    // Per-warp slices of shared memory arrays\n    const int warp_offset = warp_in_block * k;\n    float* gbest_dist = gbest_dist_base + warp_offset;\n    int*   gbest_idx  = gbest_idx_base  + warp_offset;\n    float* tA_dist    = tA_dist_base    + warp_offset;\n    int*   tA_idx     = tA_idx_base     + warp_offset;\n    float* tB_dist    = tB_dist_base    + warp_offset;\n    int*   tB_idx     = tB_idx_base     + warp_offset;\n\n    // Initialize the warp's global best buffers to +inf and -1\n    const float INF = CUDART_INF_F;\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        gbest_dist[i] = INF;\n        gbest_idx [i] = -1;\n    }\n    __syncwarp();\n\n    // Load query point (lane 0 loads from global, broadcast to warp)\n    float qx = 0.f, qy = 0.f;\n    if (active && lane == 0) {\n        float2 q = query[warp_global];\n        qx = q.x; qy = q.y;\n    }\n    qx = __shfl_sync(FULL_MASK, qx, 0);\n    qy = __shfl_sync(FULL_MASK, qy, 0);\n\n    // Iterate over data in tiles\n    for (int base = 0; base < data_count; base += TILE_POINTS) {\n        int tile_n = data_count - base;\n        if (tile_n > TILE_POINTS) tile_n = TILE_POINTS;\n\n        // Block-wide load of tile data into shared memory\n        for (int t = tid; t < tile_n; t += blockDim.x) {\n            tile_points[t] = data[base + t];\n        }\n        __syncthreads(); // ensure tile is loaded before any warp uses it\n\n        // Each lane initializes its local top-L list in registers\n        float local_dist[32];\n        int   local_idx [32];\n        #pragma unroll\n        for (int i = 0; i < 32; ++i) {\n            if (i < L) {\n                local_dist[i] = INF;\n                local_idx [i] = -1;\n            }\n        }\n\n        // Compute distances for this warp's query vs. all points in the tile;\n        // each lane strides over the tile by WARP_SIZE.\n        if (active) {\n            for (int i = lane; i < tile_n; i += WARP_SIZE) {\n                float2 p = tile_points[i];\n                float dx = p.x - qx;\n                float dy = p.y - qy;\n                float d2 = fmaf(dx, dx, dy * dy); // squared L2 distance\n                int   idx = base + i;\n                insert_into_top_ascending(d2, idx, local_dist, local_idx, L);\n            }\n        }\n\n        // Store each lane's local top-L into the warp's tile buffer A (concatenate lanes)\n        if (active) {\n            const int lane_out_start = lane * L;\n            #pragma unroll\n            for (int i = 0; i < 32; ++i) {\n                if (i < L) {\n                    tA_dist[lane_out_start + i] = local_dist[i];\n                    tA_idx [lane_out_start + i] = local_idx [i];\n                }\n            }\n        }\n        __syncwarp(); // ensure all lanes wrote their L candidates\n\n        // Merge 32 ascending sequences of length L into one ascending sequence of length k.\n        // We perform log2(32)=5 merge stages, alternating between tA and tB as src/dst.\n        // At stage s: sequence length = L * (1 << s); there are pair_count = 32 >> (s+1) merges.\n        if (active) {\n            bool src_is_A = true;\n            int stages = 0;\n            // Handle L possibly equal to 0 (shouldn't happen as k>=32), but guard anyway\n            if (L > 0) {\n                for (int s = 0; s < 5; ++s) {\n                    int seq_len = L << s;\n                    int pair_count = 32 >> (s + 1);\n                    if (pair_count <= 0) break;\n\n                    if (lane < pair_count) {\n                        int left_start  = (lane * 2) * seq_len;\n                        int right_start = left_start + seq_len;\n                        int dst_start   = left_start;\n                        if (src_is_A) {\n                            merge_two_sequences_ascending(\n                                tA_dist, tA_idx,\n                                tB_dist, tB_idx,\n                                left_start,  seq_len,\n                                right_start, seq_len,\n                                dst_start);\n                        } else {\n                            merge_two_sequences_ascending(\n                                tB_dist, tB_idx,\n                                tA_dist, tA_idx,\n                                left_start,  seq_len,\n                                right_start, seq_len,\n                                dst_start);\n                        }\n                    }\n                    __syncwarp();\n                    src_is_A = !src_is_A;\n                    ++stages;\n                }\n            }\n\n            // After merging, the final sorted tile buffer is in the source of the last stage toggle.\n            // If stages is even: src_is_A == true => tA holds final; else tB holds final.\n            float* tile_sorted_dist = src_is_A ? tA_dist : tB_dist;\n            int*   tile_sorted_idx  = src_is_A ? tA_idx  : tB_idx;\n\n            // Merge the sorted tile candidates (k) with the warp's global best (k),\n            // keeping the k smallest. Let lane 0 perform the sequential 2-way merge,\n            // then all lanes copy back to gbest in parallel.\n            float* merge_dst_dist = src_is_A ? tB_dist : tA_dist; // use the other buffer as temporary destination\n            int*   merge_dst_idx  = src_is_A ? tB_idx  : tA_idx;\n\n            if (lane == 0) {\n                int a = 0, b = 0, o = 0;\n                while (o < k) {\n                    float da = gbest_dist[a];\n                    float db = tile_sorted_dist[b];\n                    // Choose the smaller value, guarding index bounds.\n                    bool take_a = (b >= k) || (a < k && da <= db);\n                    if (take_a) {\n                        merge_dst_dist[o] = da;\n                        merge_dst_idx [o] = gbest_idx[a];\n                        ++a; ++o;\n                    } else {\n                        merge_dst_dist[o] = db;\n                        merge_dst_idx [o] = tile_sorted_idx[b];\n                        ++b; ++o;\n                    }\n                }\n            }\n            __syncwarp();\n\n            // Copy merged results back to global best cooperatively (strided by lane)\n            for (int i = lane; i < k; i += WARP_SIZE) {\n                gbest_dist[i] = merge_dst_dist[i];\n                gbest_idx [i] = merge_dst_idx [i];\n            }\n        }\n\n        __syncthreads(); // ensure all warps finished with the tile before loading the next one\n    }\n\n    // Write final results for active queries.\n    if (active) {\n        const int out_base = warp_global * k;\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            std::pair<int, float> p;\n            p.first  = gbest_idx[i];\n            p.second = gbest_dist[i];\n            result[out_base + i] = p;\n        }\n    }\n}\n\n// Host API: run_knn\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k)\n{\n    // Grid: one warp per query.\n    dim3 block(BLOCK_THREADS, 1, 1);\n    dim3 grid((query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK, 1, 1);\n\n    // Dynamic shared memory size:\n    // tile: TILE_POINTS * sizeof(float2)\n    // per-warp buffers: 3 buffers * WARPS_PER_BLOCK * k * (sizeof(float) + sizeof(int))\n    size_t smem_bytes = sizeof(float2) * size_t(TILE_POINTS)\n                      + size_t(24) * size_t(WARPS_PER_BLOCK) * size_t(k); // 24 = 6 arrays * 4 bytes each\n\n    // Opt-in larger dynamic shared memory if needed (for A100/H100 this is available).\n    int device = 0;\n    cudaGetDevice(&device);\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&prop, device);\n\n    // Set the kernel attribute to allow the required dynamic shared memory.\n    // On recent devices, sharedMemPerBlockOptin typically >= 163840 bytes (160 KB).\n    int smem_bytes_int = static_cast<int>(smem_bytes);\n    int max_optin = static_cast<int>(prop.sharedMemPerBlockOptin);\n    if (smem_bytes_int > max_optin) {\n        // If requested exceeds opt-in maximum, clamp to maximum to avoid an error.\n        // Kernel launch will fail if smem_bytes > configured limit, so choose parameters accordingly.\n        smem_bytes_int = max_optin;\n    }\n    cudaFuncSetAttribute(\n        knn_kernel_2d,\n        cudaFuncAttributeMaxDynamicSharedMemorySize,\n        smem_bytes_int\n    );\n\n    // Launch the kernel on the default stream.\n    knn_kernel_2d<<<grid, block, smem_bytes>>>(\n        query, query_count,\n        data,  data_count,\n        result, k\n    );\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 686, "completion_tokens": 16737, "total_tokens": 17423, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 13184, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db777b08190aacd218065e54d18", "custom_id": "knn03-10", "response": {"status_code": 200, "request_id": "aa4a17ae7306af5cab12425bc856c9fe", "body": {"id": "chatcmpl-C4sltzxiOyE5s534caxJPmz6xim2G", "object": "chat.completion", "created": 1755279889, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// Optimized k-NN for 2D points using warp-per-query, shared-memory tiling, and warp-synchronous updates.\n// - Each warp processes one query point and maintains a private top-k buffer in shared memory.\n// - The data points are processed in tiles cached in shared memory by the entire block.\n// - Within each tile, each warp computes distances for its query and updates its top-k set by replacing the current maximum when needed.\n// - The top-k buffer is stored unsorted during accumulation; after the full scan, it is sorted in ascending order via a bitonic sort and written to the output.\n//\n// Assumptions:\n// - k is a power of two between 32 and 1024.\n// - data_count >= k.\n// - query_count is typically large (thousands), data_count is very large (millions).\n// - Memory for query, data, and result is allocated by cudaMalloc.\n// - No dynamic device allocations; only shared memory is used within the kernel.\n\n#ifndef KNNCUDA_UTILS\n#define KNNCUDA_UTILS\n\n// Tunables selected to balance shared memory usage and occupancy.\n// TILE_POINTS * sizeof(float2) = 4096 * 8 = 32 KB\n// Per-warp top-k storage (distance+index): k * 8 bytes. For k=1024 -> 8 KB per warp.\n// With WARPS_PER_BLOCK=8 -> 64 KB for top-k buffers.\n// Total shared memory per block = 32 KB + 64 KB = 96 KB (fits typical 96 KB per-block budget).\nstatic constexpr int TILE_POINTS = 4096;\nstatic constexpr int WARPS_PER_BLOCK = 8;\nstatic constexpr int THREADS_PER_BLOCK = WARPS_PER_BLOCK * 32;\n\nstatic __device__ __forceinline__ float sqr(const float x) { return x * x; }\n\n// Recompute the maximum value and its index across an array arr[0..k-1] cooperatively within a warp.\n// Each lane scans a strided subset, then a warp-wide reduction selects the maximum and its index.\n// The resulting max_val and max_pos are broadcast to all lanes via shuffles.\nstatic __device__ __forceinline__\nvoid warp_recompute_max(const float* __restrict__ arr, int k, int lane, float &max_val, int &max_pos)\n{\n    float local_max = -CUDART_INF_F;\n    int local_idx = -1;\n\n    // Strided scan across k entries: each lane covers indices i = lane, lane+32, ...\n    for (int i = lane; i < k; i += 32) {\n        float v = arr[i];\n        if (v > local_max) {\n            local_max = v;\n            local_idx = i;\n        }\n    }\n\n    // Warp-wide reduction to find the global maximum and its position.\n    // Ties are broken by favoring the larger index for determinism (optional).\n    unsigned mask = 0xffffffffu;\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        float other_val = __shfl_down_sync(mask, local_max, offset);\n        int other_idx = __shfl_down_sync(mask, local_idx, offset);\n        if (other_val > local_max || (other_val == local_max && other_idx > local_idx)) {\n            local_max = other_val;\n            local_idx = other_idx;\n        }\n    }\n    // Broadcast the final max value and index from lane 0 to all lanes in the warp.\n    max_val = __shfl_sync(mask, local_max, 0);\n    max_pos = __shfl_sync(mask, local_idx, 0);\n}\n\n// Warp-level argmin among current candidates; ineligible lanes set their value to +INF.\n// Returns the minimum value and the lane that holds it (broadcast to all lanes).\nstatic __device__ __forceinline__\nvoid warp_argmin(float my_val, bool eligible, float &min_val, int &min_lane)\n{\n    unsigned mask = 0xffffffffu;\n    float v = eligible ? my_val : CUDART_INF_F;\n    int lane = threadIdx.x & 31;\n    int idx = lane;\n\n    // Warp-wide reduction for minimum.\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        float other_val = __shfl_down_sync(mask, v, offset);\n        int other_idx = __shfl_down_sync(mask, idx, offset);\n        if (other_val < v) {\n            v = other_val;\n            idx = other_idx;\n        }\n    }\n    min_val = __shfl_sync(mask, v, 0);\n    min_lane = __shfl_sync(mask, idx, 0);\n}\n\n// Bitonic sort of k elements (power-of-two) in ascending order across the warp, operating on shared memory arrays.\n// Each lane processes elements in a strided fashion; __syncwarp is used to ensure ordering between stages.\nstatic __device__ __forceinline__\nvoid warp_bitonic_sort(float* __restrict__ dist, int* __restrict__ idx, int k)\n{\n    unsigned mask = 0xffffffffu;\n    int lane = threadIdx.x & 31;\n\n    // Bitonic sort network: k must be a power of two.\n    for (int size = 2; size <= k; size <<= 1) {\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            for (int i = lane; i < k; i += 32) {\n                int partner = i ^ stride;\n                if (partner > i) { // ensure each pair is handled once\n                    bool ascending = ((i & size) == 0);\n                    float a = dist[i];\n                    float b = dist[partner];\n                    int ai = idx[i];\n                    int bi = idx[partner];\n\n                    bool do_swap = (a > b) == ascending;\n                    if (do_swap) {\n                        dist[i] = b; dist[partner] = a;\n                        idx[i]  = bi; idx[partner]  = ai;\n                    }\n                }\n            }\n            __syncwarp(mask);\n        }\n    }\n}\n\n#endif // KNNCUDA_UTILS\n\n// CUDA kernel implementing k-NN for 2D points.\n__global__ void knn2d_kernel(const float2* __restrict__ query,\n                              int query_count,\n                              const float2* __restrict__ data,\n                              int data_count,\n                              std::pair<int, float>* __restrict__ result,\n                              int k)\n{\n    // Warp identification\n    const int lane = threadIdx.x & 31;\n    const int warp_in_block = threadIdx.x >> 5; // 0..WARPS_PER_BLOCK-1\n    const int warps_per_block = blockDim.x >> 5; // should be WARPS_PER_BLOCK\n    const int warp_global = blockIdx.x * warps_per_block + warp_in_block;\n\n    // Per-warp activity flag. Even inactive warps must participate in __syncthreads for safe shared-memory tiling.\n    const bool warp_active = (warp_global < query_count);\n\n    // Dynamic shared memory layout:\n    // [ float2 tile[TILE_POINTS] ][ float dist[WARPS_PER_BLOCK * k] ][ int idx[WARPS_PER_BLOCK * k] ]\n    extern __shared__ unsigned char smem_raw[];\n    unsigned char* smem_ptr = smem_raw;\n\n    // Shared tile for data points\n    float2* s_tile = reinterpret_cast<float2*>(smem_ptr);\n    size_t tile_bytes = static_cast<size_t>(TILE_POINTS) * sizeof(float2);\n    smem_ptr += tile_bytes;\n\n    // Align next pointers to 16 bytes for safety\n    size_t misalign = reinterpret_cast<size_t>(smem_ptr) & 0xF;\n    if (misalign) smem_ptr += (16 - misalign);\n\n    float* s_topk_dist = reinterpret_cast<float*>(smem_ptr);\n    size_t dist_bytes = static_cast<size_t>(warps_per_block) * k * sizeof(float);\n    smem_ptr += dist_bytes;\n\n    misalign = reinterpret_cast<size_t>(smem_ptr) & 0xF;\n    if (misalign) smem_ptr += (16 - misalign);\n\n    int* s_topk_idx = reinterpret_cast<int*>(smem_ptr);\n    // size_t idx_bytes = static_cast<size_t>(warps_per_block) * k * sizeof(int); // not needed further\n\n    // Pointers to this warp's top-k buffers within shared memory\n    float* topk_dist = s_topk_dist + warp_in_block * k;\n    int*   topk_idx  = s_topk_idx + warp_in_block * k;\n\n    // Load this warp's query point into registers (broadcast from lane 0).\n    float qx = 0.0f, qy = 0.0f;\n    if (warp_active) {\n        if (lane == 0) {\n            float2 q = query[warp_global];\n            qx = q.x;\n            qy = q.y;\n        }\n        unsigned mask = 0xffffffffu;\n        qx = __shfl_sync(mask, qx, 0);\n        qy = __shfl_sync(mask, qy, 0);\n    }\n\n    // State for maintaining the top-k buffer.\n    int filled = 0;             // number of initial elements filled\n    float cur_max_val = CUDART_INF_F;\n    int cur_max_pos = -1;\n\n    // Process data in tiles\n    for (int tile_base = 0; tile_base < data_count; tile_base += TILE_POINTS) {\n        int tile_count = data_count - tile_base;\n        if (tile_count > TILE_POINTS) tile_count = TILE_POINTS;\n\n        // Cooperative load of the tile into shared memory by the whole block.\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            s_tile[i] = data[tile_base + i];\n        }\n        __syncthreads();\n\n        if (warp_active) {\n            // Stage 1: Fill initial K items into the buffer without selective replacement.\n            int remaining_to_fill = k - filled;\n            int fill_now = (remaining_to_fill > 0) ? ((remaining_to_fill < tile_count) ? remaining_to_fill : tile_count) : 0;\n\n            // Each lane fills strided positions of the initial block [filled, filled + fill_now)\n            for (int i = lane; i < fill_now; i += 32) {\n                float2 p = s_tile[i];\n                float dx = p.x - qx;\n                float dy = p.y - qy;\n                float d = fmaf(dy, dy, dx * dx);\n                topk_dist[filled + i] = d;\n                topk_idx [filled + i] = tile_base + i;\n            }\n            // Ensure the fills are visible to other lanes in this warp before recomputing max.\n            __syncwarp();\n\n            filled += fill_now;\n\n            // If we've just completed the initial fill, compute the current max and its position.\n            if (filled == k && cur_max_pos < 0) {\n                warp_recompute_max(topk_dist, k, lane, cur_max_val, cur_max_pos);\n            }\n\n            // Stage 2: For the remainder of this tile, perform selective replacements.\n            int start = fill_now; // start index in this tile after initial fill\n            // Iterate over the tile in chunks of 32 so each lane considers one candidate at a time.\n            for (int base = start; base < tile_count; base += 32) {\n                int pos = base + lane;\n                bool valid = (pos < tile_count);\n                float my_dist = CUDART_INF_F;\n                int my_idx = -1;\n\n                if (valid) {\n                    float2 p = s_tile[pos];\n                    float dx = p.x - qx;\n                    float dy = p.y - qy;\n                    my_dist = fmaf(dy, dy, dx * dx);\n                    my_idx = tile_base + pos;\n                }\n\n                // If initial fill not complete yet (possible if k > tile_count and more tiles needed),\n                // keep filling buffer from these candidates instead of selective replacement.\n                if (filled < k) {\n                    // We need to place up to (k - filled) items from this 'base' chunk.\n                    int remaining = k - filled;\n                    unsigned mask = 0xffffffffu;\n\n                    // We will select up to 'remaining' best among the current 32 candidates (or fewer if many invalid).\n                    // Do so by repeatedly picking the minimum and writing it into the next slot.\n                    for (int take = 0; take < 32 && remaining > 0; ++take) {\n                        // Compute the minimum among currently valid candidates (my_dist) and consume it.\n                        float best_val;\n                        int best_lane;\n                        warp_argmin(my_dist, valid, best_val, best_lane);\n\n                        // If no valid candidates remain in this chunk, break.\n                        if (!isfinite(best_val)) break;\n\n                        // Extract the winning candidate's index.\n                        int best_idx = __shfl_sync(mask, my_idx, best_lane);\n\n                        // Store into the next slot in top-k buffer.\n                        if (lane == 0) {\n                            topk_dist[filled] = best_val;\n                            topk_idx [filled] = best_idx;\n                        }\n                        __syncwarp(mask);\n                        filled++;\n                        remaining--;\n\n                        // Mark the winning lane's candidate as consumed so it won't be picked again.\n                        if (lane == best_lane) {\n                            valid = false;\n                            my_dist = CUDART_INF_F;\n                        }\n\n                        if (filled == k) {\n                            // Compute initial maximum once the buffer is fully filled.\n                            warp_recompute_max(topk_dist, k, lane, cur_max_val, cur_max_pos);\n                            break; // proceed to selective replacement for remaining candidates (if any)\n                        }\n                    }\n                    // If still not filled, continue to next 32-candidate block; selective replacement not yet active.\n                    if (filled < k) {\n                        continue;\n                    }\n                    // Else, fall through to selective replacement for any leftover candidates in this block.\n                }\n\n                // Selective replacement: repeatedly replace the current maximum with the smallest candidate\n                // that is less than the current maximum, until no candidate in this set qualifies.\n                if (cur_max_pos >= 0) {\n                    unsigned mask = 0xffffffffu;\n                    while (true) {\n                        bool eligible = valid && (my_dist < cur_max_val);\n                        unsigned any = __ballot_sync(mask, eligible);\n                        if (any == 0u) break; // no qualifying candidates remain in this 32-candidate set\n\n                        // Among eligible candidates, pick the minimum distance to replace the current maximum.\n                        float best_val;\n                        int best_lane;\n                        warp_argmin(my_dist, eligible, best_val, best_lane);\n\n                        // Retrieve index of the winning lane's candidate.\n                        int best_idx = __shfl_sync(mask, my_idx, best_lane);\n\n                        // Replace current maximum in the top-k buffer with the new candidate.\n                        if (lane == 0) {\n                            topk_dist[cur_max_pos] = best_val;\n                            topk_idx [cur_max_pos] = best_idx;\n                        }\n                        __syncwarp(mask);\n\n                        // Recompute the new maximum across the top-k buffer.\n                        warp_recompute_max(topk_dist, k, lane, cur_max_val, cur_max_pos);\n\n                        // Mark the winning lane as consumed for this block (so it won't be reconsidered here).\n                        if (lane == best_lane) {\n                            valid = false;\n                        }\n                    }\n                }\n            } // end per-32-candidate chunk within tile\n        } // end if warp_active\n\n        __syncthreads();\n    } // end tiles loop\n\n    // After processing all tiles, for active warps, sort the warp's top-k buffer in ascending order and write results.\n    if (warp_active) {\n        // Sort ascending by distance\n        warp_bitonic_sort(topk_dist, topk_idx, k);\n\n        // Write sorted results to global memory\n        for (int i = lane; i < k; i += 32) {\n            int out_pos = warp_global * k + i;\n            result[out_pos].first  = topk_idx[i];\n            result[out_pos].second = topk_dist[i];\n        }\n    }\n}\n\n// Host interface: launches the kernel.\n// query: device pointer to float2 queries, count q\n// data:  device pointer to float2 data points, count n\n// result: device pointer to std::pair<int,float> of size q*k\nextern \"C\"\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k)\n{\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Launch configuration\n    dim3 block(THREADS_PER_BLOCK);\n    dim3 grid((query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK);\n\n    // Compute dynamic shared memory size:\n    // tile + per-warp top-k distances + per-warp top-k indices (with alignment padding)\n    size_t smem = 0;\n    smem += static_cast<size_t>(TILE_POINTS) * sizeof(float2); // tile\n    // Align to 16 bytes\n    smem = (smem + 15) & ~static_cast<size_t>(15);\n    smem += static_cast<size_t>(WARPS_PER_BLOCK) * k * sizeof(float); // distances\n    // Align to 16 bytes\n    smem = (smem + 15) & ~static_cast<size_t>(15);\n    smem += static_cast<size_t>(WARPS_PER_BLOCK) * k * sizeof(int);   // indices\n\n    // Launch kernel\n    knn2d_kernel<<<grid, block, smem>>>(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 686, "completion_tokens": 20340, "total_tokens": 21026, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 16512, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db872288190bcbd89621ed39d8c", "custom_id": "knn04-01", "response": {"status_code": 200, "request_id": "9f8478068a2d35a8cd8927f044d64eb2", "body": {"id": "chatcmpl-C4sltpSDPowZipAtvmovywuzvz7T2", "object": "chat.completion", "created": 1755279889, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cuda_runtime_api.h>\n#include <utility>\n\n// This implementation assigns one warp (32 threads) to process one query point.\n// The data points are processed in tiles loaded into shared memory by the whole block.\n// Each warp maintains an intermediate private top-k (indices and distances) in shared memory,\n// updated using warp-synchronous operations. After scanning all data points, the per-warp\n// top-k list is sorted using a warp-synchronous bitonic sort and written to the output.\n\n// Tunable parameters (chosen for modern data center GPUs like A100/H100)\n#ifndef KNN_WARPS_PER_BLOCK\n#define KNN_WARPS_PER_BLOCK 8                // 8 warps per block => 256 threads per block\n#endif\n\n#ifndef KNN_THREADS_PER_BLOCK\n#define KNN_THREADS_PER_BLOCK (KNN_WARPS_PER_BLOCK * 32)\n#endif\n\n// Helper: warp size constant\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Device helper: warp-synchronous argmax reduction on values, returning the max value\n// in lane 0 and the lane index where the maximum resides (also in lane 0).\n// All lanes participate with the same mask; the output is valid in lane 0.\n// After calling this, broadcast with __shfl_sync(mask, value, 0) to share among lanes.\n__device__ __forceinline__\nvoid warp_argmax_lane0(float val_in, int lane_id, float &max_val_out_lane0, int &max_lane_out_lane0) {\n    unsigned mask = 0xffffffffu;\n    float best_val = val_in;\n    int   best_lane = lane_id;\n\n    // Reduce using pair (value, lane) with preference for larger value\n    // Note: Since distances are non-negative, no special NaN handling is needed here.\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        float other_val = __shfl_down_sync(mask, best_val, offset);\n        int   other_lane = __shfl_down_sync(mask, best_lane, offset);\n        if (other_val > best_val) {\n            best_val = other_val;\n            best_lane = other_lane;\n        }\n    }\n    if (lane_id == 0) {\n        max_val_out_lane0 = best_val;\n        max_lane_out_lane0 = best_lane;\n    }\n}\n\n// Device helper: recompute the local (per-lane) maximum within a lane's contiguous slice\n// of the warp's top-k shared array.\n// Each warp owns 'k' entries in shared memory. Each lane owns a slice of L = k / 32 entries.\n// Arguments:\n//   topk_dists_base: pointer to the beginning of this warp's distance array in shared memory\n//   L:               entries per lane (k / 32)\n//   lane_id:         0..31 lane index\n// Outputs:\n//   local_max_val:   maximum distance value found in this lane's slice\n//   local_max_pos:   position within the lane's slice [0, L) where the maximum resides\n__device__ __forceinline__\nvoid recompute_local_max(const float* topk_dists_base, int L, int lane_id,\n                         float &local_max_val, int &local_max_pos)\n{\n    // Scan this lane's slice: [lane_id * L, lane_id * L + L)\n    int start = lane_id * L;\n    float mval = topk_dists_base[start];\n    int mpos = 0;\n#pragma unroll\n    for (int i = 1; i < 32; ++i) {\n        if (i >= L) break;\n        float v = topk_dists_base[start + i];\n        if (v > mval) { mval = v; mpos = i; }\n    }\n    local_max_val = mval;\n    local_max_pos = mpos;\n}\n\n// Device helper: warp-synchronous bitonic sort over the warp's private top-k arrays in shared memory.\n// Sorts in ascending order by distance.\n// Each warp owns a contiguous segment [warp_base, warp_base + k) in shared memory.\n// Each lane handles a contiguous slice of length L = k / 32.\n// This is a standard in-place bitonic sorting network implemented with warp-synchronous barriers.\n__device__ __forceinline__\nvoid warp_bitonic_sort_shared(float* topk_dists, int* topk_index, int warp_base, int k, int L, int lane_id)\n{\n    unsigned mask = 0xffffffffu;\n\n    // Bitonic sort on k elements. k is guaranteed to be a power of two.\n    for (int size = 2; size <= k; size <<= 1) {\n        // direction alternates per subsequence; computed by (i & size) == 0\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            // Each thread processes its L items\n#pragma unroll\n            for (int li = 0; li < 32; ++li) {\n                if (li >= L) break;\n\n                int local_idx = lane_id * L + li;             // [0, k)\n                int partner   = local_idx ^ stride;           // partner index within [0, k)\n\n                if (partner > local_idx && partner < k) {\n                    bool up = ((local_idx & size) == 0);      // true: ascending; false: descending\n\n                    int idx_a = warp_base + local_idx;\n                    int idx_b = warp_base + partner;\n\n                    float a = topk_dists[idx_a];\n                    float b = topk_dists[idx_b];\n                    int   ia = topk_index[idx_a];\n                    int   ib = topk_index[idx_b];\n\n                    // Compare-and-swap according to direction\n                    if ((a > b) == up) {\n                        topk_dists[idx_a] = b;\n                        topk_dists[idx_b] = a;\n                        topk_index[idx_a] = ib;\n                        topk_index[idx_b] = ia;\n                    }\n                }\n            }\n            // Synchronize within warp to ensure the next phase sees updated values\n            __syncwarp(mask);\n        }\n    }\n}\n\n// Main kernel: one warp per query.\n__global__ void knn_kernel_warp32(const float2* __restrict__ query, int query_count,\n                                  const float2* __restrict__ data, int data_count,\n                                  std::pair<int, float>* __restrict__ result,\n                                  int k, int tile_capacity_points)\n{\n    // Shared memory layout:\n    // [ float2 tile[tile_capacity_points] ]\n    // [ float  topk_dists[KNN_WARPS_PER_BLOCK * k] ]\n    // [   int  topk_index[KNN_WARPS_PER_BLOCK * k] ]\n    extern __shared__ unsigned char smem_raw[];\n    float2* __restrict__ tile = reinterpret_cast<float2*>(smem_raw);\n    size_t tile_bytes = static_cast<size_t>(tile_capacity_points) * sizeof(float2);\n    float* __restrict__ topk_dists = reinterpret_cast<float*>(smem_raw + tile_bytes);\n    int*   __restrict__ topk_index = reinterpret_cast<int*>(topk_dists + KNN_WARPS_PER_BLOCK * k);\n\n    const int lane_id = threadIdx.x & (WARP_SIZE - 1);\n    const int warp_id_in_block = threadIdx.x >> 5; // 0..KNN_WARPS_PER_BLOCK-1\n    const int warps_per_block = KNN_WARPS_PER_BLOCK;\n    const int global_warp_id = blockIdx.x * warps_per_block + warp_id_in_block;\n\n    if (global_warp_id >= query_count) {\n        return;\n    }\n\n    // Each warp processes one query\n    const float2 q = query[global_warp_id];\n\n    // Per-warp base pointers into shared top-k arrays\n    const int warp_base = warp_id_in_block * k;\n\n    // L = k / 32 (guaranteed integer because k is power of two and >= 32)\n    const int L = k >> 5;\n\n    // Initialize per-warp top-k arrays with +infinity / -1\n    const float INF = CUDART_INF_F;\n    for (int i = lane_id; i < k; i += WARP_SIZE) {\n        topk_dists[warp_base + i] = INF;\n        topk_index[warp_base + i] = -1;\n    }\n    __syncwarp();\n\n    // Initialize local maxima for each lane's slice\n    float local_max_val = INF;\n    int   local_max_pos = 0;\n\n    // Compute initial warp-wide threshold (worst distance among current top-k)\n    // Initially it's +INF; we set owner lane to 0 and pos 0. We'll maintain/upate incrementally.\n    float thr_val = INF;\n    int   thr_lane = 0;\n    int   thr_local_pos = 0;\n\n    // Process data in tiles\n    for (int tile_start = 0; tile_start < data_count; tile_start += tile_capacity_points) {\n        int tile_count = data_count - tile_start;\n        if (tile_count > tile_capacity_points) tile_count = tile_capacity_points;\n\n        // Load tile into shared memory cooperatively by the entire block\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            tile[i] = data[tile_start + i];\n        }\n        __syncthreads(); // Ensure tile is fully loaded before processing\n\n        // Iterate over points in the tile in steps of warp size\n        for (int base = 0; base < tile_count; base += WARP_SIZE) {\n            int idx_in_tile = base + lane_id;\n\n            // Compute squared L2 distance for this lane's candidate (if valid)\n            float cand_dist = INF;\n            int   cand_index = -1;\n            bool  valid = (idx_in_tile < tile_count);\n            if (valid) {\n                float2 p = tile[idx_in_tile];\n                float dx = p.x - q.x;\n                float dy = p.y - q.y;\n                // Fused multiply-add can help precision and performance; compiler will usually generate FMA.\n                cand_dist = dx * dx + dy * dy;\n                cand_index = tile_start + idx_in_tile;\n            }\n\n            unsigned warp_mask = 0xffffffffu;\n            unsigned valid_mask = __ballot_sync(warp_mask, valid);\n\n            // Fast path: if no valid lanes, continue\n            if (valid_mask == 0u) continue;\n\n            // Determine which lanes' candidates pass the current threshold\n            unsigned accept_mask = __ballot_sync(warp_mask, (valid && (cand_dist < thr_val)));\n\n            // Insert accepted candidates one by one in lane order, updating the threshold incrementally.\n            while (accept_mask) {\n                int leader = __ffs(accept_mask) - 1;\n\n                // Broadcast candidate from leader lane\n                float lead_dist = __shfl_sync(warp_mask, cand_dist, leader);\n                int   lead_idx  = __shfl_sync(warp_mask, cand_index, leader);\n\n                // Re-check against (possibly updated) threshold to ensure correctness\n                if (lead_dist < thr_val) {\n                    // Replace the current worst element with this candidate\n                    // Compute the global index of the worst element: warp_base + thr_lane * L + thr_local_pos\n                    int worst_global_pos = warp_base + thr_lane * L + thr_local_pos;\n\n                    // Perform the replacement by a single lane (lane 0) to avoid write conflicts\n                    if (lane_id == 0) {\n                        topk_dists[worst_global_pos] = lead_dist;\n                        topk_index[worst_global_pos] = lead_idx;\n                    }\n                    __syncwarp(warp_mask);\n\n                    // Only the lane that owns the replaced slice needs to recompute its local maximum\n                    if (lane_id == thr_lane) {\n                        recompute_local_max(topk_dists + warp_base, L, lane_id, local_max_val, local_max_pos);\n                    }\n                    __syncwarp(warp_mask);\n\n                    // Recompute the warp-wide maximum (new threshold) using the per-lane maxima.\n                    float max_val_lane0 = 0.0f;\n                    int   max_lane_lane0 = 0;\n                    warp_argmax_lane0(local_max_val, lane_id, max_val_lane0, max_lane_lane0);\n\n                    // Broadcast the threshold value and owner lane to all lanes\n                    thr_val  = __shfl_sync(warp_mask, max_val_lane0, 0);\n                    thr_lane = __shfl_sync(warp_mask, max_lane_lane0, 0);\n\n                    // Broadcast the local position of the max within the owner lane to all lanes\n                    thr_local_pos = __shfl_sync(warp_mask, local_max_pos, thr_lane);\n                }\n\n                // Remove the processed leader lane from the acceptance mask\n                accept_mask &= (accept_mask - 1u);\n\n                // Optionally tighten: Early filter with updated threshold for remaining candidates\n                // (Re-evaluate whether remaining candidates still pass after threshold update)\n                unsigned still_ok = __ballot_sync(warp_mask, (valid && (cand_dist < thr_val)));\n                accept_mask &= still_ok;\n            }\n        }\n\n        // Ensure all warps are done reading the tile before loading the next one\n        __syncthreads();\n    }\n\n    // After processing all tiles, the per-warp top-k is complete but unsorted.\n    // Sort in ascending order within the warp's segment using a warp-synchronous bitonic sort.\n    // First ensure local maxima values correspond to current top-k; not needed for sorting.\n    __syncwarp();\n\n    warp_bitonic_sort_shared(topk_dists, topk_index, warp_base, k, L, lane_id);\n\n    // Write out the sorted results to global memory in row-major order per query.\n    // Each lane writes its L entries.\n#pragma unroll\n    for (int i = 0; i < 32; ++i) {\n        if (i >= L) break;\n        int local_pos = lane_id * L + i;       // position within [0, k)\n        int global_pos = warp_base + local_pos;\n        int out_pos = global_warp_id * k + local_pos;\n\n        result[out_pos].first  = topk_index[global_pos];\n        result[out_pos].second = topk_dists[global_pos];\n    }\n}\n\n// Host interface: run_knn\n// - query: pointer to device array of float2 (size query_count)\n// - data:  pointer to device array of float2 (size data_count)\n// - result: pointer to device array of std::pair<int,float> (size query_count * k)\n// - k: power of two between 32 and 1024 inclusive\nextern \"C\"\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data,  int data_count,\n             std::pair<int, float> *result, int k)\n{\n    // Basic parameter checks (assumed valid as per problem statement, left as debug guards)\n    if (query_count <= 0 || data_count <= 0 || k < 32) return;\n\n    // Determine dynamic shared memory requirements.\n    // We allocate:\n    // - tile_capacity_points * sizeof(float2)\n    // - plus KNN_WARPS_PER_BLOCK * k * (sizeof(float) + sizeof(int)) for per-warp top-k arrays\n    // Choose an initial tile capacity; adjust down if exceeding device limits.\n    int tile_capacity_points = 4096; // initial choice; multiple of 32 for perfect coalescing\n    if (tile_capacity_points < WARP_SIZE) tile_capacity_points = WARP_SIZE;\n    // Query device's maximum opt-in shared memory per block\n    int device_id = 0;\n    cudaGetDevice(&device_id);\n    int max_smem_optin = 0;\n    cudaDeviceGetAttribute(&max_smem_optin, cudaDevAttrMaxSharedMemoryPerBlockOptin, device_id);\n\n    size_t topk_bytes = static_cast<size_t>(KNN_WARPS_PER_BLOCK) * static_cast<size_t>(k) * (sizeof(float) + sizeof(int));\n    // Ensure at least enough shared memory for top-k arrays; tile size may need to be reduced accordingly.\n    size_t smem_needed = topk_bytes + static_cast<size_t>(tile_capacity_points) * sizeof(float2);\n\n    // If needed, reduce tile size to fit the opt-in limit.\n    if (max_smem_optin > 0 && smem_needed > static_cast<size_t>(max_smem_optin)) {\n        size_t remaining = static_cast<size_t>(max_smem_optin) - topk_bytes;\n        if (static_cast<long long>(remaining) < static_cast<long long>(sizeof(float2))) {\n            // Still try to run with minimal tile size of 32; kernel will run but shared memory may be tight.\n            tile_capacity_points = WARP_SIZE;\n            smem_needed = topk_bytes + static_cast<size_t>(tile_capacity_points) * sizeof(float2);\n        } else {\n            tile_capacity_points = static_cast<int>(remaining / sizeof(float2));\n            // round down to a multiple of warp size for coalescing\n            tile_capacity_points = (tile_capacity_points / WARP_SIZE) * WARP_SIZE;\n            if (tile_capacity_points < WARP_SIZE) tile_capacity_points = WARP_SIZE;\n            smem_needed = topk_bytes + static_cast<size_t>(tile_capacity_points) * sizeof(float2);\n        }\n    }\n\n    // Set the kernel's dynamic shared memory size attribute (opt-in for >48KB per block on modern GPUs)\n    // If this fails, we proceed; the kernel launch may fail if smem exceeds allowed size.\n    cudaFuncSetAttribute(knn_kernel_warp32, cudaFuncAttributeMaxDynamicSharedMemorySize, static_cast<int>(smem_needed));\n\n    // Compute grid configuration: one warp per query\n    const int threads_per_block = KNN_THREADS_PER_BLOCK;\n    const int warps_per_block = KNN_WARPS_PER_BLOCK;\n    int total_warps = (query_count + warps_per_block - 1) / warps_per_block;\n    int blocks = total_warps;\n    if (blocks <= 0) blocks = 1;\n\n    // Launch kernel\n    knn_kernel_warp32<<<blocks, threads_per_block, smem_needed>>>(\n        query, query_count, data, data_count, result, k, tile_capacity_points\n    );\n\n    // Optionally synchronize or check for errors here as needed by the caller.\n    // cudaDeviceSynchronize();\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 706, "completion_tokens": 20353, "total_tokens": 21059, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 16384, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db78e488190855f3fae1af8d015", "custom_id": "knn04-02", "response": {"status_code": 200, "request_id": "ae061eb52e88aaa79839d1793d1016d6", "body": {"id": "chatcmpl-C4smOmkiKxjxyEgJvlMGwfg5KIwCb", "object": "chat.completion", "created": 1755279920, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <math_constants.h>\n#include <utility>\n\n// Optimized KNN for 2D points using one warp per query, shared-memory tiling for data points,\n// batched candidate insertion, and warp-parallel merge via rank-based scatter.\n\n// Notes:\n// - k is assumed to be a power of two in [32, 1024].\n// - Each warp (32 threads) handles one query.\n// - The block loads a tile of data points to shared memory; each warp scans the tile against its query.\n// - Per-warp intermediate top-k (indices + distances) is stored in shared memory.\n// - Candidate updates are batched: each lane buffers a few better-than-worst candidates,\n//   then the warp sorts and merges them with the current top-k using a parallel rank-scatter.\n// - Uses no additional device allocations.\n\nnamespace knn_detail {\n\n// Simple POD replacement with the same layout as std::pair<int, float>.\nstruct PairIF { int first; float second; };\n\nconstexpr int WARP_SIZE = 32;\nconstexpr int WARPS_PER_BLOCK = 4;             // 4 warps (128 threads) per block balances shared memory and occupancy.\nconstexpr int THREADS_PER_BLOCK = WARP_SIZE * WARPS_PER_BLOCK;\nconstexpr int TILE_POINTS = 2048;              // Number of data points per shared-memory tile (8 bytes per point -> 16KB).\nconstexpr int MAX_CAND_PER_LANE = 8;           // Buffered candidates per lane before a merge.\nconstexpr int MAX_CAND_TOTAL = WARP_SIZE * MAX_CAND_PER_LANE; // Max candidates per warp per batch (<= 256).\n\n// Warp utilities\n__device__ __forceinline__ unsigned full_mask() { return 0xffffffffu; }\n\n__device__ __forceinline__ int lane_id() { return threadIdx.x & (WARP_SIZE - 1); }\n\n__device__ __forceinline__ int warp_id_in_block() { return threadIdx.x >> 5; }\n\n// Warp-exclusive prefix sum of val (32 threads assumed active in warp)\n__device__ __forceinline__ int warp_exclusive_prefix_sum(int val, unsigned mask = 0xffffffffu) {\n    int lane = lane_id();\n    int res = val;\n    #pragma unroll\n    for (int d = 1; d < WARP_SIZE; d <<= 1) {\n        int n = __shfl_up_sync(mask, res, d);\n        if (lane >= d) res += n;\n    }\n    return res - val; // exclusive\n}\n\n// Warp-reduction sum (32 threads)\n__device__ __forceinline__ int warp_reduce_sum(int val, unsigned mask = 0xffffffffu) {\n    #pragma unroll\n    for (int d = WARP_SIZE >> 1; d > 0; d >>= 1) {\n        val += __shfl_down_sync(mask, val, d);\n    }\n    return val;\n}\n\n// Count of elements < x in sorted ascending array arr[0..n-1]\n__device__ __forceinline__ int count_less(const float* arr, int n, float x) {\n    int lo = 0, hi = n;\n    while (lo < hi) {\n        int mid = (lo + hi) >> 1;\n        float v = arr[mid];\n        if (v < x) lo = mid + 1;\n        else hi = mid;\n    }\n    return lo;\n}\n\n// Count of elements <= x in sorted ascending array arr[0..n-1]\n__device__ __forceinline__ int count_less_equal(const float* arr, int n, float x) {\n    int lo = 0, hi = n;\n    while (lo < hi) {\n        int mid = (lo + hi) >> 1;\n        float v = arr[mid];\n        if (v <= x) lo = mid + 1;\n        else hi = mid;\n    }\n    return lo;\n}\n\n// Compute next power of two >= n, for n > 0 and n <= 1024 (small bounds here)\n__device__ __forceinline__ int next_pow2(int n) {\n    n--;\n    n |= n >> 1; n |= n >> 2; n |= n >> 4;\n    n |= n >> 8; n |= n >> 16;\n    return n + 1;\n}\n\n// Bitonic sort (ascending) on shared-memory arrays (dists, idxs) of length n,\n// using warp-synchronous parallelism. Elements beyond n up to next power-of-two\n// are assumed initialized to +inf and dummy index.\n__device__ __forceinline__ void bitonic_sort_shared(float* dists, int* idxs, int n) {\n    const unsigned mask = full_mask();\n    const int L = next_pow2(n);\n    // Sort network over length L\n    for (int k = 2; k <= L; k <<= 1) {\n        for (int j = k >> 1; j > 0; j >>= 1) {\n            // Each lane processes multiple indices strided by warp\n            for (int i = lane_id(); i < L; i += WARP_SIZE) {\n                int ixj = i ^ j;\n                if (ixj > i) {\n                    bool up = ((i & k) == 0);\n                    float ai = dists[i];\n                    float aj = dists[ixj];\n                    int ii = idxs[i];\n                    int ij = idxs[ixj];\n                    // Compare in ascending order when 'up' is true, descending otherwise\n                    bool swap = (ai > aj) ^ (!up);\n                    if (swap) {\n                        dists[i] = aj; dists[ixj] = ai;\n                        idxs[i] = ij; idxs[ixj] = ii;\n                    }\n                }\n            }\n            __syncwarp(mask);\n        }\n    }\n}\n\ntemplate<int K>\n__global__ void knn_kernel(const float2* __restrict__ query,\n                           int query_count,\n                           const float2* __restrict__ data,\n                           int data_count,\n                           PairIF* __restrict__ result) {\n    // Shared memory layout:\n    // - sTile: cached data points for current tile\n    // - Per-warp arrays for top-K, candidate buffer, and merge buffer\n    __shared__ float2 sTile[TILE_POINTS];\n\n    __shared__ float sTopDist[WARPS_PER_BLOCK * K];\n    __shared__ int   sTopIdx[WARPS_PER_BLOCK * K];\n\n    __shared__ float sCandDist[WARPS_PER_BLOCK * MAX_CAND_TOTAL];\n    __shared__ int   sCandIdx[WARPS_PER_BLOCK * MAX_CAND_TOTAL];\n\n    __shared__ float sMergeDist[WARPS_PER_BLOCK * (K + MAX_CAND_TOTAL)];\n    __shared__ int   sMergeIdx[WARPS_PER_BLOCK * (K + MAX_CAND_TOTAL)];\n\n    const int lane = lane_id();\n    const int warpInBlock = warp_id_in_block();\n    const int warpGlobal = blockIdx.x * WARPS_PER_BLOCK + warpInBlock;\n    const bool warpActive = (warpGlobal < query_count);\n    const unsigned wmask = full_mask();\n\n    // Pointers to this warp's sections in shared memory\n    float* topDist = sTopDist + warpInBlock * K;\n    int*   topIdx  = sTopIdx  + warpInBlock * K;\n\n    float* candDist = sCandDist + warpInBlock * MAX_CAND_TOTAL;\n    int*   candIdx  = sCandIdx  + warpInBlock * MAX_CAND_TOTAL;\n\n    float* mergeDist = sMergeDist + warpInBlock * (K + MAX_CAND_TOTAL);\n    int*   mergeIdx  = sMergeIdx  + warpInBlock * (K + MAX_CAND_TOTAL);\n\n    float2 q = make_float2(0.f, 0.f);\n    if (warpActive) {\n        // Load this warp's query point to registers\n        if (lane == 0) {\n            q = query[warpGlobal];\n        }\n        // Broadcast within warp\n        q.x = __shfl_sync(wmask, q.x, 0);\n        q.y = __shfl_sync(wmask, q.y, 0);\n\n        // Initialize top-K arrays to +inf and -1\n        for (int i = lane; i < K; i += WARP_SIZE) {\n            topDist[i] = CUDART_INF_F;\n            topIdx[i]  = -1;\n        }\n    }\n\n    __syncthreads(); // Ensure shared memory initialized before use in tiles\n\n    // For each tile of data points\n    for (int base = 0; base < data_count; base += TILE_POINTS) {\n        const int tileCount = min(TILE_POINTS, data_count - base);\n\n        // Load tile into shared memory cooperatively by all threads in block\n        for (int i = threadIdx.x; i < tileCount; i += blockDim.x) {\n            sTile[i] = data[base + i];\n        }\n        __syncthreads();\n\n        if (warpActive) {\n            // Local buffered candidates per lane\n            float bufDist[MAX_CAND_PER_LANE];\n            int   bufIdx[MAX_CAND_PER_LANE];\n            int bufCount = 0;\n\n            // Current worst distance in top-K (ascending order -> worst at index K-1)\n            float worst = topDist[K - 1];\n\n            // Process tile: each lane handles points strided by warp size\n            for (int i = lane; i < tileCount; i += WARP_SIZE) {\n                float2 p = sTile[i];\n                float dx = p.x - q.x;\n                float dy = p.y - q.y;\n                float dist = fmaf(dx, dx, dy * dy);\n                if (dist < worst) {\n                    // Buffer this candidate\n                    bufDist[bufCount] = dist;\n                    bufIdx[bufCount] = base + i;\n                    bufCount++;\n\n                    // Flush buffered candidates if full\n                    if (bufCount == MAX_CAND_PER_LANE) {\n                        // Sort the local buffer (small n) ascending by distance (insertion sort)\n                        for (int a = 1; a < bufCount; ++a) {\n                            float vd = bufDist[a];\n                            int   vi = bufIdx[a];\n                            int b = a - 1;\n                            while (b >= 0 && bufDist[b] > vd) {\n                                bufDist[b + 1] = bufDist[b];\n                                bufIdx[b + 1] = bufIdx[b];\n                                --b;\n                            }\n                            bufDist[b + 1] = vd;\n                            bufIdx[b + 1] = vi;\n                        }\n\n                        // Warp-wide gather of candidates into shared memory\n                        int myCount = bufCount;\n                        int myOffset = warp_exclusive_prefix_sum(myCount, wmask);\n                        int total = __shfl_sync(wmask, myOffset + myCount, WARP_SIZE - 1);\n\n                        // Write my buffer into warp's candidate array\n                        for (int j = 0; j < myCount; ++j) {\n                            candDist[myOffset + j] = bufDist[j];\n                            candIdx[myOffset + j]  = bufIdx[j];\n                        }\n                        // Pad remaining up to next_pow2(total) with +inf\n                        int L = next_pow2(max(1, total));\n                        for (int j = myOffset + myCount; j < L; j += WARP_SIZE) {\n                            int idx = j - myOffset - myCount + lane; // Distribute padding across lanes\n                            int pos = myOffset + myCount + idx;\n                            if (pos < L) {\n                                candDist[pos] = CUDART_INF_F;\n                                candIdx[pos]  = -1;\n                            }\n                        }\n                        __syncwarp(wmask);\n\n                        if (total > 0) {\n                            // Bitonic sort of candidates (ascending)\n                            bitonic_sort_shared(candDist, candIdx, total);\n\n                            // Parallel rank-scatter merge of top-K (A) and candidates (C)\n                            // A: topDist/topIdx of size K, ascending\n                            // C: candDist/candIdx of size 'total', ascending\n                            // Compute ranks and scatter into mergeDist/mergeIdx of size K + total\n                            // Ranks for A elements: i + count_less(C, A[i])\n                            for (int iA = lane; iA < K; iA += WARP_SIZE) {\n                                float v = topDist[iA];\n                                int r = iA + count_less(candDist, total, v);\n                                mergeDist[r] = v;\n                                mergeIdx[r]  = topIdx[iA];\n                            }\n                            // Ranks for C elements: j + count_less_equal(A, C[j])\n                            for (int j = lane; j < total; j += WARP_SIZE) {\n                                float v = candDist[j];\n                                int r = j + count_less_equal(topDist, K, v);\n                                mergeDist[r] = v;\n                                mergeIdx[r]  = candIdx[j];\n                            }\n                            __syncwarp(wmask);\n\n                            // Write back first K elements into top-K arrays (ascending)\n                            for (int iA = lane; iA < K; iA += WARP_SIZE) {\n                                topDist[iA] = mergeDist[iA];\n                                topIdx[iA]  = mergeIdx[iA];\n                            }\n                            __syncwarp(wmask);\n\n                            // Refresh worst\n                            worst = topDist[K - 1];\n                        }\n\n                        // Reset buffer\n                        bufCount = 0;\n                    }\n                }\n            }\n\n            // Flush remaining buffered candidates (same as above)\n            if (bufCount > 0) {\n                for (int a = 1; a < bufCount; ++a) {\n                    float vd = bufDist[a];\n                    int   vi = bufIdx[a];\n                    int b = a - 1;\n                    while (b >= 0 && bufDist[b] > vd) {\n                        bufDist[b + 1] = bufDist[b];\n                        bufIdx[b + 1] = bufIdx[b];\n                        --b;\n                    }\n                    bufDist[b + 1] = vd;\n                    bufIdx[b + 1] = vi;\n                }\n\n                int myCount = bufCount;\n                int myOffset = warp_exclusive_prefix_sum(myCount, wmask);\n                int total = __shfl_sync(wmask, myOffset + myCount, WARP_SIZE - 1);\n\n                for (int j = 0; j < myCount; ++j) {\n                    candDist[myOffset + j] = bufDist[j];\n                    candIdx[myOffset + j]  = bufIdx[j];\n                }\n                int L = next_pow2(max(1, total));\n                for (int j = myOffset + myCount; j < L; j += WARP_SIZE) {\n                    int idx = j - myOffset - myCount + lane;\n                    int pos = myOffset + myCount + idx;\n                    if (pos < L) {\n                        candDist[pos] = CUDART_INF_F;\n                        candIdx[pos]  = -1;\n                    }\n                }\n                __syncwarp(wmask);\n\n                if (total > 0) {\n                    bitonic_sort_shared(candDist, candIdx, total);\n\n                    for (int iA = lane; iA < K; iA += WARP_SIZE) {\n                        float v = topDist[iA];\n                        int r = iA + count_less(candDist, total, v);\n                        mergeDist[r] = v;\n                        mergeIdx[r]  = topIdx[iA];\n                    }\n                    for (int j = lane; j < total; j += WARP_SIZE) {\n                        float v = candDist[j];\n                        int r = j + count_less_equal(topDist, K, v);\n                        mergeDist[r] = v;\n                        mergeIdx[r]  = candIdx[j];\n                    }\n                    __syncwarp(wmask);\n\n                    for (int iA = lane; iA < K; iA += WARP_SIZE) {\n                        topDist[iA] = mergeDist[iA];\n                        topIdx[iA]  = mergeIdx[iA];\n                    }\n                    __syncwarp(wmask);\n                }\n            }\n        }\n\n        __syncthreads(); // Done using current tile; proceed to load next tile\n    }\n\n    if (warpActive) {\n        // Write final top-K results for this query\n        PairIF* out = result + warpGlobal * K;\n        for (int i = lane; i < K; i += WARP_SIZE) {\n            PairIF p;\n            p.first = topIdx[i];\n            p.second = topDist[i];\n            out[i] = p;\n        }\n    }\n}\n\n} // namespace knn_detail\n\n// Host callable entry point\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data, int data_count,\n             std::pair<int, float> *result, int k) {\n    using namespace knn_detail;\n\n    if (query_count <= 0 || data_count <= 0) return;\n\n    dim3 block(THREADS_PER_BLOCK);\n    dim3 grid((query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK);\n\n    // Reinterpret result pointer to POD pair with identical layout\n    PairIF* d_out = reinterpret_cast<PairIF*>(const_cast<std::pair<int,float>*>(result));\n\n    // Dispatch based on k (power of two between 32 and 1024)\n    switch (k) {\n        case 32:\n            knn_kernel<32><<<grid, block>>>(query, query_count, data, data_count, d_out);\n            break;\n        case 64:\n            knn_kernel<64><<<grid, block>>>(query, query_count, data, data_count, d_out);\n            break;\n        case 128:\n            knn_kernel<128><<<grid, block>>>(query, query_count, data, data_count, d_out);\n            break;\n        case 256:\n            knn_kernel<256><<<grid, block>>>(query, query_count, data, data_count, d_out);\n            break;\n        case 512:\n            knn_kernel<512><<<grid, block>>>(query, query_count, data, data_count, d_out);\n            break;\n        case 1024:\n            knn_kernel<1024><<<grid, block>>>(query, query_count, data, data_count, d_out);\n            break;\n        default:\n            // Unsupported k; do nothing. Preconditions guarantee k is valid.\n            break;\n    }\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 706, "completion_tokens": 13816, "total_tokens": 14522, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 9856, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db7ec288190883a9faf8e2e20ad", "custom_id": "knn04-03", "response": {"status_code": 200, "request_id": "0a177c48121e672524444a50fa837189", "body": {"id": "chatcmpl-C4snKGjdq8EkF2N2q5te9GNFOUZlj", "object": "chat.completion", "created": 1755279978, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// This implementation assigns one warp (32 threads) to each query. The block cooperatively loads\n// tiles of the data points into shared memory, and each warp computes distances from its assigned\n// query to the cached points. Each warp keeps an intermediate top-k (k is a power of two and\n// multiple of 32; each lane owns k/32 slots). Updates to the intermediate result from 32 candidates\n// are performed cooperatively using warp shuffles. At the end, each warp writes its k results into\n// shared memory and performs an in-warp bitonic sort (ascending by distance) to produce the final\n// k-NN list per query, which is then written to the output array.\n//\n// Design highlights:\n// - One warp per query for predictable control flow and warp-level communication.\n// - Block-wide shared memory tiling of the data for cache reuse, with a tile size chosen to balance\n//   shared memory usage between the tile and the final per-warp sorting scratch buffer.\n// - Incremental top-k maintenance using a distributed replacement of the current global maximum\n//   among the k kept candidates, processing up to 32 candidates per iteration.\n// - Final ordering guaranteed by a single in-warp bitonic sort over the k intermediate results.\n// - No additional global memory allocations; only shared memory is used.\n\n#ifndef KNN_WARPS_PER_BLOCK\n#define KNN_WARPS_PER_BLOCK 8\n#endif\n\n#ifndef KNN_TILE_SIZE\n#define KNN_TILE_SIZE 4096\n#endif\n\n// Pair struct used in shared memory scratch for sorting (distance, index).\nstruct PairDF {\n    float dist;\n    int   idx;\n};\n\n// Warp-level argmax reduction for (value, lane, pos) triple. Returns the triple broadcast to all lanes.\n__device__ __forceinline__ void warp_argmax(float local_val, int local_pos, int lane_id,\n                                            float &out_val, int &out_lane, int &out_pos, unsigned mask) {\n    float val = local_val;\n    int pos   = local_pos;\n    int ln    = lane_id;\n\n    // Tree reduction by comparing values, breaking ties by lane then position to get deterministic behavior.\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        float oth_val = __shfl_down_sync(mask, val, offset);\n        int   oth_ln  = __shfl_down_sync(mask, ln,  offset);\n        int   oth_pos = __shfl_down_sync(mask, pos, offset);\n\n        bool take_other = (oth_val > val) || (oth_val == val && (oth_ln > ln || (oth_ln == ln && oth_pos > pos)));\n        if (take_other) {\n            val = oth_val;\n            ln  = oth_ln;\n            pos = oth_pos;\n        }\n    }\n    // Broadcast the result from lane 0 to all lanes in the warp.\n    out_val  = __shfl_sync(mask, val, 0);\n    out_lane = __shfl_sync(mask, ln,  0);\n    out_pos  = __shfl_sync(mask, pos, 0);\n}\n\n__global__ void knn_kernel(const float2 * __restrict__ query, int query_count,\n                           const float2 * __restrict__ data,  int data_count,\n                           std::pair<int, float> * __restrict__ result, int k)\n{\n    // Identify warp and lane.\n    const int lane_id        = threadIdx.x & 31;\n    const int warp_id_in_blk = threadIdx.x >> 5;\n    const int warps_per_blk  = blockDim.x >> 5;\n    const int warp_global_id = blockIdx.x * warps_per_blk + warp_id_in_blk;\n\n    const bool warp_active = (warp_global_id < query_count);\n    const unsigned full_mask = 0xFFFFFFFFu;\n\n    // Shared memory layout:\n    // [ 0 .. KNN_TILE_SIZE * sizeof(float2) ) => tile of data points (float2)\n    // [ KNN_TILE_SIZE * sizeof(float2) .. )   => per-warp scratch buffers for final sort (k PairDF per warp)\n    extern __shared__ unsigned char smem_raw[];\n    float2 *tile = reinterpret_cast<float2 *>(smem_raw);\n    PairDF *scratch_base = reinterpret_cast<PairDF *>(smem_raw + sizeof(float2) * KNN_TILE_SIZE);\n    PairDF *warp_scratch = scratch_base + warp_id_in_blk * k; // warp-private scratch of length k\n\n    // Each warp is responsible for one query index.\n    int q_idx = warp_global_id;\n\n    // Broadcast the query point to all lanes of the warp.\n    float qx = 0.f, qy = 0.f;\n    if (warp_active) {\n        float2 q;\n        if (lane_id == 0) {\n            q = query[q_idx];\n        }\n        unsigned mask = __activemask();\n        qx = __shfl_sync(mask, q.x, 0);\n        qy = __shfl_sync(mask, q.y, 0);\n    }\n\n    // Per-lane local storage for the warp's distributed top-k (each lane holds s = k/32 items).\n    const int s = k >> 5; // since k is a power of two between 32 and 1024, s in [1, 32]\n    float best_dist[32]; // max size; only first s used\n    int   best_idx[32];\n    int   filled_lane = 0;        // number of filled slots in this lane (for prefill phase)\n    bool  has_threshold = false;  // whether the initial K candidates have been collected\n\n    // Local cached \"max\" per lane among its s entries (valid only when has_threshold true).\n    float local_max_val = -CUDART_INF_F;\n    int   local_max_pos = 0;\n\n    // Current global threshold triple (value, lane, pos) for the worst (maximum distance) among the K kept.\n    float thr_val = CUDART_INF_F;\n    int   thr_lane = 0, thr_pos = 0;\n\n    // Process data in tiles loaded into shared memory by the whole thread block.\n    for (int tile_start = 0; tile_start < data_count; tile_start += KNN_TILE_SIZE) {\n        int tile_count = data_count - tile_start;\n        if (tile_count > KNN_TILE_SIZE) tile_count = KNN_TILE_SIZE;\n\n        // All threads in the block cooperatively load the tile into shared memory.\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            tile[i] = data[tile_start + i];\n        }\n        __syncthreads();\n\n        // Each warp processes the tile against its query.\n        if (warp_active) {\n            // Iterate over the tile in groups of 32 so that each lane handles one candidate at a time.\n            for (int j_base = 0; j_base < tile_count; j_base += 32) {\n                int j = j_base + lane_id;\n                bool valid = (j < tile_count);\n\n                float2 p = make_float2(0.f, 0.f);\n                if (valid) p = tile[j];\n\n                float dx = p.x - qx;\n                float dy = p.y - qy;\n                float dist = fmaf(dy, dy, dx * dx);  // squared L2 distance\n                int   didx = tile_start + j;\n\n                // Prefill phase: keep inserting until each lane has filled 's' items.\n                if (!has_threshold) {\n                    if (valid && (filled_lane < s)) {\n                        best_dist[filled_lane] = dist;\n                        best_idx[filled_lane]  = didx;\n                        ++filled_lane;\n                    }\n                    // After attempting to fill this round, check if the whole warp has completed prefill.\n                    unsigned mask = __activemask();\n                    int lane_full = (filled_lane >= s) ? 1 : 0;\n                    unsigned full_mask_ballot = __ballot_sync(mask, lane_full);\n                    if (full_mask_ballot == full_mask) {\n                        // Compute initial local maxima per lane.\n                        float mval = best_dist[0];\n                        int mpos = 0;\n                        #pragma unroll\n                        for (int t = 1; t < s; ++t) {\n                            float v = best_dist[t];\n                            if (v > mval) { mval = v; mpos = t; }\n                        }\n                        local_max_val = mval;\n                        local_max_pos = mpos;\n                        // Compute global maximum across the warp to set the initial threshold.\n                        warp_argmax(local_max_val, local_max_pos, lane_id, thr_val, thr_lane, thr_pos, mask);\n                        has_threshold = true;\n                    }\n                    // Continue to next 32-candidate batch.\n                    continue;\n                }\n\n                // After prefill: filter candidates against the current threshold and replace the worst if needed.\n                // Build a mask of candidates that are valid and would improve the current top-k.\n                unsigned mask = __activemask();\n                int accept = (valid && (dist < thr_val)) ? 1 : 0;\n                unsigned accept_mask = __ballot_sync(mask, accept);\n\n                // Process the accepted candidates one by one (at most 32 per batch).\n                while (accept_mask) {\n                    // Lane id of the next candidate to insert (lowest set bit).\n                    int src_lane = __ffs(accept_mask) - 1;\n\n                    // Broadcast the candidate from src_lane to all lanes.\n                    float cand_dist = __shfl_sync(mask, dist, src_lane);\n                    int   cand_idx  = __shfl_sync(mask, didx, src_lane);\n\n                    // Replace the current global worst with this candidate.\n                    if (lane_id == thr_lane) {\n                        best_dist[thr_pos] = cand_dist;\n                        best_idx[thr_pos]  = cand_idx;\n                        // Recompute this lane's local maximum.\n                        float mval = best_dist[0];\n                        int mpos = 0;\n                        #pragma unroll\n                        for (int t = 1; t < s; ++t) {\n                            float v = best_dist[t];\n                            if (v > mval) { mval = v; mpos = t; }\n                        }\n                        local_max_val = mval;\n                        local_max_pos = mpos;\n                    }\n\n                    // Recompute the global threshold after the replacement.\n                    warp_argmax(local_max_val, local_max_pos, lane_id, thr_val, thr_lane, thr_pos, mask);\n\n                    // Clear this candidate from the accept mask and continue.\n                    accept_mask &= ~(1u << src_lane);\n                }\n            } // end for j_base\n        } // end if warp_active\n\n        __syncthreads(); // ensure tile is not accessed before being overwritten by the next tile\n    } // end for tile_start\n\n    // Final sort and writeback of results.\n    if (warp_active) {\n        // Write the warp's k candidates into its reserved shared memory buffer.\n        for (int t = 0; t < s; ++t) {\n            int g = lane_id * s + t;\n            warp_scratch[g].dist = best_dist[t];\n            warp_scratch[g].idx  = best_idx[t];\n        }\n        __syncwarp();\n\n        // In-warp bitonic sort over k elements in shared memory (ascending by distance).\n        // Each thread processes a strided subset of indices to perform compare-exchange on disjoint pairs.\n        for (int size = 2; size <= k; size <<= 1) {\n            for (int stride = size >> 1; stride > 0; stride >>= 1) {\n                for (int i = lane_id; i < k; i += 32) {\n                    int j = i ^ stride;\n                    if (j > i) {\n                        bool ascending = ((i & size) == 0);\n                        PairDF a = warp_scratch[i];\n                        PairDF b = warp_scratch[j];\n                        bool do_swap = ascending ? (a.dist > b.dist) : (a.dist < b.dist);\n                        if (do_swap) {\n                            warp_scratch[i] = b;\n                            warp_scratch[j] = a;\n                        }\n                    }\n                }\n                __syncwarp();\n            }\n        }\n\n        // Write the sorted neighbors to the output array for this query.\n        // Output layout: for query q_idx, result[q_idx * k + j] = (index, distance) of the j-th nearest neighbor.\n        for (int j = lane_id; j < k; j += 32) {\n            int out_idx = q_idx * k + j;\n            PairDF p = warp_scratch[j];\n            // std::pair<int,float> is assumed trivially assignable in device code.\n            result[out_idx] = std::pair<int, float>(p.idx, p.dist);\n        }\n    }\n}\n\n// Host entry point. Launches the kernel with chosen hyperparameters and dynamic shared memory size.\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count,\n             std::pair<int, float> *result, int k)\n{\n    // Choose execution configuration.\n    const int warps_per_block = KNN_WARPS_PER_BLOCK;\n    const int threads_per_block = warps_per_block * 32;\n    const int total_warps = (query_count + 0) / 1; // one warp per query\n    const int blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Dynamic shared memory size:\n    // - Tile of float2: KNN_TILE_SIZE * sizeof(float2)\n    // - Per-warp scratch for final sort: warps_per_block * k * sizeof(PairDF)\n    size_t shmem_bytes = sizeof(float2) * KNN_TILE_SIZE + sizeof(PairDF) * (size_t)warps_per_block * (size_t)k;\n\n    // Opt-in to larger dynamic shared memory if needed (on modern GPUs this is supported).\n    cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)shmem_bytes);\n\n    // Launch the kernel.\n    dim3 grid(blocks);\n    dim3 block(threads_per_block);\n    knn_kernel<<<grid, block, shmem_bytes>>>(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 706, "completion_tokens": 22657, "total_tokens": 23363, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 19584, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db78fd081909ac4bae77a99a983", "custom_id": "knn04-04", "response": {"status_code": 200, "request_id": "303a206b3bee1bd2b55a9ca25f7e060f", "body": {"id": "chatcmpl-C4sw2q0HOY8rszCMmY2pwYUzp0gUi", "object": "chat.completion", "created": 1755280518, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// This implementation computes k-NN for 2D points using one warp (32 threads) per query.\n// Key design points:\n// - The dataset is processed in tiles loaded into shared memory by the whole block.\n// - Each warp maintains a private top-k structure in shared memory (sorted ascending by distance).\n// - Candidates are collected per warp into a shared \"insertion buffer\" in parallel.\n// - When the buffer fills (or at the end), the warp sorts the buffer in parallel (bitonic sort, warp-synchronous)\n//   and then performs a parallel merge (merge-path across 32 lanes) with the current top-k to keep the best k.\n// - All updates use warp-level synchronization (__syncwarp), and tiles are loaded using block synchronization (__syncthreads).\n//\n// Assumptions:\n// - k is a power of two, 32 <= k <= 1024.\n// - data_count >= k.\n// - Pointers query, data, result refer to device memory allocated via cudaMalloc.\n//\n// Tunables chosen for modern data center GPUs (A100/H100):\n// - Warps per block: 4 (i.e., 128 threads per block).\n// - Tile size: 2048 points per tile.\n// - Per-warp buffer size: 256 candidates.\n// These balance shared memory usage (~88KB worst-case at k=1024) and parallel efficiency.\n//\n// Note: The algorithm uses squared Euclidean distance.\n//       The output neighbors for each query are written in non-decreasing order of distance.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Tunable parameters\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 4\n#endif\n\n#ifndef TILE_POINTS\n#define TILE_POINTS 2048\n#endif\n\n#ifndef WARP_BUFFER_CAP\n#define WARP_BUFFER_CAP 256\n#endif\n\n#ifndef ROUND_BATCH\n#define ROUND_BATCH 8\n#endif\n\n// Utility: next power-of-two for positive integers\n__device__ __forceinline__ int next_pow2(int x) {\n    x--;\n    x |= x >> 1;\n    x |= x >> 2;\n    x |= x >> 4;\n    x |= x >> 8;\n    x |= x >> 16;\n    return x + 1;\n}\n\n// Utility: squared L2 distance between two float2 points\n__device__ __forceinline__ float dist2(const float2 a, const float2 b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    return dx * dx + dy * dy;\n}\n\n// Warp-synchronous exclusive prefix sum of 'val' (32 threads).\n// Returns the exclusive prefix; 'total' receives the warp-wide sum (valid in all lanes).\n__device__ __forceinline__ int warp_excl_prefix_sum(int val, int &total, unsigned mask) {\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n    int inclusive = val;\n    // Inclusive scan using shfl_up\n    for (int offset = 1; offset < WARP_SIZE; offset <<= 1) {\n        int n = __shfl_up_sync(mask, inclusive, offset);\n        if (lane >= offset) inclusive += n;\n    }\n    total = __shfl_sync(mask, inclusive, WARP_SIZE - 1);\n    int exclusive = inclusive - val;\n    return exclusive;\n}\n\n// Warp-synchronous bitonic sort (ascending) on pairs (dist, idx) stored in shared memory.\n// Npad must be a power of two; elements beyond 'N' should have been padded with +INF to handle non-power-of-two sizes.\n// The algorithm uses the standard bitonic network with XOR-swap pattern and warp-level synchronization.\n__device__ __forceinline__ void warp_bitonic_sort_pairs(float *dist, int *idx, int N, int Npad, unsigned mask) {\n    // Pad remainder [N .. Npad-1] assumed already set to +INF and idx=-1 by caller.\n    // Perform bitonic sort over Npad elements. Only elements < N carry meaningful finite values.\n    for (int k = 2; k <= Npad; k <<= 1) {\n        for (int j = k >> 1; j > 0; j >>= 1) {\n            for (int i = threadIdx.x & (WARP_SIZE - 1); i < Npad; i += WARP_SIZE) {\n                int ixj = i ^ j;\n                if (ixj > i) {\n                    float ai = dist[i];\n                    float aj = dist[ixj];\n                    int ii = idx[i];\n                    int ij = idx[ixj];\n                    // Ascending order if (i & k) == 0; descending otherwise\n                    bool ascending = ((i & k) == 0);\n                    bool do_swap = ascending ? (ai > aj) : (ai < aj);\n                    if (do_swap) {\n                        dist[i] = aj;\n                        dist[ixj] = ai;\n                        idx[i] = ij;\n                        idx[ixj] = ii;\n                    }\n                }\n            }\n            __syncwarp(mask);\n        }\n    }\n    // After sorting, the first N entries are sorted ascending; padded +INF naturally go to the end.\n    (void)N; // N is not used here beyond padding check.\n}\n\n// Merge-path search: find partition (i, j) for diagonal 'diag' in the merge of arrays A (len a_len) and B (len b_len).\n// Both A and B are sorted ascending. We find i in [max(0, diag - b_len), min(diag, a_len)] s.t.\n// (i == 0 || A[i-1] <= B[diag - i]) && (diag - i == 0 || B[diag - i - 1] < A[i])\n__device__ __forceinline__ int merge_path_search(const float *A, int a_len, const float *B, int b_len, int diag) {\n    int low = max(0, diag - b_len);\n    int high = min(diag, a_len);\n    while (low < high) {\n        int i = (low + high) >> 1;\n        int j = diag - i;\n        float a_i = (i < a_len) ? A[i] : CUDART_INF_F;\n        float b_jm1 = (j > 0) ? B[j - 1] : -CUDART_INF_F;\n        if (a_i < b_jm1) {\n            low = i + 1;\n        } else {\n            high = i;\n        }\n    }\n    return low;\n}\n\n// Warp-parallel merge (merge-path) of two sorted lists A (len a_len) and B (len b_len) into output C (len out_len).\n// out_len should be <= a_len + b_len. Each lane handles a disjoint segment of the output.\n__device__ __forceinline__ void warp_merge_path_pairs(\n    const float *A_dist, const int *A_idx, int a_len,\n    const float *B_dist, const int *B_idx, int b_len,\n    float *C_dist, int *C_idx, int out_len, unsigned mask) {\n\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n    int begin = (lane * out_len) / WARP_SIZE;\n    int end   = ((lane + 1) * out_len) / WARP_SIZE;\n    if (begin >= end) {\n        return;\n    }\n\n    // Find starting partition (i, j) for 'begin', and ending partition for 'end'\n    int i_begin = merge_path_search(A_dist, a_len, B_dist, b_len, begin);\n    int j_begin = begin - i_begin;\n    int i_end   = merge_path_search(A_dist, a_len, B_dist, b_len, end);\n    int j_end   = end - i_end;\n\n    int i = i_begin;\n    int j = j_begin;\n    int out = begin;\n\n    // Serial merge for this lane's segment\n    while (out < end) {\n        // Pick the smaller current element, breaking ties in favor of A to keep stability\n        float a_val = (i < a_len) ? A_dist[i] : CUDART_INF_F;\n        float b_val = (j < b_len) ? B_dist[j] : CUDART_INF_F;\n\n        bool take_a = (a_val <= b_val);\n        if (take_a) {\n            C_dist[out] = a_val;\n            C_idx[out]  = A_idx[i];\n            ++i;\n        } else {\n            C_dist[out] = b_val;\n            C_idx[out]  = B_idx[j];\n            ++j;\n        }\n        ++out;\n    }\n}\n\n// Flush-and-merge: For a single warp, sort its buffer (ascending) and merge with current top-k.\n// The result is stored back into top-k (sorted ascending). Buffer count is reset to zero.\n__device__ __forceinline__ void warp_flush_merge(\n    float *topk_dist, int *topk_idx,\n    float *tmp_dist,  int *tmp_idx,\n    float *buf_dist,  int *buf_idx,\n    int k, int *buf_count_ptr, unsigned mask) {\n\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n    // Load buffer count (uniform across warp)\n    int buf_n = __shfl_sync(mask, (lane == 0 ? *buf_count_ptr : 0), 0);\n    if (buf_n <= 0) {\n        return;\n    }\n\n    // Sort the buffer using warp-synchronous bitonic sort (pad to next power-of-two with +INF)\n    int n_pad = next_pow2(buf_n);\n    for (int i = lane; i < n_pad; i += WARP_SIZE) {\n        if (i >= buf_n) {\n            buf_dist[i] = CUDART_INF_F;\n            buf_idx[i]  = -1;\n        }\n    }\n    __syncwarp(mask);\n    warp_bitonic_sort_pairs(buf_dist, buf_idx, buf_n, n_pad, mask);\n    __syncwarp(mask);\n\n    // Merge the current top-k (size k, sorted ascending, potentially with +INF tail) with buffer (size buf_n, sorted)\n    // Keep only the smallest k results.\n    int out_len = k;     // We always produce exactly k outputs.\n    int a_len   = k;\n    int b_len   = buf_n;\n\n    warp_merge_path_pairs(topk_dist, topk_idx, a_len,\n                          buf_dist,  buf_idx,  b_len,\n                          tmp_dist,  tmp_idx,  out_len, mask);\n    __syncwarp(mask);\n\n    // Copy merged result back to top-k arrays\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        topk_dist[i] = tmp_dist[i];\n        topk_idx[i]  = tmp_idx[i];\n    }\n    __syncwarp(mask);\n\n    // Reset buffer count to zero\n    if (lane == 0) {\n        *buf_count_ptr = 0;\n    }\n    __syncwarp(mask);\n}\n\n// Main kernel\n__global__ void knn_kernel_2d(\n    const float2 * __restrict__ query, int query_count,\n    const float2 * __restrict__ data,  int data_count,\n    std::pair<int, float> * __restrict__ result,\n    int k) {\n\n    // Shared memory layout:\n    // [ tile (TILE_POINTS float2) ]\n    // [ For each warp in block:\n    //     topk_dist[k], tmp_dist[k], buf_dist[WARP_BUFFER_CAP],\n    //     topk_idx[k],  tmp_idx[k],  buf_idx[WARP_BUFFER_CAP],\n    //     buf_count (int)\n    // ]\n    extern __shared__ unsigned char smem[];\n    float2 *tile = reinterpret_cast<float2*>(smem);\n\n    // Compute per-warp base pointers in shared memory\n    // Compute sizes (in elements and bytes)\n    const int per_warp_floats = 2 * k + WARP_BUFFER_CAP; // topk_dist[k] + tmp_dist[k] + buf_dist[cap]\n    const int per_warp_ints   = 2 * k + WARP_BUFFER_CAP; // topk_idx[k]  + tmp_idx[k]  + buf_idx[cap]\n    const size_t per_warp_bytes =\n        per_warp_floats * sizeof(float) +\n        per_warp_ints   * sizeof(int) +\n        sizeof(int); // buf_count\n\n    unsigned char *ptr = reinterpret_cast<unsigned char*>(tile + TILE_POINTS);\n\n    int warp_in_block = threadIdx.x / WARP_SIZE;\n    int lane          = threadIdx.x & (WARP_SIZE - 1);\n    int global_warp   = blockIdx.x * WARPS_PER_BLOCK + warp_in_block;\n    if (global_warp >= query_count) return;\n\n    unsigned warp_mask = __activemask();\n\n    // Compute per-warp base pointer region\n    unsigned char *warp_base = ptr + warp_in_block * per_warp_bytes;\n\n    // Dist arrays\n    float *topk_dist = reinterpret_cast<float*>(warp_base);\n    float *tmp_dist  = topk_dist + k;\n    float *buf_dist  = tmp_dist  + k;\n\n    // Index arrays (after float arrays)\n    int   *topk_idx  = reinterpret_cast<int*>(buf_dist + WARP_BUFFER_CAP);\n    int   *tmp_idx   = topk_idx + k;\n    int   *buf_idx   = tmp_idx  + k;\n\n    // Buffer count pointer (after int arrays)\n    int   *buf_count_ptr = reinterpret_cast<int*>(buf_idx + WARP_BUFFER_CAP);\n\n    // Initialize top-k with +INF and idx = -1; initialize buffer count to 0\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        topk_dist[i] = CUDART_INF_F;\n        topk_idx[i]  = -1;\n    }\n    if (lane == 0) {\n        *buf_count_ptr = 0;\n    }\n    __syncwarp(warp_mask);\n\n    // Load query point for this warp's query into registers and broadcast\n    float2 q;\n    if (lane == 0) {\n        q = query[global_warp];\n    }\n    q.x = __shfl_sync(warp_mask, q.x, 0);\n    q.y = __shfl_sync(warp_mask, q.y, 0);\n\n    // Current threshold tau = worst (largest) distance in top-k (last element since top-k is sorted ascending).\n    float tau = CUDART_INF_F;\n\n    // Process data in tiles\n    for (int tile_start = 0; tile_start < data_count; tile_start += TILE_POINTS) {\n        int tile_size = min(TILE_POINTS, data_count - tile_start);\n\n        // Load tile into shared memory cooperatively by the whole block\n        for (int t = threadIdx.x; t < tile_size; t += blockDim.x) {\n            tile[t] = data[tile_start + t];\n        }\n        __syncthreads(); // synchronize entire block before using 'tile'\n\n        // Process tile in \"rounds\": each lane computes distances for ROUND_BATCH points per round\n        for (int base = 0; base < tile_size; base += WARP_SIZE * ROUND_BATCH) {\n            // Lane-local candidate buffers for this round (in registers)\n            float cand_d[ROUND_BATCH];\n            int   cand_i[ROUND_BATCH];\n            int   acc = 0;\n\n            // Compute up to ROUND_BATCH candidates per lane\n            #pragma unroll\n            for (int m = 0; m < ROUND_BATCH; ++m) {\n                int rel = base + lane * ROUND_BATCH + m;\n                if (rel < tile_size) {\n                    float2 p = tile[rel];\n                    float d2 = dist2(q, p);\n                    // Global index of this data point\n                    int gidx = tile_start + rel;\n                    // Check against current threshold\n                    if (d2 < tau) {\n                        cand_d[acc] = d2;\n                        cand_i[acc] = gidx;\n                        ++acc;\n                    }\n                }\n            }\n\n            // Compute warp-wide prefix and total of accepted candidates\n            int total = 0;\n            int excl  = warp_excl_prefix_sum(acc, total, warp_mask);\n\n            // If buffer would overflow, flush first (uniform decision by lane 0)\n            int need_flush = 0;\n            if (lane == 0) {\n                int cur = *buf_count_ptr;\n                if (cur + total > WARP_BUFFER_CAP) {\n                    need_flush = 1;\n                }\n            }\n            need_flush = __shfl_sync(warp_mask, need_flush, 0);\n            if (need_flush) {\n                // Flush and merge buffer into top-k, update tau\n                warp_flush_merge(topk_dist, topk_idx, tmp_dist, tmp_idx, buf_dist, buf_idx, k, buf_count_ptr, warp_mask);\n                __syncwarp(warp_mask);\n                // Update tau from last element of top-k (sorted ascending)\n                tau = topk_dist[k - 1];\n            }\n\n            // Append this round's accepted candidates into the warp buffer contiguously\n            int base_off = 0;\n            if (lane == 0) {\n                base_off = *buf_count_ptr;\n            }\n            base_off = __shfl_sync(warp_mask, base_off, 0);\n\n            // Each lane writes its accepted candidates at positions [base_off + excl ... base_off + excl + acc)\n            for (int j = 0; j < acc; ++j) {\n                int pos = base_off + excl + j;\n                // pos is guaranteed < WARP_BUFFER_CAP due to prior flush if needed\n                buf_dist[pos] = cand_d[j];\n                buf_idx[pos]  = cand_i[j];\n            }\n            __syncwarp(warp_mask);\n\n            // Lane 0 updates buffer count\n            if (lane == 0) {\n                *buf_count_ptr = base_off + total;\n            }\n            __syncwarp(warp_mask);\n        }\n\n        __syncthreads(); // ensure tile is no longer used before it may be overwritten\n    }\n\n    // Final flush for remaining candidates in the buffer\n    int rem = __shfl_sync(warp_mask, (lane == 0 ? *buf_count_ptr : 0), 0);\n    if (rem > 0) {\n        warp_flush_merge(topk_dist, topk_idx, tmp_dist, tmp_idx, buf_dist, buf_idx, k, buf_count_ptr, warp_mask);\n        __syncwarp(warp_mask);\n    }\n\n    // Write out results for this query: top-k are sorted ascending\n    const int out_base = global_warp * k;\n    for (int j = lane; j < k; j += WARP_SIZE) {\n        // result[out_base + j] = { index, distance }\n        std::pair<int, float> out_pair;\n        out_pair.first  = topk_idx[j];\n        out_pair.second = topk_dist[j];\n        result[out_base + j] = out_pair;\n    }\n}\n\n// Host interface\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data,  int data_count,\n             std::pair<int, float> *result, int k) {\n    // Choose launch configuration\n    dim3 block(WARPS_PER_BLOCK * WARP_SIZE, 1, 1);\n    dim3 grid((query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK, 1, 1);\n\n    // Compute dynamic shared memory size\n    const size_t tile_bytes = static_cast<size_t>(TILE_POINTS) * sizeof(float2);\n    const size_t per_warp_floats = static_cast<size_t>(2 * k + WARP_BUFFER_CAP); // float arrays\n    const size_t per_warp_ints   = static_cast<size_t>(2 * k + WARP_BUFFER_CAP); // int arrays\n    const size_t per_warp_bytes  = per_warp_floats * sizeof(float) +\n                                   per_warp_ints   * sizeof(int) +\n                                   sizeof(int); // buffer count\n    const size_t smem_bytes = tile_bytes + static_cast<size_t>(WARPS_PER_BLOCK) * per_warp_bytes;\n\n    // Set kernel attribute to allow large dynamic shared memory if needed (CC >= 7.0)\n    cudaFuncSetAttribute(knn_kernel_2d,\n                         cudaFuncAttributeMaxDynamicSharedMemorySize,\n                         static_cast<int>(smem_bytes));\n\n    // Launch kernel\n    knn_kernel_2d<<<grid, block, smem_bytes>>>(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 706, "completion_tokens": 21002, "total_tokens": 21708, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 16576, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db79e7081909412312bff2a1c91", "custom_id": "knn04-05", "response": {"status_code": 200, "request_id": "e91d2a850e498114f5fea8e30b6289b7", "body": {"id": "chatcmpl-C4sonvH2Kzv7L9CrzExKVwuQF599b", "object": "chat.completion", "created": 1755280069, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cmath>\n#include <cstdint>\n#include <utility>\n\n// This implementation assigns one warp (32 threads) per query.\n// The block cooperatively loads batches (\"tiles\") of data points into shared memory.\n// Each warp computes squared L2 distances from its query to the points in the tile.\n// For each group of 32 candidates (one per lane), the warp performs a batched insertion\n// into its intermediate top-k structure, replacing the current global worsts as needed.\n// Each query's intermediate top-k is held privately by its warp, distributed across threads:\n// each lane stores k/32 candidates in registers. This avoids extra global or shared memory.\n// After processing all data, each warp merges its distributed per-lane lists to produce the\n// final k nearest neighbors, written in ascending distance order.\n//\n// Notes on parameters and assumptions:\n// - k is a power of two between 32 and 1024 (inclusive). Thus k % 32 == 0.\n// - data_count >= k.\n// - Inputs and outputs are allocated in device memory by cudaMalloc.\n// - result is std::pair<int, float>*, but the kernel writes via a POD with the same layout.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\nstruct PairIF {\n    int   first;\n    float second;\n};\n\nstatic inline __device__ unsigned lane_id() {\n    return threadIdx.x & (WARP_SIZE - 1);\n}\n\n// Pair for warp-level reductions carrying value and lane id.\nstruct PairValLane {\n    float val;\n    int   lane;\n};\n\n// Warp-wide min reduction on (val, lane). Tie-breaks by smaller lane id.\nstatic inline __device__ PairValLane warp_min_pair(PairValLane a, unsigned mask) {\n    // Tree reduction: offsets 16,8,4,2,1 for 32-lane warp\n    #pragma unroll\n    for (int offset = WARP_SIZE >> 1; offset > 0; offset >>= 1) {\n        float bval = __shfl_down_sync(mask, a.val, offset);\n        int   blane = __shfl_down_sync(mask, a.lane, offset);\n        // Choose minimum value; on tie, smaller lane id.\n        if (bval < a.val || (bval == a.val && blane < a.lane)) {\n            a.val  = bval;\n            a.lane = blane;\n        }\n    }\n    return a;\n}\n\n// Warp-wide max reduction on (val, lane). Tie-breaks by smaller lane id (arbitrary but deterministic).\nstatic inline __device__ PairValLane warp_max_pair(PairValLane a, unsigned mask) {\n    #pragma unroll\n    for (int offset = WARP_SIZE >> 1; offset > 0; offset >>= 1) {\n        float bval = __shfl_down_sync(mask, a.val, offset);\n        int   blane = __shfl_down_sync(mask, a.lane, offset);\n        if (bval > a.val || (bval == a.val && blane < a.lane)) {\n            a.val  = bval;\n            a.lane = blane;\n        }\n    }\n    return a;\n}\n\n// Recompute the local worst (maximum distance) and its position for a thread's local list.\nstatic inline __device__ void recompute_local_worst(const float* __restrict__ dists, int S, float &worst, int &pos) {\n    float w = dists[0];\n    int p = 0;\n    #pragma unroll\n    for (int i = 1; i < 32; ++i) {\n        if (i >= S) break;\n        float v = dists[i];\n        if (v > w) {\n            w = v;\n            p = i;\n        }\n    }\n    worst = w;\n    pos = p;\n}\n\n// Batched insertion of up to 32 candidates (one per lane) into the warp's distributed top-k.\n// Each iteration:\n//  - Find the best (minimum distance) pending candidate across 32 lanes.\n//  - Find the current global worst (maximum distance) across the warp's local lists.\n//  - If the best candidate improves over the global worst, replace the owner lane's worst slot.\n//  - Mark the inserted candidate as consumed and repeat, up to 32 times or until no improvement.\nstatic inline __device__ void batch_insert_32(float candDist, int candIdx,\n                                              float* __restrict__ localDists,\n                                              int*   __restrict__ localIdx,\n                                              int S,\n                                              float &localWorst, int &localWorstPos,\n                                              unsigned mask) {\n    unsigned lid = lane_id();\n    float myCandDist = candDist;\n    int   myCandIdx  = candIdx;\n\n    #pragma unroll\n    for (int it = 0; it < WARP_SIZE; ++it) {\n        PairValLane bestCand { myCandDist, (int)lid };\n        bestCand = warp_min_pair(bestCand, mask);\n\n        PairValLane worstNow { localWorst, (int)lid };\n        worstNow = warp_max_pair(worstNow, mask);\n\n        float minCand = bestCand.val;\n        float maxWorst = worstNow.val;\n\n        // If even the best pending candidate cannot improve over the current global worst,\n        // then none of the current 32 candidates can.\n        if (!(minCand < maxWorst)) {\n            break;\n        }\n\n        // Broadcast selected candidate (distance and index) and target position\n        float insDist = __shfl_sync(mask, myCandDist, bestCand.lane);\n        int   insIdx  = __shfl_sync(mask, myCandIdx,  bestCand.lane);\n        int   tgtPos  = __shfl_sync(mask, localWorstPos, worstNow.lane);\n\n        // The owner of the current global worst replaces its worst slot.\n        if ((int)lid == worstNow.lane) {\n            localDists[tgtPos] = insDist;\n            localIdx[tgtPos]   = insIdx;\n            // Recompute local worst for next iterations.\n            recompute_local_worst(localDists, S, localWorst, localWorstPos);\n        }\n        // Mark candidate as consumed at its producing lane.\n        if ((int)lid == bestCand.lane) {\n            myCandDist = CUDART_INF_F;\n            myCandIdx  = -1;\n        }\n    }\n}\n\n__global__ void knn_kernel_warp_per_query(const float2* __restrict__ query,\n                                          int query_count,\n                                          const float2* __restrict__ data,\n                                          int data_count,\n                                          PairIF* __restrict__ result,\n                                          int k,\n                                          int tile_size) {\n    extern __shared__ float2 shPoints[]; // tile of data points: tile_size float2's\n\n    // Thread and warp identifiers\n    const int lid  = lane_id();\n    const int warp_in_block = threadIdx.x >> 5;  // / 32\n    const int warps_per_block = blockDim.x >> 5;\n    const int q = blockIdx.x * warps_per_block + warp_in_block; // one query per warp\n    const bool valid_warp = (q < query_count);\n    const unsigned full_mask = 0xFFFFFFFFu;\n\n    // Load the query point once per warp and broadcast\n    float2 qpt = make_float2(0.f, 0.f);\n    if (valid_warp && lid == 0) {\n        qpt = query[q];\n    }\n    qpt.x = __shfl_sync(full_mask, qpt.x, 0);\n    qpt.y = __shfl_sync(full_mask, qpt.y, 0);\n\n    // Each lane will hold S = k/32 candidates (indices and distances) in registers.\n    // k is a power of two in [32, 1024], so S in [1, 32].\n    const int S = k >> 5;\n    // Reserve the maximum needed (32) and use only the first S entries.\n    float best_dists[32];\n    int   best_idx[32];\n\n    // Initialize per-lane list with +inf distances.\n    #pragma unroll\n    for (int i = 0; i < 32; ++i) {\n        if (i < S) {\n            best_dists[i] = CUDART_INF_F;\n            best_idx[i]   = -1;\n        }\n    }\n    // Track the local worst (max distance) and its position.\n    float localWorst;\n    int   localWorstPos;\n    recompute_local_worst(best_dists, S, localWorst, localWorstPos);\n\n    // Iterate over data in tiles cooperatively loaded into shared memory.\n    for (int base = 0; base < data_count; base += tile_size) {\n        const int tile_count = min(tile_size, data_count - base);\n\n        // Cooperative load into shared memory\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            shPoints[i] = data[base + i];\n        }\n        __syncthreads();\n\n        // Each warp processes the tile against its query.\n        if (valid_warp) {\n            // Process the tile in groups of 32 candidates: one per lane.\n            for (int t = 0; t < tile_count; t += WARP_SIZE) {\n                const int it = t + lid;\n                float candDist = CUDART_INF_F;\n                int   candIdx  = -1;\n\n                if (it < tile_count) {\n                    float2 p = shPoints[it];\n                    float dx = qpt.x - p.x;\n                    float dy = qpt.y - p.y;\n                    // Squared Euclidean distance\n                    float d = dx * dx + dy * dy;\n                    candDist = d;\n                    candIdx  = base + it;\n                }\n\n                // Batched insertion of up to 32 candidates.\n                batch_insert_32(candDist, candIdx,\n                                best_dists, best_idx, S,\n                                localWorst, localWorstPos,\n                                full_mask);\n            }\n        }\n\n        __syncthreads(); // Ensure tile is no longer used before loading the next one\n    }\n\n    // Finalize: for each warp (query), sort and write the k nearest neighbors in ascending order.\n    if (valid_warp) {\n        // Sort each lane's local list ascending (S <= 32) via insertion sort.\n        #pragma unroll\n        for (int i = 1; i < 32; ++i) {\n            if (i >= S) break;\n            float keyd = best_dists[i];\n            int   keyi = best_idx[i];\n            int j = i - 1;\n            while (j >= 0 && best_dists[j] > keyd) {\n                best_dists[j + 1] = best_dists[j];\n                best_idx[j + 1]   = best_idx[j];\n                --j;\n            }\n            best_dists[j + 1] = keyd;\n            best_idx[j + 1]   = keyi;\n        }\n\n        // Multiway merge across 32 sorted sublists (one per lane) to produce k sorted results.\n        int ptr = 0; // pointer into this lane's local sorted list\n        float headDist = (ptr < S) ? best_dists[ptr] : CUDART_INF_F;\n        int   headIdx  = (ptr < S) ? best_idx[ptr]   : -1;\n\n        for (int out = 0; out < k; ++out) {\n            // Find the minimum head across lanes\n            PairValLane cand { headDist, (int)lid };\n            PairValLane mn = warp_min_pair(cand, full_mask);\n\n            // Broadcast the selected head (distance and index) from the winning lane\n            float selDist = __shfl_sync(full_mask, headDist, mn.lane);\n            int   selIdx  = __shfl_sync(full_mask, headIdx,  mn.lane);\n\n            // Write output for this query at position 'out'. Use a single lane to perform the store.\n            if (lid == 0) {\n                const int out_offset = q * k + out;\n                result[out_offset].first  = selIdx;\n                result[out_offset].second = selDist;\n            }\n\n            // Advance the pointer in the winning lane\n            if ((int)lid == mn.lane) {\n                ++ptr;\n                headDist = (ptr < S) ? best_dists[ptr] : CUDART_INF_F;\n                headIdx  = (ptr < S) ? best_idx[ptr]   : -1;\n            }\n        }\n    }\n}\n\n// Host-side function: determines launch configuration and shared memory tile size,\n// then launches the kernel.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data, int data_count,\n             std::pair<int, float> *result, int k) {\n    // Configuration: 8 warps per block (256 threads) is a good balance for A100/H100.\n    const int warps_per_block = 8;\n    const int threads_per_block = warps_per_block * WARP_SIZE;\n\n    // Decide shared memory tile size (number of float2 points) based on device capability.\n    // Target a large tile for good cache reuse; adjust to hardware limits.\n    int device = 0;\n    cudaGetDevice(&device);\n\n    int max_smem_default = 0;\n    int max_smem_optin   = 0;\n    cudaDeviceGetAttribute(&max_smem_default, cudaDevAttrMaxSharedMemoryPerBlock, device);\n    cudaDeviceGetAttribute(&max_smem_optin,   cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n\n    // Target tile: 8192 points -> 64KB shared memory (8192 * 8 bytes).\n    int tile_target = 8192;\n    // Cap by data_count and at least 32 (one warp-wide batch).\n    tile_target = (data_count < tile_target) ? data_count : tile_target;\n    if (tile_target < WARP_SIZE) tile_target = WARP_SIZE;\n\n    // Maximum dynamic shared memory we can request for this kernel.\n    int max_smem_available = (max_smem_optin > 0) ? max_smem_optin : max_smem_default;\n    size_t bytes_wanted = static_cast<size_t>(tile_target) * sizeof(float2);\n    size_t bytes_cap    = static_cast<size_t>(max_smem_available);\n\n    // If we want more than default, opt-in to larger dynamic shared memory.\n    if ((int)bytes_wanted > max_smem_default && max_smem_optin > 0) {\n        // Request the exact amount we plan to use; driver will clamp to maximum if needed.\n        int bytes_request = (int)min(bytes_wanted, bytes_cap);\n        cudaFuncSetAttribute(knn_kernel_warp_per_query,\n                             cudaFuncAttributeMaxDynamicSharedMemorySize,\n                             bytes_request);\n    }\n\n    // Final tile size limited by what we can get.\n    size_t bytes_final_cap = (size_t)((max_smem_optin > 0) ? max_smem_optin : max_smem_default);\n    size_t bytes_final = bytes_wanted <= bytes_final_cap ? bytes_wanted : bytes_final_cap;\n    int tile_size = static_cast<int>(bytes_final / sizeof(float2));\n    // Ensure tile_size is a positive multiple of 32 for coalesced warp processing.\n    tile_size = (tile_size / WARP_SIZE) * WARP_SIZE;\n    if (tile_size < WARP_SIZE) tile_size = WARP_SIZE;\n    if (tile_size > data_count) tile_size = (data_count / WARP_SIZE) * WARP_SIZE + ((data_count % WARP_SIZE) ? WARP_SIZE : 0);\n    if (tile_size > data_count) tile_size = data_count; // clamp if overshot on rounding\n\n    // Grid dimensions: one warp per query.\n    int warp_count = (query_count + warps_per_block - 1) / warps_per_block;\n    dim3 grid(warp_count);\n    dim3 block(threads_per_block);\n\n    // Reinterpret result as POD to write from device easily.\n    PairIF* result_if = reinterpret_cast<PairIF*>(result);\n\n    // Launch kernel with dynamic shared memory equal to tile_size * sizeof(float2).\n    size_t smem_bytes = static_cast<size_t>(tile_size) * sizeof(float2);\n    knn_kernel_warp_per_query<<<grid, block, smem_bytes>>>(query, query_count, data, data_count, result_if, k, tile_size);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 706, "completion_tokens": 18395, "total_tokens": 19101, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 14848, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db7b0d081908aa830f2a50ab40a", "custom_id": "knn04-06", "response": {"status_code": 200, "request_id": "f1682897f80402e927b69ec8521f9c8b", "body": {"id": "chatcmpl-C4spfj5dmv17iCVWNVEwqXYhhmHcH", "object": "chat.completion", "created": 1755280123, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cuda.h>\n#include <utility>\n\n// Optimized CUDA k-NN (squared L2) for 2D points.\n// - One warp (32 threads) processes one query.\n// - Data points are processed in shared-memory tiles loaded by the whole block.\n// - Each warp keeps a distributed top-k structure across lanes (k/32 slots per lane),\n//   maintained in descending order (slot 0 = worst/largest distance).\n// - For each tile, each warp evaluates candidates in micro-batches of 32 points.\n//   The warp cooperatively inserts multiple qualifying candidates into its distributed top-k\n//   using warp-wide reductions (argmin/argmax) and shuffles, replacing the current global worst\n//   until no remaining candidate can improve the set.\n// - At the end, the warp emits the k neighbors in ascending order by repeatedly selecting\n//   the current best among lanes using warp-wide argmin.\n\n// Tunable parameters selected for modern data center GPUs (A100/H100).\n#ifndef KNN_WARP_SIZE\n#define KNN_WARP_SIZE 32\n#endif\n\n#ifndef KNN_BLOCK_THREADS\n#define KNN_BLOCK_THREADS 256  // 8 warps per block\n#endif\n\n#ifndef KNN_TILE_POINTS\n#define KNN_TILE_POINTS 4096   // 4096 float2 = 32KB shared memory; fits well on A100/H100\n#endif\n\n// Full warp mask constant for shuffles\n#ifndef FULL_WARP_MASK\n#define FULL_WARP_MASK 0xFFFFFFFFu\n#endif\n\n// Device utility: get lane id (0..31)\n__device__ __forceinline__ int lane_id() {\n    int lid;\n#if __CUDA_ARCH__ >= 300\n    asm volatile (\"mov.s32 %0, %laneid;\" : \"=r\"(lid));\n#else\n    lid = threadIdx.x & (KNN_WARP_SIZE - 1);\n#endif\n    return lid;\n}\n\n// Device utility: argmax across warp (value, lane_id).\n// Ties are resolved in favor of smaller lane_id.\nstruct WarpArgMax {\n    float val;\n    int lane;\n};\n\n__device__ __forceinline__ WarpArgMax warp_argmax(float v) {\n    WarpArgMax p{v, lane_id()};\n    // Always use full-warp mask, set v = -INF for inactive lanes (if any) beforehand.\n    #pragma unroll\n    for (int offset = KNN_WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        float v2 = __shfl_down_sync(FULL_WARP_MASK, p.val, offset);\n        int   l2 = __shfl_down_sync(FULL_WARP_MASK, p.lane, offset);\n        if (v2 > p.val || (v2 == p.val && l2 < p.lane)) {\n            p.val = v2;\n            p.lane = l2;\n        }\n    }\n    // Broadcast winner to all lanes\n    float max_val = __shfl_sync(FULL_WARP_MASK, p.val, 0);\n    int   max_lane = __shfl_sync(FULL_WARP_MASK, p.lane, 0);\n    return WarpArgMax{max_val, max_lane};\n}\n\n// Device utility: argmin across warp (value, lane_id).\n// Ties are resolved in favor of smaller lane_id.\nstruct WarpArgMin {\n    float val;\n    int lane;\n};\n\n__device__ __forceinline__ WarpArgMin warp_argmin(float v) {\n    WarpArgMin p{v, lane_id()};\n    // Always use full-warp mask, set v = +INF for inactive lanes (if any) beforehand.\n    #pragma unroll\n    for (int offset = KNN_WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        float v2 = __shfl_down_sync(FULL_WARP_MASK, p.val, offset);\n        int   l2 = __shfl_down_sync(FULL_WARP_MASK, p.lane, offset);\n        if (v2 < p.val || (v2 == p.val && l2 < p.lane)) {\n            p.val = v2;\n            p.lane = l2;\n        }\n    }\n    // Broadcast winner to all lanes\n    float min_val = __shfl_sync(FULL_WARP_MASK, p.val, 0);\n    int   min_lane = __shfl_sync(FULL_WARP_MASK, p.lane, 0);\n    return WarpArgMin{min_val, min_lane};\n}\n\n// Replace the current worst (slot 0) in a descending-ordered array of length S\n// with a new (better/smaller) value and sift it down to restore descending order.\n// Descending means: d[0] >= d[1] >= ... >= d[S-1]\n// After insertion (new value is smaller), it bubbles right until order holds.\n__device__ __forceinline__ void replace_worst_and_sift(float d[], int idx[], int S, float new_d, int new_i) {\n    d[0] = new_d;\n    idx[0] = new_i;\n    // Bubble down until d[j] >= d[j+1] holds again\n    #pragma unroll\n    for (int j = 0; j < KNN_WARP_SIZE; ++j) { // upper bound unrolled; loop exits early when S reached\n        if (j + 1 >= S) break;\n        if (d[j] < d[j + 1]) {\n            float td = d[j + 1];\n            d[j + 1] = d[j];\n            d[j] = td;\n            int ti = idx[j + 1];\n            idx[j + 1] = idx[j];\n            idx[j] = ti;\n        } else {\n            break;\n        }\n    }\n}\n\n// Kernel implementing k-NN for 2D points, squared L2 distances.\n// Each warp processes one query; block-level shared memory caches tiles of data points.\n__global__ void knn_kernel(const float2* __restrict__ query,\n                           int query_count,\n                           const float2* __restrict__ data,\n                           int data_count,\n                           std::pair<int, float>* __restrict__ result,\n                           int k)\n{\n    extern __shared__ float2 sh_data[]; // Shared memory tile for data points\n\n    const int lane = lane_id();\n    const int warp_in_block = threadIdx.x >> 5; // threadIdx.x / 32\n    const int warps_per_block = blockDim.x >> 5;\n    const int warp_global = blockIdx.x * warps_per_block + warp_in_block;\n\n    const bool warp_active = (warp_global < query_count);\n\n    // Per-warp query coordinates (broadcast from lane 0).\n    float qx = 0.0f, qy = 0.0f;\n    if (warp_active && lane == 0) {\n        float2 q = query[warp_global];\n        qx = q.x;\n        qy = q.y;\n    }\n    qx = __shfl_sync(FULL_WARP_MASK, qx, 0);\n    qy = __shfl_sync(FULL_WARP_MASK, qy, 0);\n\n    // Per-lane top-k storage (distributed across the warp).\n    // S = k / 32; since k is guaranteed to be power of two between 32 and 1024,\n    // S is an integer in [1, 32].\n    const int S = k >> 5; // k / 32\n    float best_d[1024 / KNN_WARP_SIZE]; // up to 32\n    int   best_i[1024 / KNN_WARP_SIZE];\n\n    // Initialize per-lane arrays as descending with +INF (worst) to the left.\n    // After first replacements, values bubble toward the right (smaller indices carry larger values).\n    #pragma unroll\n    for (int t = 0; t < 1024 / KNN_WARP_SIZE; ++t) {\n        if (t < S) {\n            best_d[t] = CUDART_INF_F;\n            best_i[t] = -1;\n        }\n    }\n\n    // Iterate through data in tiles loaded into shared memory by the entire block.\n    for (int tile_base = 0; tile_base < data_count; tile_base += KNN_TILE_POINTS) {\n        const int tile_count = min(KNN_TILE_POINTS, data_count - tile_base);\n\n        // Block-wide cooperative load into shared memory\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            sh_data[i] = data[tile_base + i];\n        }\n\n        __syncthreads(); // Ensure tile loaded\n\n        if (warp_active) {\n            // Process the tile in micro-batches of 32 candidates (one per lane).\n            for (int base = 0; base < tile_count; base += KNN_WARP_SIZE) {\n                const int idx_in_tile = base + lane;\n                const bool has_cand = (idx_in_tile < tile_count);\n\n                // Compute candidate distance for this lane (or set to +INF if no candidate in this micro-batch)\n                float cand_dist = CUDART_INF_F;\n                int   cand_idx  = -1;\n                if (has_cand) {\n                    float2 p = sh_data[idx_in_tile];\n                    float dx = p.x - qx;\n                    float dy = p.y - qy;\n                    // Squared L2 distance using FMA\n                    cand_dist = fmaf(dx, dx, dy * dy);\n                    cand_idx = tile_base + idx_in_tile;\n                }\n\n                // Cooperative insertion: attempt to insert as many candidates (<=32) into the distributed top-k.\n                // At each iteration:\n                // - Find the current global worst among all lanes' worsts (best_d[0])\n                // - Among the 32 candidates, find the best (minimum) whose distance is smaller than the global worst\n                // - Replace the global worst with that candidate (in its owner lane), then mark the candidate consumed\n                // Repeat until no candidates improve the set.\n                #pragma unroll\n                for (int iter = 0; iter < KNN_WARP_SIZE; ++iter) {\n                    // Find current global worst in the distributed top-k across lanes (each lane contributes best_d[0])\n                    float my_worst = best_d[0];\n                    // All lanes participate; my_worst is meaningful for all lanes\n                    WarpArgMax worst = warp_argmax(my_worst);\n                    float worst_val = worst.val;\n\n                    // Compute the best candidate among lanes that can improve (cand_dist < worst_val)\n                    float cand_for_min = (cand_dist < worst_val) ? cand_dist : CUDART_INF_F;\n                    WarpArgMin best_cand = warp_argmin(cand_for_min);\n\n                    // If no candidate improves (min >= worst), we are done with this micro-batch\n                    if (!(best_cand.val < worst_val)) {\n                        break;\n                    }\n\n                    // Broadcast the winning candidate's index to all lanes\n                    int winning_cand_idx = __shfl_sync(FULL_WARP_MASK, cand_idx, best_cand.lane);\n\n                    // Lane owning the global worst performs the replacement and sifts locally\n                    if (lane == worst.lane) {\n                        replace_worst_and_sift(best_d, best_i, S, best_cand.val, winning_cand_idx);\n                    }\n\n                    // Mark the winning candidate as consumed so it won't be selected again\n                    if (lane == best_cand.lane) {\n                        cand_dist = CUDART_INF_F;\n                        // cand_idx can remain unchanged; distance sentinel prevents re-selection\n                    }\n\n                    // Implicit warp-synchronous progress; no explicit sync needed within warp\n                }\n            }\n        }\n\n        __syncthreads(); // Ensure all warps finished using this tile before loading the next one\n    }\n\n    // Emit per-query results in ascending order: result[q * k + j] is the j-th nearest neighbor.\n    if (warp_active) {\n        // For each lane, we will pop from the right (smallest) of its descending array.\n        int ptr = S - 1; // points to current smallest in this lane (best)\n        float head_dist = (ptr >= 0) ? best_d[ptr] : CUDART_INF_F;\n        int   head_idx  = (ptr >= 0) ? best_i[ptr] : -1;\n\n        // Base offset in the global result array for this query\n        const int out_base = warp_global * k;\n\n        // Repeatedly select the current best among lanes and write to output\n        for (int j = 0; j < k; ++j) {\n            // Find the global minimum among head_dist of all lanes\n            WarpArgMin m = warp_argmin(head_dist);\n\n            // Broadcast the winning index and distance\n            int   w_idx = __shfl_sync(FULL_WARP_MASK, head_idx, m.lane);\n            float w_dst = m.val;\n\n            // Write only from lane 0 (or any one lane); to avoid inter-thread race, let the owner of \"minimum\" write.\n            if (lane == m.lane) {\n                // Advance this lane's pointer for next round\n                --ptr;\n                head_dist = (ptr >= 0) ? best_d[ptr] : CUDART_INF_F;\n                head_idx  = (ptr >= 0) ? best_i[ptr] : -1;\n            }\n\n            // Broadcast final values to a single chosen lane to perform the store; use lane 0 for uniformity\n            int   out_idx = __shfl_sync(FULL_WARP_MASK, w_idx, m.lane);\n            float out_dst = __shfl_sync(FULL_WARP_MASK, w_dst, m.lane);\n\n            if (lane == 0) {\n                result[out_base + j].first  = out_idx;\n                result[out_base + j].second = out_dst;\n            }\n        }\n    }\n}\n\n// Host function: launch the kernel.\n// Assumes:\n// - query, data, and result are device pointers allocated by cudaMalloc.\n// - data_count >= k, and k is a power of two between 32 and 1024 inclusive.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data, int data_count,\n             std::pair<int, float> *result, int k)\n{\n    // Derived launch configuration\n    const dim3 block(KNN_BLOCK_THREADS);\n    const int warps_per_block = KNN_BLOCK_THREADS / KNN_WARP_SIZE;\n    const int num_blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Shared memory size for one tile\n    const size_t shmem_size = static_cast<size_t>(KNN_TILE_POINTS) * sizeof(float2);\n\n    // Launch kernel\n    knn_kernel<<<num_blocks, block, shmem_size>>>(query, query_count, data, data_count, result, k);\n\n    // The caller is responsible for error checking and synchronization if desired.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 706, "completion_tokens": 16133, "total_tokens": 16839, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 12928, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db827c48190abab09615957b0c3", "custom_id": "knn04-07", "response": {"status_code": 200, "request_id": "5d81ff52a2963a12dff0be5011140ef8", "body": {"id": "chatcmpl-C4sqHsCMvDbGNJ7t0jLig7ckh6tpG", "object": "chat.completion", "created": 1755280161, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#include <utility>\n\n// Optimized k-NN for 2D points using one warp (32 threads) per query.\n// - Loads data points in shared memory in tiles cooperatively by the block.\n// - Each warp computes distances from its query to all cached points.\n// - Per warp, maintains a private top-k buffer in shared memory (double-buffered).\n// - Updates the intermediate top-k with 32 candidates at a time via a warp-cooperative\n//   merge using the merge-path algorithm.\n// - Uses warp-level shuffles to sort the 32 candidates and to access them during merging.\n\nstatic inline __device__ int lane_id() {\n    return threadIdx.x & 31;\n}\n\n// Warp-wide bitonic sort of 32 (dist, idx) pairs in ascending order by dist.\n// Each thread contributes one element; after sort, lane i holds the i-th smallest element.\n// This uses the classic XOR-shuffle bitonic network with branchless min/max selection.\nstatic inline __device__ void warp_bitonic_sort32_asc(float &dist, int &idx, unsigned mask) {\n    int lane = lane_id();\n    // k is the size of the sequence being merged/sorted at this stage\n    for (unsigned k = 2; k <= 32; k <<= 1) {\n        // j is the distance to the partner in the compare-exchange within stage k\n        for (unsigned j = k >> 1; j > 0; j >>= 1) {\n            float oth_dist = __shfl_xor_sync(mask, dist, j);\n            int   oth_idx  = __shfl_xor_sync(mask, idx,  j);\n\n            bool up = ((lane & k) == 0); // upper/lower half determines direction\n            bool comp = (dist > oth_dist);\n            // Select min/max without branches\n            float min_d = comp ? oth_dist : dist;\n            float max_d = comp ? dist : oth_dist;\n            int   min_i = comp ? oth_idx  : idx;\n            int   max_i = comp ? idx      : oth_idx;\n\n            dist = up ? min_d : max_d;\n            idx  = up ? min_i : max_i;\n        }\n    }\n}\n\n// Binary-search the merge path partition for diagonal D when merging A (length m)\n// and B (length n). A is in memory (ascending). B elements are held one per lane\n// in registers and accessed via shuffles from b_dist_local.\n// Returns the number of elements to take from A (denoted 'a'), so that b = D - a.\n// Reference: \"GPU Merge Path\" (Green, McColl, Bader).\nstatic inline __device__ int mergepath_search_A_for_diag(\n    int D, const float* A, int m, int n, float b_dist_local, unsigned mask)\n{\n    int low  = max(0, D - n);\n    int high = min(D, m);\n    // We search for the smallest 'a' such that A[a-1] <= B[D-a]\n    while (low < high) {\n        int a = (low + high) >> 1;\n        int b = D - a;\n\n        float A_im1 = (a > 0) ? A[a - 1] : -CUDART_INF_F;\n        float B_j   = (b < n) ? __shfl_sync(mask, b_dist_local, b) : CUDART_INF_F;\n\n        if (A_im1 > B_j) {\n            high = a;\n        } else {\n            low = a + 1;\n        }\n    }\n    return low;\n}\n\n// Merge the current warp-private top-k (arrays curDist/curIdx, length k, ascending)\n// with 32 new sorted candidates B (one per lane, sorted ascending via bitonic) into\n// the next buffer (nextDist/nextIdx), keeping only the smallest k of (k + 32).\n// Uses the merge-path algorithm to partition the k outputs across the 32 lanes.\n// Each lane writes a disjoint contiguous segment [diag_start, diag_end) of the result.\nstatic inline __device__ void warp_merge_topk_with_32(\n    const float* curDist, const int* curIdx,\n    float* nextDist, int* nextIdx,\n    int k,\n    float b_dist_local, int b_idx_local,\n    unsigned mask)\n{\n    const int lane = lane_id();\n    const int n = 32;\n\n    // Partition the first k outputs evenly across the 32 lanes\n    int diag_start = (int)(((long long)lane * k) >> 5);              // floor(lane * k / 32)\n    int diag_end   = (int)((((long long)lane + 1) * k) >> 5);        // floor((lane+1)*k/32)\n\n    // Find (a_start, b_start) and (a_end, b_end) for the two diagonals\n    int a_start = mergepath_search_A_for_diag(diag_start, curDist, k, n, b_dist_local, mask);\n    int b_start = diag_start - a_start;\n\n    int a_end = mergepath_search_A_for_diag(diag_end, curDist, k, n, b_dist_local, mask);\n    int b_end = diag_end - a_end;\n\n    int i = a_start;\n    int j = b_start;\n    int o = diag_start;\n\n    // Sequentially merge this lane's chunk\n    while (o < diag_end) {\n        float a_val = (i < k) ? curDist[i] : CUDART_INF_F;\n        float b_val;\n        if (j < n) {\n            b_val = __shfl_sync(mask, b_dist_local, j);\n        } else {\n            b_val = CUDART_INF_F;\n        }\n\n        // Select the smaller element\n        if (b_val < a_val) {\n            // Taking from B\n            int b_idx = __shfl_sync(mask, b_idx_local, j);\n            nextDist[o] = b_val;\n            nextIdx[o]  = b_idx;\n            ++j;\n        } else {\n            // Taking from A\n            nextDist[o] = a_val;\n            nextIdx[o]  = curIdx[i];\n            ++i;\n        }\n        ++o;\n    }\n}\n\n// The CUDA kernel that performs k-NN for 2D points.\n// Each warp (32 threads) handles one query point.\n// Shared memory layout (dynamic):\n//   - float2 sData[tile_size]                      // cached data points\n//   - float  topk_dist0[warps_per_block * k]       // per-warp top-k distances (buffer 0)\n//   - int    topk_idx0 [warps_per_block * k]       // per-warp top-k indices   (buffer 0)\n//   - float  topk_dist1[warps_per_block * k]       // per-warp top-k distances (buffer 1)\n//   - int    topk_idx1 [warps_per_block * k]       // per-warp top-k indices   (buffer 1)\n__global__ void knn2d_kernel(const float2* __restrict__ query, int query_count,\n                             const float2* __restrict__ data,  int data_count,\n                             std::pair<int, float>* __restrict__ result,\n                             int k, int tile_size)\n{\n    extern __shared__ unsigned char smem_raw[];\n    float2* sData = reinterpret_cast<float2*>(smem_raw);\n\n    const int warps_per_block = blockDim.x >> 5;\n    const int warp_id_in_block = threadIdx.x >> 5;\n    const int lane = lane_id();\n    const int warp_global_id = blockIdx.x * warps_per_block + warp_id_in_block;\n    const bool warp_active = (warp_global_id < query_count);\n\n    // Compute shared memory pointers for per-warp top-k buffers\n    size_t offset_bytes = (size_t)tile_size * sizeof(float2);\n\n    float* topk_dist0 = reinterpret_cast<float*>(smem_raw + offset_bytes);\n    offset_bytes += (size_t)warps_per_block * k * sizeof(float);\n\n    int*   topk_idx0  = reinterpret_cast<int*>(smem_raw + offset_bytes);\n    offset_bytes += (size_t)warps_per_block * k * sizeof(int);\n\n    float* topk_dist1 = reinterpret_cast<float*>(smem_raw + offset_bytes);\n    offset_bytes += (size_t)warps_per_block * k * sizeof(float);\n\n    int*   topk_idx1  = reinterpret_cast<int*>(smem_raw + offset_bytes);\n    // offset_bytes += warps_per_block * k * sizeof(int); // not needed further\n\n    // Warp-local base pointers for the two buffers\n    float* my_dist0 = topk_dist0 + warp_id_in_block * k;\n    int*   my_idx0  = topk_idx0  + warp_id_in_block * k;\n    float* my_dist1 = topk_dist1 + warp_id_in_block * k;\n    int*   my_idx1  = topk_idx1  + warp_id_in_block * k;\n\n    // Initialize both buffers to +inf distances and -1 indices.\n    // This provides a correct starting state for the merge operations.\n    for (int pos = lane; pos < k; pos += 32) {\n        my_dist0[pos] = CUDART_INF_F;\n        my_idx0 [pos] = -1;\n        my_dist1[pos] = CUDART_INF_F;\n        my_idx1 [pos] = -1;\n    }\n    __syncwarp(); // ensure initialization is done within the warp\n\n    // Current/next buffer toggle\n    bool use_buf0 = true;\n\n    // Load the query point into registers and broadcast within the warp\n    float qx = 0.0f, qy = 0.0f;\n    if (warp_active) {\n        if (lane == 0) {\n            float2 q = query[warp_global_id];\n            qx = q.x; qy = q.y;\n        }\n        unsigned mask = __activemask();\n        qx = __shfl_sync(mask, qx, 0);\n        qy = __shfl_sync(mask, qy, 0);\n    }\n\n    // Process the dataset in tiles\n    for (int tile_start = 0; tile_start < data_count; tile_start += tile_size) {\n        int tile_count = min(tile_size, data_count - tile_start);\n\n        // Cooperative loading of the tile into shared memory\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            sData[i] = data[tile_start + i];\n        }\n\n        __syncthreads(); // ensure the whole tile is cached\n\n        if (warp_active) {\n            // Current and next top-k buffers for this warp\n            float* curDist = use_buf0 ? my_dist0 : my_dist1;\n            int*   curIdx  = use_buf0 ? my_idx0  : my_idx1;\n            float* nxtDist = use_buf0 ? my_dist1 : my_dist0;\n            int*   nxtIdx  = use_buf0 ? my_idx1  : my_idx0;\n\n            const unsigned full_mask = __activemask(); // all 32 threads in this warp are active here\n\n            // Process the tile in groups of 32 points; each lane handles one point per group\n            int group_count = (tile_count + 31) >> 5;\n            for (int g = 0; g < group_count; ++g) {\n                int idx_in_tile = (g << 5) + lane;\n                float cand_dist;\n                int   cand_idx;\n\n                if (idx_in_tile < tile_count) {\n                    float2 p = sData[idx_in_tile];\n                    float dx = p.x - qx;\n                    float dy = p.y - qy;\n                    // Squared Euclidean distance\n                    cand_dist = __fmaf_rn(dx, dx, dy * dy);\n                    cand_idx  = tile_start + idx_in_tile;\n                } else {\n                    // Inactive lanes in the last group contribute +inf (ignored by merges)\n                    cand_dist = CUDART_INF_F;\n                    cand_idx  = -1;\n                }\n\n                // Sort the 32 candidates in ascending order by distance (across the warp)\n                warp_bitonic_sort32_asc(cand_dist, cand_idx, full_mask);\n\n                // Merge the sorted 32 candidates with the current top-k into the next buffer\n                warp_merge_topk_with_32(curDist, curIdx, nxtDist, nxtIdx, k, cand_dist, cand_idx, full_mask);\n\n                __syncwarp(full_mask); // ensure all lanes completed writes before swapping buffers\n\n                // Swap buffers for the next group\n                float* tmpD = curDist; curDist = nxtDist; nxtDist = tmpD;\n                int*   tmpI = curIdx;  curIdx  = nxtIdx;  nxtIdx  = tmpI;\n                use_buf0 = (curDist == my_dist0); // track current buffer for consistency\n            }\n        }\n\n        __syncthreads(); // allow all warps to finish with this tile before loading the next\n    }\n\n    // Write out the final top-k for this query in ascending order (j-th nearest first)\n    if (warp_active) {\n        float* curDist = use_buf0 ? my_dist0 : my_dist1;\n        int*   curIdx  = use_buf0 ? my_idx0  : my_idx1;\n\n        size_t out_base = (size_t)warp_global_id * (size_t)k;\n        for (int pos = lane; pos < k; pos += 32) {\n            // Accessing std::pair fields directly is safe and avoids constructing temporaries\n            result[out_base + pos].first  = curIdx[pos];\n            result[out_base + pos].second = curDist[pos];\n        }\n    }\n}\n\n// Host interface to launch the k-NN kernel.\n// - Decides the number of warps per block (default 8).\n// - Computes an appropriate tile size based on available shared memory and k.\n// - Allocates sufficient dynamic shared memory for the data tile and per-warp top-k buffers.\n// - Uses cudaFuncSetAttribute to allow large dynamic shared memory on A100/H100.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data,  int data_count,\n             std::pair<int, float> *result, int k)\n{\n    // Choose number of warps per block. 8 warps (256 threads) gives good balance on A100/H100.\n    int warps_per_block = 8;\n    int threads_per_block = warps_per_block * 32;\n\n    // Determine the maximum dynamic shared memory per block supported (opt-in).\n    int device = 0;\n    cudaGetDevice(&device);\n\n    int smem_max_optin = 0;\n    cudaDeviceGetAttribute(&smem_max_optin, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    if (smem_max_optin == 0) {\n        // Fallback to non-optin limit if opt-in not supported (should not happen on A100/H100).\n        cudaDeviceGetAttribute(&smem_max_optin, cudaDevAttrMaxSharedMemoryPerBlock, device);\n    }\n\n    // Compute per-block shared memory usage for per-warp top-k buffers (double-buffered).\n    size_t topk_bytes_per_block = (size_t)warps_per_block * (size_t)k * (sizeof(float) + sizeof(int)) * 2;\n\n    // Reduce warps per block if necessary to fit shared memory constraints.\n    while (topk_bytes_per_block + (size_t)32 * sizeof(float2) > (size_t)smem_max_optin && warps_per_block > 1) {\n        warps_per_block >>= 1; // halve the warps per block\n        threads_per_block = warps_per_block * 32;\n        topk_bytes_per_block = (size_t)warps_per_block * (size_t)k * (sizeof(float) + sizeof(int)) * 2;\n    }\n\n    // Compute maximum tile size that fits into the remaining shared memory budget.\n    size_t max_tile_bytes = (topk_bytes_per_block < (size_t)smem_max_optin)\n                          ? ((size_t)smem_max_optin - topk_bytes_per_block)\n                          : 0;\n    // Prefer a large tile (e.g., up to 4096 points), but cap to what's available.\n    int desired_tile_points = 4096;\n    int tile_points = (int)(max_tile_bytes / sizeof(float2));\n    if (tile_points > desired_tile_points) tile_points = desired_tile_points;\n    if (tile_points < 32) tile_points = 32; // ensure at least one full warp of points\n    // Round down to a multiple of 32 for simplicity (complete candidate groups)\n    tile_points = (tile_points / 32) * 32;\n    if (tile_points < 32) tile_points = 32;\n\n    // Recompute final dynamic shared memory size\n    size_t dynamic_smem_bytes = (size_t)tile_points * sizeof(float2) + topk_bytes_per_block;\n\n    // Opt-in to large dynamic shared memory if necessary\n    cudaFuncSetAttribute(knn2d_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)dynamic_smem_bytes);\n\n    // Compute grid configuration\n    int blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Launch the kernel\n    knn2d_kernel<<<blocks, threads_per_block, dynamic_smem_bytes>>>(\n        query, query_count, data, data_count, result, k, tile_points\n    );\n\n    // Optional: synchronize or check errors here if desired by the caller.\n    // cudaDeviceSynchronize();\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 706, "completion_tokens": 20141, "total_tokens": 20847, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 16320, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db8fb788190b1f731ee7fb42b32", "custom_id": "knn04-08", "response": {"status_code": 200, "request_id": "8b7dd632bb84c719626493ebb7c9d7e1", "body": {"id": "chatcmpl-C4sqzQAEhwPgWfYmIY3vAjLF8iBpY", "object": "chat.completion", "created": 1755280205, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n#include <cfloat>\n#include <stdint.h>\n\n// Optimized CUDA k-NN (k is power-of-two between 32 and 1024) for 2D points.\n// Each query is processed by a single warp (32 threads). A thread block contains\n// multiple warps; the block collaboratively loads a batch (tile) of data points\n// into shared memory. Each warp then computes distances from its query to all\n// cached data points and cooperatively maintains a per-warp top-k using a\n// 32-ary max-heap stored in shared memory. Updating the heap uses all 32 threads\n// cooperatively (warp-level parallelism) and supports multiple candidate updates\n// per tile. At the end, the heap is converted to ascending order via heap-sort\n// and written to the output.\n//\n// Key implementation notes:\n// - 32-ary heap yields depth <= 2 for k <= 1024 (since 32^2 = 1024), making\n//   each sift-down very cheap while maximally utilizing a warp for the argmax\n//   over the 32 children at each heap level.\n// - Data points are processed in batches (tiles) loaded into shared memory by\n//   the entire block. Each warp then iterates over this tile in groups of 32\n//   candidates at a time.\n// - For each group of up to 32 candidates, we cooperatively determine winners\n//   (those with distance < current heap root). Among those, we iteratively pick\n//   the smallest winner (warp argmin) and perform a replace-root operation on\n//   the heap cooperatively with the warp. This uses multiple threads in the\n//   warp to update the intermediate result with multiple candidates per tile.\n// - Each warp maintains a private heap of distances and indices in shared memory,\n//   with no additional global memory allocations.\n//\n// Tunable parameters:\n// - WARPS_PER_BLOCK: number of warps per block (4 => 128 threads).\n// - BATCH_SIZE: number of data points per tile loaded into shared memory.\n//\n// These are selected to balance shared memory usage and occupancy for A100/H100.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Tunable hyper-parameters.\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 4\n#endif\n#ifndef THREADS_PER_BLOCK\n#define THREADS_PER_BLOCK (WARPS_PER_BLOCK * WARP_SIZE)\n#endif\n#ifndef BATCH_SIZE\n#define BATCH_SIZE 8192  // BATCH_SIZE * sizeof(float2) + WARPS_PER_BLOCK * k * 8 bytes must fit in SMEM\n#endif\n\n// Full warp mask for modern architectures (assumes 32-thread warps).\nstatic __device__ __forceinline__ unsigned full_warp_mask() { return 0xFFFFFFFFu; }\n\n// Compute squared Euclidean distance between float2 points.\nstatic __device__ __forceinline__ float sqr_distance(const float2 a, const float2 b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    // Use FMA to reduce instruction count.\n    return fmaf(dx, dx, dy * dy);\n}\n\n// Warp-wide argmax reduction for (value, index) pairs.\n// Returns the maximum value and its absolute array index (not lane index).\n// The reduction uses all 32 lanes; for invalid participants, pass value = -INFINITY.\nstatic __device__ __forceinline__ void warp_argmax(float value, int index, float &max_val_out, int &max_idx_out) {\n    float v = value;\n    int   i = index;\n    // Tree-reduction using shfl_down. Full warp participates.\n    #pragma unroll\n    for (int offset = WARP_SIZE >> 1; offset > 0; offset >>= 1) {\n        float v_other = __shfl_down_sync(full_warp_mask(), v, offset);\n        int   i_other = __shfl_down_sync(full_warp_mask(), i, offset);\n        if (v_other > v) {\n            v = v_other;\n            i = i_other;\n        }\n    }\n    // Broadcast the result from lane 0 to all lanes.\n    max_val_out = __shfl_sync(full_warp_mask(), v, 0);\n    max_idx_out = __shfl_sync(full_warp_mask(), i, 0);\n}\n\n// Warp-wide argmin reduction across lanes; lanes not in 'active_mask' should pass value=+INFINITY and any lane_id.\n// Returns minimal value and the lane id that held it.\nstatic __device__ __forceinline__ void warp_argmin_lane(float value, int lane_id, float &min_val_out, int &min_lane_out) {\n    float v = value;\n    int   l = lane_id;\n    #pragma unroll\n    for (int offset = WARP_SIZE >> 1; offset > 0; offset >>= 1) {\n        float v_other = __shfl_down_sync(full_warp_mask(), v, offset);\n        int   l_other = __shfl_down_sync(full_warp_mask(), l, offset);\n        // Prefer strictly smaller; tie-breaker on lower lane id (optional)\n        if ((v_other < v) || ((v_other == v) && (l_other < l))) {\n            v = v_other;\n            l = l_other;\n        }\n    }\n    min_val_out  = __shfl_sync(full_warp_mask(), v, 0);\n    min_lane_out = __shfl_sync(full_warp_mask(), l, 0);\n}\n\n// Sift-down operation for a 32-ary max-heap stored in shared memory.\n// Cooperatively executed by an entire warp. 'temp_dist'/'temp_idx' are the\n// values to be placed starting at 'start_idx'. The heap has 'heap_size' elements.\n//\n// Heap layout (0-based):\n//  - Node i has up to 32 children at positions [32*i + 1, ..., 32*i + 32] (within [0, heap_size-1]).\nstatic __device__ __forceinline__ void warp_heap_sift_down(float *heap_dists, int *heap_idxs, int heap_size,\n                                                           int start_idx, float temp_dist, int temp_idx,\n                                                           int lane) {\n    int i = start_idx;\n    // Common warp path; no divergence among lanes.\n    while (true) {\n        int child_start = i * WARP_SIZE + 1;\n        if (child_start >= heap_size) {\n            // No children; place here.\n            if (lane == 0) {\n                heap_dists[i] = temp_dist;\n                heap_idxs[i]  = temp_idx;\n            }\n            return;\n        }\n        int child_count = heap_size - child_start;\n        if (child_count > WARP_SIZE) child_count = WARP_SIZE;\n\n        // Each lane inspects one child; lanes beyond child_count set value to -inf.\n        int child_pos = child_start + lane;\n        float child_val = (lane < child_count) ? heap_dists[child_pos] : -CUDART_INF_F;\n        int child_abs_idx = child_pos;\n\n        // Compute argmax among children.\n        float max_child_val;\n        int   max_child_abs_idx;\n        warp_argmax(child_val, child_abs_idx, max_child_val, max_child_abs_idx);\n\n        // If temp is larger or equal than the maximum child, place temp here.\n        if (!(temp_dist < max_child_val)) {\n            if (lane == 0) {\n                heap_dists[i] = temp_dist;\n                heap_idxs[i]  = temp_idx;\n            }\n            return;\n        }\n\n        // Move the maximum child up.\n        if (lane == 0) {\n            heap_dists[i] = max_child_val;\n            heap_idxs[i]  = heap_idxs[max_child_abs_idx];\n        }\n        // Continue sifting down.\n        i = max_child_abs_idx;\n        // Loop until leaf reached.\n    }\n}\n\n// Build a 32-ary max-heap in-place for the first 'heap_size' elements of arrays.\n// All lanes in the warp cooperate on each sift-down.\nstatic __device__ __forceinline__ void warp_heap_build(float *heap_dists, int *heap_idxs, int heap_size, int lane) {\n    if (heap_size <= 1) return;\n    // Last internal node index in 32-ary heap: floor((n - 2) / 32).\n    int last_internal = (heap_size - 2) / WARP_SIZE;\n    for (int i = last_internal; i >= 0; --i) {\n        float temp_dist = heap_dists[i];\n        int   temp_idx  = heap_idxs[i];\n        warp_heap_sift_down(heap_dists, heap_idxs, heap_size, i, temp_dist, temp_idx, lane);\n    }\n}\n\n// Replace the root (maximum) of the 32-ary max-heap with (new_dist, new_idx) and fix heap property.\n// All lanes in the warp cooperate via argmax of children at each level.\nstatic __device__ __forceinline__ void warp_heap_replace_root(float *heap_dists, int *heap_idxs, int heap_size,\n                                                              float new_dist, int new_idx, int lane) {\n    if (heap_size == 0) return;\n    // Place new at root then sift-down.\n    warp_heap_sift_down(heap_dists, heap_idxs, heap_size, 0, new_dist, new_idx, lane);\n}\n\n// Pop the heap root (maximum) and replace it with the last element, then sift-down.\n// The popped pair is returned via references (only lane 0 uses them).\nstatic __device__ __forceinline__ void warp_heap_pop(float *heap_dists, int *heap_idxs, int &heap_size,\n                                                     float &out_dist, int &out_idx, int lane) {\n    // Read root\n    float root_dist = heap_dists[0];\n    int   root_idx  = heap_idxs[0];\n    if (lane == 0) {\n        out_dist = root_dist;\n        out_idx  = root_idx;\n    }\n    // Move last to root and reduce heap size\n    int new_size = heap_size - 1;\n    if (new_size > 0) {\n        float last_dist = heap_dists[new_size];\n        int   last_idx  = heap_idxs[new_size];\n        // Set heap_size to new_size (uniform across warp)\n        heap_size = new_size;\n        // Sift-down the last element placed at root\n        warp_heap_sift_down(heap_dists, heap_idxs, heap_size, 0, last_dist, last_idx, lane);\n    } else {\n        // Heap becomes empty\n        heap_size = new_size;\n    }\n}\n\n// Main KNN kernel: each warp processes one query.\n__global__ void knn_kernel(const float2 * __restrict__ query, int query_count,\n                           const float2 * __restrict__ data,  int data_count,\n                           std::pair<int, float> * __restrict__ result, int k) {\n    // Shared memory layout:\n    // [0 .. BATCH_SIZE-1]: float2 tile of data points\n    // Then per-warp heaps:\n    //   distances: WARPS_PER_BLOCK * k floats\n    //   indices:   WARPS_PER_BLOCK * k ints\n    extern __shared__ unsigned char smem[];\n    // Data tile at the beginning\n    float2 *s_data = reinterpret_cast<float2*>(smem);\n    size_t s_data_bytes = static_cast<size_t>(BATCH_SIZE) * sizeof(float2);\n    // Per-warp heaps after the data tile\n    float *s_heap_dists_base = reinterpret_cast<float*>(smem + s_data_bytes);\n    int   *s_heap_idxs_base  = reinterpret_cast<int*>(s_heap_dists_base + WARPS_PER_BLOCK * k);\n\n    const int tid   = threadIdx.x;\n    const int lane  = tid & (WARP_SIZE - 1);\n    const int warp  = tid >> 5; // warp index within block\n    const int warp_global = blockIdx.x * WARPS_PER_BLOCK + warp;\n\n    const bool warp_active = (warp_global < query_count);\n\n    // Warp-private heap pointers in shared memory.\n    float *heap_dists = s_heap_dists_base + warp * k;\n    int   *heap_idxs  = s_heap_idxs_base  + warp * k;\n\n    // Load query for this warp\n    float2 q;\n    if (warp_active) {\n        // Lane 0 loads query and broadcasts to other lanes\n        if (lane == 0) {\n            q = query[warp_global];\n        }\n        q.x = __shfl_sync(full_warp_mask(), q.x, 0);\n        q.y = __shfl_sync(full_warp_mask(), q.y, 0);\n    } else {\n        // Avoid uninitialized use in inactive warps\n        q.x = 0.0f; q.y = 0.0f;\n    }\n\n    // Initialize heap size and (optionally) content.\n    int heap_size = 0;\n\n    // Process the dataset in batches loaded into shared memory\n    for (int base = 0; base < data_count; base += BATCH_SIZE) {\n        int tile_count = data_count - base;\n        if (tile_count > BATCH_SIZE) tile_count = BATCH_SIZE;\n\n        // Block-wide cooperative load into shared memory\n        for (int i = tid; i < tile_count; i += blockDim.x) {\n            s_data[i] = data[base + i];\n        }\n        __syncthreads();\n\n        // Each warp processes this tile. Iterate in groups of 32 points (one per lane)\n        for (int j = 0; j < tile_count; j += WARP_SIZE) {\n            // Local lane's candidate index within tile and global index\n            int local_idx = j + lane;\n            bool valid = (local_idx < tile_count);\n\n            // Compute candidate distance (or INF if invalid)\n            float2 p;\n            if (valid) {\n                p = s_data[local_idx];\n            } else {\n                p.x = 0.0f; p.y = 0.0f;\n            }\n            float cand_dist = valid ? sqr_distance(q, p) : CUDART_INF_F;\n            int   cand_gidx = base + local_idx; // global index in data array\n\n            if (!warp_active) {\n                continue; // inactive warps just participate in __syncthreads outside\n            }\n\n            // Phase 1: Fill the heap until it has k elements.\n            if (heap_size < k) {\n                // Number of valid candidates in this group\n                int group_valid = tile_count - j;\n                if (group_valid > WARP_SIZE) group_valid = WARP_SIZE;\n\n                int capacity_left = k - heap_size;\n                int nfill = (capacity_left < group_valid) ? capacity_left : group_valid;\n\n                if (lane < nfill) {\n                    // Append directly to heap arrays (unsorted build buffer)\n                    heap_dists[heap_size + lane] = cand_dist;\n                    heap_idxs [heap_size + lane] = cand_gidx;\n                }\n                // Update heap_size uniformly across warp\n                heap_size += nfill;\n\n                // If we just completed filling to k, build the 32-ary max-heap cooperatively\n                if (heap_size == k) {\n                    // Build heap over first k elements\n                    warp_heap_build(heap_dists, heap_idxs, k, lane);\n\n                    // Process leftover candidates in this group if any\n                    int leftover = group_valid - nfill;\n                    if (leftover > 0) {\n                        // Determine current threshold (root of max-heap)\n                        float root = heap_dists[0];\n                        // Active mask of leftover lanes only\n                        bool is_leftover_lane = (lane >= nfill) && (lane < group_valid);\n                        float active_val = (is_leftover_lane && (cand_dist < root)) ? cand_dist : CUDART_INF_F;\n\n                        // Iteratively pick minimal among active and replace root\n                        // (All threads cooperate via argmin and heap replace)\n                        // We do not re-evaluate activity after replacements because the threshold only decreases.\n                        unsigned active_mask = __ballot_sync(full_warp_mask(), is_leftover_lane && (cand_dist < root));\n                        while (active_mask) {\n                            // Compute argmin among currently active lanes\n                            float min_val;\n                            int   min_lane;\n                            warp_argmin_lane(( (active_mask >> lane) & 1u ) ? cand_dist : CUDART_INF_F, lane, min_val, min_lane);\n                            // Broadcast winner's data\n                            float win_dist = __shfl_sync(full_warp_mask(), cand_dist, min_lane);\n                            int   win_idx  = __shfl_sync(full_warp_mask(), cand_gidx,  min_lane);\n\n                            // Replace root with winner and fix heap\n                            warp_heap_replace_root(heap_dists, heap_idxs, k, win_dist, win_idx, lane);\n\n                            // Clear this winner from active set\n                            if (lane == min_lane) {\n                                // Make this lane inactive\n                                active_mask &= ~(1u << min_lane);\n                            }\n                            // Broadcast the updated active_mask to all lanes\n                            active_mask = __shfl_sync(full_warp_mask(), active_mask, min_lane);\n                        }\n                    }\n                }\n                // Continue to next group\n                continue;\n            }\n\n            // Phase 2: Heap is full; process current group of candidates against current threshold.\n            // Read current heap root (threshold)\n            float root = heap_dists[0];\n\n            // Determine which lanes have candidates better than current root\n            bool is_winner = valid && (cand_dist < root);\n            unsigned win_mask = __ballot_sync(full_warp_mask(), is_winner);\n\n            // Repeatedly extract the smallest among winners and replace heap root\n            while (win_mask) {\n                float min_val;\n                int   min_lane;\n                warp_argmin_lane( ((win_mask >> lane) & 1u) ? cand_dist : CUDART_INF_F, lane, min_val, min_lane);\n\n                // Broadcast winner's data to all lanes\n                float win_dist = __shfl_sync(full_warp_mask(), cand_dist, min_lane);\n                int   win_idx  = __shfl_sync(full_warp_mask(), cand_gidx,  min_lane);\n\n                // Replace root cooperatively and restore heap property\n                warp_heap_replace_root(heap_dists, heap_idxs, k, win_dist, win_idx, lane);\n\n                // Clear this winner bit from mask (only the winner lane modifies it; then broadcast)\n                if (lane == min_lane) {\n                    win_mask &= ~(1u << min_lane);\n                }\n                win_mask = __shfl_sync(full_warp_mask(), win_mask, min_lane);\n            }\n        }\n\n        __syncthreads(); // Ensure all warps are done before the next tile load\n    }\n\n    // Finalize: transform heap into ascending order and write to result.\n    if (warp_active) {\n        int out_count = k;\n        // Heap-sort: repeatedly pop the maximum to the end, resulting in ascending order.\n        while (out_count > 0) {\n            float top_dist = 0.0f;\n            int   top_idx  = -1;\n            // Pop root to (top_dist, top_idx)\n            warp_heap_pop(heap_dists, heap_idxs, out_count, top_dist, top_idx, lane);\n            if (lane == 0) {\n                // Write to result; popping max to the end yields ascending order in [0..k-1]\n                int out_pos = out_count; // after decrement, out_count is the index for this max\n                // Store pair: index and distance\n                result[warp_global * k + out_pos].first  = top_idx;\n                result[warp_global * k + out_pos].second = top_dist;\n            }\n            // Continue until heap empty\n        }\n    }\n}\n\n// Host-side launcher. Assumes device pointers are valid and cudaMalloc'd.\n// k is a power-of-two between 32 and 1024 inclusive and data_count >= k.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data,  int data_count,\n             std::pair<int, float> *result, int k) {\n    // Grid configuration: one warp per query\n    dim3 block(THREADS_PER_BLOCK, 1, 1);\n    int warps_total = (query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK;\n    dim3 grid(warps_total, 1, 1);\n\n    // Dynamic shared memory size:\n    // - Data tile\n    // - Per-warp heaps: WARPS_PER_BLOCK * k * (sizeof(float) + sizeof(int))\n    size_t smem_size = static_cast<size_t>(BATCH_SIZE) * sizeof(float2)\n                     + static_cast<size_t>(WARPS_PER_BLOCK) * static_cast<size_t>(k) * (sizeof(float) + sizeof(int));\n\n    knn_kernel<<<grid, block, smem_size>>>(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 706, "completion_tokens": 27017, "total_tokens": 27723, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 22528, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db7c5f88190b5b64c37718f61e1", "custom_id": "knn04-09", "response": {"status_code": 200, "request_id": "e13e3a23f61303d2320193e721d4a62d", "body": {"id": "chatcmpl-C4ss2J6O9PZPAz83FbTxw8uDLVkqG", "object": "chat.completion", "created": 1755280270, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda.h>\n#include <cuda_runtime.h>\n#include <math_constants.h>\n#include <utility>\n\n// This implementation assigns exactly one warp (32 threads) to each query point.\n// It processes the dataset in tiles cached in shared memory. Each warp maintains\n// a private k-NN result set distributed across its 32 lanes in registers: each lane\n// holds k/32 entries. Distances for 32 candidates are computed in parallel,\n// then the warp collaboratively inserts the best candidates into the current k-NN\n// set by repeatedly identifying the smallest new candidate and replacing the\n// current global worst (largest) entry in the k-NN set. At the end, each warp\n// writes its k results to shared memory and performs a warp-scope bitonic sort\n// to produce results sorted by ascending distance, then writes to global memory.\n\nstruct PairIF {\n    int   first;\n    float second;\n};\n\nstatic_assert(sizeof(PairIF) == sizeof(std::pair<int, float>), \"PairIF must match std::pair<int,float> layout\");\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Utilities for warp-level operations.\nstatic __forceinline__ __device__ unsigned warp_full_mask() {\n    return 0xFFFFFFFFu;\n}\n\nstatic __forceinline__ __device__ int lane_id() {\n#if __CUDACC_VER_MAJOR__ >= 9\n    return threadIdx.x & (WARP_SIZE - 1);\n#else\n    return threadIdx.x % WARP_SIZE;\n#endif\n}\n\nstruct ArgPairF {\n    float val;\n    int   idx;\n};\n\n// Warp-wide argmax reduction: returns (max value, lane index containing it) broadcast to all lanes.\nstatic __forceinline__ __device__ ArgPairF warp_argmax(float v) {\n    unsigned mask = warp_full_mask();\n    int l = lane_id();\n    float maxv = v;\n    int   maxi = l;\n#pragma unroll\n    for (int offset = WARP_SIZE >> 1; offset > 0; offset >>= 1) {\n        float ov = __shfl_down_sync(mask, maxv, offset);\n        int   oi = __shfl_down_sync(mask, maxi, offset);\n        if (ov > maxv) {\n            maxv = ov;\n            maxi = oi;\n        }\n    }\n    ArgPairF r;\n    r.val = __shfl_sync(mask, maxv, 0);\n    r.idx = __shfl_sync(mask, maxi, 0);\n    return r;\n}\n\n// Warp-wide argmin reduction: returns (min value, lane index containing it) broadcast to all lanes.\nstatic __forceinline__ __device__ ArgPairF warp_argmin(float v) {\n    unsigned mask = warp_full_mask();\n    int l = lane_id();\n    float minv = v;\n    int   mini = l;\n#pragma unroll\n    for (int offset = WARP_SIZE >> 1; offset > 0; offset >>= 1) {\n        float ov = __shfl_down_sync(mask, minv, offset);\n        int   oi = __shfl_down_sync(mask, mini, offset);\n        if (ov < minv) {\n            minv = ov;\n            mini = oi;\n        }\n    }\n    ArgPairF r;\n    r.val = __shfl_sync(mask, minv, 0);\n    r.idx = __shfl_sync(mask, mini, 0);\n    return r;\n}\n\n// Find local argmax across S entries in register arrays.\ntemplate <int MAX_S>\nstatic __forceinline__ __device__\nvoid local_argmax(const float (&dist)[MAX_S], int S, float &maxv, int &maxp) {\n    maxv = -CUDART_INF_F;\n    maxp = 0;\n#pragma unroll\n    for (int i = 0; i < MAX_S; ++i) {\n        if (i < S) {\n            float v = dist[i];\n            if (v > maxv) {\n                maxv = v;\n                maxp = i;\n            }\n        }\n    }\n}\n\n// Copy per-lane stripe of k results to warp scratch shared memory.\n// Each lane L writes S entries at positions [L*S .. L*S+S-1].\ntemplate <int MAX_S>\nstatic __forceinline__ __device__\nvoid write_warp_scratch(PairIF *warpScratch, int lane, const int (&idx)[MAX_S], const float (&dist)[MAX_S], int S) {\n#pragma unroll\n    for (int i = 0; i < MAX_S; ++i) {\n        if (i < S) {\n            int pos = lane * S + i;\n            warpScratch[pos].first  = idx[i];\n            warpScratch[pos].second = dist[i];\n        }\n    }\n}\n\n// Warp-scope bitonic sort for PairIF array in shared memory.\n// - count must be a power of two (here, k).\n// - Sorting by ascending .second (distance).\nstatic __forceinline__ __device__\nvoid warp_bitonic_sort_pairs(PairIF *arr, int count) {\n    unsigned mask = warp_full_mask();\n    int lane = lane_id();\n    // Standard in-place bitonic sort; threads in a warp cooperatively process all indices.\n    for (int k = 2; k <= count; k <<= 1) {\n        for (int j = k >> 1; j > 0; j >>= 1) {\n            // Each thread handles multiple indices in strides of WARP_SIZE.\n            for (int i = lane; i < count; i += WARP_SIZE) {\n                int ixj = i ^ j;\n                if (ixj > i) {\n                    bool up = ((i & k) == 0);\n                    PairIF ai = arr[i];\n                    PairIF aj = arr[ixj];\n                    bool swap = up ? (ai.second > aj.second) : (ai.second < aj.second);\n                    if (swap) {\n                        arr[i]   = aj;\n                        arr[ixj] = ai;\n                    }\n                }\n            }\n            __syncwarp(mask);\n        }\n    }\n}\n\nextern \"C\" __global__\nvoid knn_kernel_2d(const float2 * __restrict__ query,\n                   int query_count,\n                   const float2 * __restrict__ data,\n                   int data_count,\n                   PairIF * __restrict__ result,\n                   int k,\n                   int tile_points,\n                   int warps_per_block)\n{\n    // Dynamic shared memory layout:\n    // [0 .. tile_points-1] float2 tile of data points\n    // [tile_points .. tile_points + warps_per_block*k - 1] PairIF per-warp scratch\n    extern __shared__ unsigned char smem_uchar[];\n    float2 *s_points = reinterpret_cast<float2*>(smem_uchar);\n    PairIF *s_pairs  = reinterpret_cast<PairIF*>(s_points + tile_points);\n\n    int tid  = threadIdx.x;\n    int lane = lane_id();\n    int warp = tid >> 5;\n\n    // Query index handled by this warp.\n    int qidx = blockIdx.x * warps_per_block + warp;\n    bool has_query = (qidx < query_count);\n\n    // Number of per-lane slots in k-NN set.\n    // k is guaranteed to be a power of two between 32 and 1024 inclusive.\n    const int MAX_S = 32;\n    int S = k >> 5; // k / 32\n    // Intermediate k-NN kept in registers: lane-striped layout, S entries per lane.\n    float best_dist[MAX_S];\n    int   best_idx [MAX_S];\n#pragma unroll\n    for (int i = 0; i < MAX_S; ++i) {\n        if (i < S) {\n            best_dist[i] = CUDART_INF_F;\n            best_idx[i]  = -1;\n        }\n    }\n\n    // Load query point into registers and broadcast to all lanes in this warp.\n    float qx = 0.0f, qy = 0.0f;\n    if (has_query && lane == 0) {\n        float2 q = query[qidx];\n        qx = q.x; qy = q.y;\n    }\n    unsigned full = warp_full_mask();\n    qx = __shfl_sync(full, qx, 0);\n    qy = __shfl_sync(full, qy, 0);\n\n    // Process data in tiles.\n    for (int base = 0; base < data_count; base += tile_points) {\n        int tile_count = data_count - base;\n        if (tile_count > tile_points) tile_count = tile_points;\n\n        // Load tile into shared memory cooperatively across the whole block.\n        for (int i = tid; i < tile_count; i += blockDim.x) {\n            s_points[i] = data[base + i];\n        }\n        __syncthreads();\n\n        // Each warp processes this tile against its query point.\n        if (has_query) {\n            // Process the tile in groups of 32 candidates so each lane contributes one candidate.\n            for (int offset = 0; offset < tile_count; offset += WARP_SIZE) {\n                int local_idx = offset + lane;\n\n                float cand_dist = CUDART_INF_F;\n                int   cand_idx  = -1;\n\n                if (local_idx < tile_count) {\n                    float2 p = s_points[local_idx];\n                    float dx = qx - p.x;\n                    float dy = qy - p.y;\n                    cand_dist = dx * dx + dy * dy;\n                    cand_idx  = base + local_idx;\n                }\n\n                // Compute current global threshold T = maximum distance among the current k-NN set.\n                float lmaxv;\n                int   lmaxp;\n                local_argmax<MAX_S>(best_dist, S, lmaxv, lmaxp);\n                ArgPairF gmax = warp_argmax(lmaxv);\n                float T = gmax.val;\n\n                // Determine which candidates beat the current threshold.\n                bool pass = (cand_dist < T);\n                unsigned passmask = __ballot_sync(full, pass);\n\n                // Insert all passing candidates one-by-one in order of increasing distance.\n                while (passmask) {\n                    float v = pass ? cand_dist : CUDART_INF_F;\n                    ArgPairF mn = warp_argmin(v);\n                    int bestLane = mn.idx;\n                    float bestVal = mn.val;\n                    int bestId = __shfl_sync(full, cand_idx, bestLane);\n\n                    // Find current global worst location (lane and position).\n                    local_argmax<MAX_S>(best_dist, S, lmaxv, lmaxp);\n                    gmax = warp_argmax(lmaxv);\n                    int worstLane = gmax.idx;\n                    int worstPos  = __shfl_sync(full, lmaxp, worstLane);\n\n                    // Replace worst with the best candidate.\n                    if (lane == worstLane) {\n                        best_dist[worstPos] = bestVal;\n                        best_idx [worstPos] = bestId;\n                    }\n\n                    // Remove this candidate from further consideration in this group.\n                    if (lane == bestLane) {\n                        pass = false;\n                        cand_dist = CUDART_INF_F;\n                    }\n                    passmask = __ballot_sync(full, pass);\n                }\n            }\n        }\n\n        __syncthreads(); // Synchronize before next tile load.\n    }\n\n    // After processing all tiles, each warp has its k results distributed across lanes.\n    // Move to shared memory scratch private to the warp and sort ascending by distance.\n    if (has_query) {\n        PairIF *warpScratch = s_pairs + warp * k;\n\n        // Write in lane-striped order: positions lane*S + i.\n        write_warp_scratch<MAX_S>(warpScratch, lane, best_idx, best_dist, S);\n        __syncwarp(full);\n\n        // Warp-scope bitonic sort on k items in shared memory.\n        warp_bitonic_sort_pairs(warpScratch, k);\n        __syncwarp(full);\n\n        // Write sorted results to global memory at positions result[qidx * k + j].\n        PairIF *dst = result + qidx * k;\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            dst[i] = warpScratch[i];\n        }\n    }\n}\n\n// Host-side launcher that configures tile size and block dimensions based on available shared memory.\n// - Uses dynamic shared memory for both the data tile and per-warp sorting scratch.\n// - Attempts to opt-in to maximum dynamic shared memory per block for data center GPUs (A100/H100).\nextern \"C\"\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data, int data_count,\n             std::pair<int, float> *result, int k)\n{\n    // Determine device properties and available dynamic shared memory.\n    int device = 0;\n    cudaGetDevice(&device);\n\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&prop, device);\n\n    int maxOptin = 0;\n    cudaDeviceGetAttribute(&maxOptin, cudaDevAttrMaxDynamicSharedMemoryPerBlockOptin, device);\n    size_t maxDynSmem = (maxOptin > 0) ? (size_t)maxOptin : (size_t)prop.sharedMemPerBlock;\n\n    // Choose number of warps per block to balance occupancy and shared memory usage.\n    // Try from 8 warps down to 1, ensuring we can allocate at least 32 points per tile.\n    int chosenWarpsPerBlock = 8;\n    int threadsPerBlock = 0;\n    int tile_points = 0;\n\n    // We need shared memory for:\n    // - tile_points * sizeof(float2)\n    // - warps_per_block * k * sizeof(PairIF)  (per-warp sorting scratch)\n    for (int wpb = 8; wpb >= 1; wpb >>= 1) {\n        int tpb = wpb * WARP_SIZE;\n        if (tpb > prop.maxThreadsPerBlock) continue;\n\n        size_t scratchBytes = (size_t)wpb * (size_t)k * sizeof(PairIF);\n        if (scratchBytes >= maxDynSmem) continue; // no room for tile\n\n        int tpoints = (int)((maxDynSmem - scratchBytes) / sizeof(float2));\n        if (tpoints >= WARP_SIZE) {\n            chosenWarpsPerBlock = wpb;\n            threadsPerBlock = tpb;\n            tile_points = tpoints;\n            break;\n        }\n    }\n\n    if (threadsPerBlock == 0) {\n        // Fallback: minimal configuration with 1 warp; tile points at least 32 or as many as fit.\n        chosenWarpsPerBlock = 1;\n        threadsPerBlock = WARP_SIZE;\n        size_t scratchBytes = (size_t)chosenWarpsPerBlock * (size_t)k * sizeof(PairIF);\n        size_t remaining = (maxDynSmem > scratchBytes) ? (maxDynSmem - scratchBytes) : 0;\n        tile_points = (int)(remaining / sizeof(float2));\n        if (tile_points < WARP_SIZE) tile_points = WARP_SIZE; // ensure progress\n    }\n\n    // Total dynamic shared memory required for kernel launch.\n    size_t smemBytes = (size_t)tile_points * sizeof(float2)\n                     + (size_t)chosenWarpsPerBlock * (size_t)k * sizeof(PairIF);\n\n    // Opt-in to large dynamic shared memory if supported.\n    cudaFuncSetAttribute(knn_kernel_2d, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)smemBytes);\n    cudaFuncSetCacheConfig(knn_kernel_2d, cudaFuncCachePreferShared);\n\n    int blocks = (query_count + chosenWarpsPerBlock - 1) / chosenWarpsPerBlock;\n\n    // Launch kernel. The result pointer is reinterpreted as PairIF to match device layout.\n    knn_kernel_2d<<<blocks, threadsPerBlock, smemBytes>>>(\n        query, query_count,\n        data, data_count,\n        reinterpret_cast<PairIF*>(result),\n        k,\n        tile_points,\n        chosenWarpsPerBlock\n    );\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 706, "completion_tokens": 17647, "total_tokens": 18353, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 14208, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db7cdfc81908780816e7b1a6522", "custom_id": "knn04-10", "response": {"status_code": 200, "request_id": "61e84cec840db5d193d7d770c6f4badf", "body": {"id": "chatcmpl-C4ssT38b2l3JreEJTmGatXxWbSBZd", "object": "chat.completion", "created": 1755280297, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// KNN for 2D points using warp-per-query with shared-memory tiling.\n// - Each warp (32 threads) processes one query point.\n// - The block cooperatively loads tiles of data points into shared memory.\n// - Each warp computes distances from its query to the tile points and updates a per-warp top-k buffer in shared memory.\n// - Candidates from each warp are processed in sorted ascending order per 32-point batch using warp-level bitonic sort with shuffles.\n// - The per-warp intermediate top-k buffer is updated with multiple candidates per batch in a warp-synchronous manner.\n// - After scanning all data points, each warp sorts its k results (bitonic sort in shared memory) and writes them to the output.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Tunable parameters:\n// Number of warps per block. 8 warps (256 threads) is a good balance for H100/A100.\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 8\n#endif\n\n// Tile size: number of data points (float2) loaded into shared memory at once.\n// 4096 points => 32KB shared memory for tile, leaving room for per-warp top-k buffers even for k=1024.\n#ifndef TILE_POINTS\n#define TILE_POINTS 4096\n#endif\n\n// Full warp mask\n#ifndef FULL_MASK\n#define FULL_MASK 0xFFFFFFFFu\n#endif\n\n// Device utility: warp-level bitonic sort of 32 keys with associated values, ascending order.\n// Each lane holds one key/value; after the sort, lane 'i' holds the i-th smallest key/value among the warp.\n__device__ __forceinline__\nvoid warp_bitonic_sort_asc_32(float &key, int &val, unsigned lane_id)\n{\n    // Standard warp-level bitonic sorting network using XOR shuffles.\n    for (int k = 2; k <= WARP_SIZE; k <<= 1)\n    {\n        // For each stage, do log2(k) steps with decreasing 'j'\n        for (int j = k >> 1; j > 0; j >>= 1)\n        {\n            float other_key = __shfl_xor_sync(FULL_MASK, key, j);\n            int other_val   = __shfl_xor_sync(FULL_MASK, val, j);\n\n            bool up = ((lane_id & k) == 0);\n            // If up: keep smaller on lower lane; If down: keep larger on lower lane.\n            bool cond = (key > other_key) == up;\n            float selected_key = cond ? other_key : key;\n            int   selected_val = cond ? other_val : val;\n\n            // The partner lane must keep the complementary element, but since all lanes execute\n            // the same code, this formulation ensures consistency.\n            key = selected_key;\n            val = selected_val;\n        }\n    }\n}\n\n// Device utility: find the maximum value and its index in arr[0..len-1] across a warp cooperatively.\n// Each lane scans a strided subset of the array (indices lane, lane+32, lane+64, ...), then a warp reduction finds the maximum.\n// The results (max value and index) are broadcast to all lanes of the warp via shuffles.\n__device__ __forceinline__\nvoid warp_find_max_with_index(const float* __restrict__ arr, int len, unsigned lane_id, float &out_max_val, int &out_max_idx)\n{\n    float local_max = -CUDART_INF_F;\n    int local_idx = -1;\n\n    // Strided scan across the per-warp array in shared memory\n    for (int i = lane_id; i < len; i += WARP_SIZE)\n    {\n        float v = arr[i];\n        if (v > local_max)\n        {\n            local_max = v;\n            local_idx = i;\n        }\n    }\n\n    // Warp-wide reduction to find global max and its index\n    // We reduce with shfl_down; after reduction, lane 0 has the final result.\n    for (int offset = WARP_SIZE >> 1; offset > 0; offset >>= 1)\n    {\n        float other_val = __shfl_down_sync(FULL_MASK, local_max, offset);\n        int other_idx   = __shfl_down_sync(FULL_MASK, local_idx, offset);\n        if (other_val > local_max)\n        {\n            local_max = other_val;\n            local_idx = other_idx;\n        }\n    }\n\n    // Broadcast to all lanes\n    out_max_val = __shfl_sync(FULL_MASK, local_max, 0);\n    out_max_idx = __shfl_sync(FULL_MASK, local_idx, 0);\n}\n\n// CUDA kernel implementing k-NN for 2D points with squared Euclidean distances.\n__global__ void knn_kernel_2d_warp(const float2* __restrict__ query,\n                                   int query_count,\n                                   const float2* __restrict__ data,\n                                   int data_count,\n                                   std::pair<int, float>* __restrict__ result,\n                                   int k)\n{\n    // Thread, warp, and block configuration\n    const unsigned tid       = threadIdx.x;\n    const unsigned lane_id   = tid & (WARP_SIZE - 1);\n    const unsigned warp_id   = tid >> 5; // warp index within the block\n    const unsigned warps_per_block = blockDim.x >> 5;\n\n    // The query index for this warp\n    const int qidx = blockIdx.x * warps_per_block + warp_id;\n    const bool has_query = (qidx < query_count);\n\n    // Dynamic shared memory layout:\n    // [0 .. TILE_POINTS-1]                -> float2 tile points\n    // [next .. next + warps*k - 1]        -> float distances (per-warp top-k buffers)\n    // [next .. next + warps*k - 1]        -> int indices (per-warp top-k buffers)\n    extern __shared__ unsigned char s_mem[];\n    unsigned char* smem_ptr = s_mem;\n\n    // Shared tile of data points\n    float2* s_tile = reinterpret_cast<float2*>(smem_ptr);\n    smem_ptr += TILE_POINTS * sizeof(float2);\n\n    // Per-warp top-k buffers in shared memory\n    float* s_best_d = reinterpret_cast<float*>(smem_ptr);\n    smem_ptr += (warps_per_block * k) * sizeof(float);\n\n    int* s_best_i = reinterpret_cast<int*>(smem_ptr);\n    // smem_ptr += (warps_per_block * k) * sizeof(int); // not needed further\n\n    // Pointers to this warp's private top-k buffers\n    float* warp_best_d = s_best_d + warp_id * k;\n    int*   warp_best_i = s_best_i + warp_id * k;\n\n    // Initialize this warp's top-k buffers\n    int count = 0; // number of elements currently in the top-k buffer (initially empty)\n    if (has_query)\n    {\n        for (int i = lane_id; i < k; i += WARP_SIZE)\n        {\n            warp_best_d[i] = CUDART_INF_F;\n            warp_best_i[i] = -1;\n        }\n        __syncwarp();\n    }\n\n    // Load the query point into registers, one lane loads and broadcasts\n    float qx = 0.0f, qy = 0.0f;\n    if (lane_id == 0 && has_query)\n    {\n        float2 q = query[qidx];\n        qx = q.x; qy = q.y;\n    }\n    qx = __shfl_sync(FULL_MASK, qx, 0);\n    qy = __shfl_sync(FULL_MASK, qy, 0);\n\n    // Iterate over data in tiles\n    for (int tile_start = 0; tile_start < data_count; tile_start += TILE_POINTS)\n    {\n        const int tile_count = min(TILE_POINTS, data_count - tile_start);\n\n        // Block-wide cooperative load of the tile into shared memory\n        for (int t = tid; t < tile_count; t += blockDim.x)\n        {\n            s_tile[t] = data[tile_start + t];\n        }\n        __syncthreads();\n\n        if (has_query)\n        {\n            // Process the tile in batches of 32 points (one warp batch)\n            for (int base = 0; base < tile_count; base += WARP_SIZE)\n            {\n                const int j = base + lane_id;\n                const bool valid = (j < tile_count);\n\n                // Compute candidate distance for this lane (or INF if invalid)\n                float cand_d = CUDART_INF_F;\n                int   cand_i = -1;\n                if (valid)\n                {\n                    float2 p = s_tile[j];\n                    float dx = p.x - qx;\n                    float dy = p.y - qy;\n                    // Squared Euclidean distance\n                    cand_d = __fmaf_rn(dy, dy, dx * dx);\n                    cand_i = tile_start + j; // global index\n                }\n\n                // Sort the 32 lane-local candidates ascending to process the best first.\n                // Invalid lanes (cand_d = INF) move to the end.\n                float sort_key = cand_d;\n                int   sort_val = cand_i;\n                warp_bitonic_sort_asc_32(sort_key, sort_val, lane_id);\n\n                // After sorting: lane 't' holds the t-th smallest candidate among this batch.\n                // Fill phase: append as many as needed to reach k.\n                int valid_count = __popc(__ballot_sync(FULL_MASK, sort_key < CUDART_INF_F));\n                int remaining = k - count;\n                int to_fill = remaining > 0 ? min(remaining, valid_count) : 0;\n\n                // Write the first 'to_fill' candidates into the top-k buffer at positions [count, count+to_fill)\n                if (to_fill > 0)\n                {\n                    if ((int)lane_id < to_fill)\n                    {\n                        warp_best_d[count + lane_id] = sort_key;\n                        warp_best_i[count + lane_id] = sort_val;\n                    }\n                    __syncwarp();\n                    if (lane_id == 0) count += to_fill;\n                    count = __shfl_sync(FULL_MASK, count, 0);\n                }\n\n                // Replacement phase: for remaining candidates in ascending order, try to insert if better than current worst.\n                if (count == k && valid_count > to_fill)\n                {\n                    // Compute current tau (maximum distance in the buffer) and its index cooperatively\n                    float tau;\n                    int tau_idx;\n                    warp_find_max_with_index(warp_best_d, k, lane_id, tau, tau_idx);\n\n                    // Iterate over remaining sorted candidates (from best to worst)\n                    for (int t = to_fill; t < valid_count; ++t)\n                    {\n                        // Broadcast the t-th candidate from lane 't' to all lanes\n                        float c_d = __shfl_sync(FULL_MASK, sort_key, t);\n                        int   c_i = __shfl_sync(FULL_MASK, sort_val, t);\n\n                        // Early exit if candidate is not better than current worst\n                        if (!(c_d < tau))\n                            break;\n\n                        // Replace the current worst with this candidate\n                        if (lane_id == 0)\n                        {\n                            warp_best_d[tau_idx] = c_d;\n                            warp_best_i[tau_idx] = c_i;\n                        }\n                        __syncwarp();\n\n                        // Recompute tau for the next insertion attempt\n                        warp_find_max_with_index(warp_best_d, k, lane_id, tau, tau_idx);\n                    }\n                }\n            } // end for base\n        } // end if has_query\n\n        __syncthreads(); // Ensure all warps are done before loading the next tile\n    } // end tiles loop\n\n    // Finalize: sort the top-k buffer for this query ascending by distance and write to output\n    if (has_query)\n    {\n        // In-place bitonic sort across k elements in this warp's private shared memory buffers.\n        // All 32 threads in the warp cooperate, each handling a strided subset of indices.\n        for (int size = 2; size <= k; size <<= 1)\n        {\n            for (int stride = size >> 1; stride > 0; stride >>= 1)\n            {\n                for (int i = lane_id; i < k; i += WARP_SIZE)\n                {\n                    int j = i ^ stride;\n                    if (j > i)\n                    {\n                        bool up = ((i & size) == 0);\n                        float ai = warp_best_d[i];\n                        float aj = warp_best_d[j];\n                        int bi = warp_best_i[i];\n                        int bj = warp_best_i[j];\n                        bool do_swap = (ai > aj) == up;\n                        if (do_swap)\n                        {\n                            warp_best_d[i] = aj;\n                            warp_best_d[j] = ai;\n                            warp_best_i[i] = bj;\n                            warp_best_i[j] = bi;\n                        }\n                    }\n                }\n                __syncwarp();\n            }\n        }\n\n        // Write out the sorted results for this query\n        const int out_base = qidx * k;\n        for (int i = lane_id; i < k; i += WARP_SIZE)\n        {\n            result[out_base + i].first  = warp_best_i[i];\n            result[out_base + i].second = warp_best_d[i]; // squared Euclidean distances\n        }\n    }\n}\n\n// Host-side interface\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k)\n{\n    // Launch configuration\n    const int warps_per_block = WARPS_PER_BLOCK;\n    const int threads_per_block = warps_per_block * WARP_SIZE;\n\n    // Grid dimension: one warp per query\n    const int grid_blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Dynamic shared memory size:\n    // - TILE_POINTS * sizeof(float2) for the data tile\n    // - warps_per_block * k * (sizeof(float) + sizeof(int)) for per-warp top-k buffers\n    size_t smem_bytes = static_cast<size_t>(TILE_POINTS) * sizeof(float2)\n                      + static_cast<size_t>(warps_per_block) * static_cast<size_t>(k) * (sizeof(float) + sizeof(int));\n\n    // Launch the kernel\n    knn_kernel_2d_warp<<<grid_blocks, threads_per_block, smem_bytes>>>(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 706, "completion_tokens": 18798, "total_tokens": 19504, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 15680, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db7f4508190a21859e3f5660f1d", "custom_id": "knn05-01", "response": {"status_code": 200, "request_id": "b67ee65e3ad2a9202bcab63b270ee4d1", "body": {"id": "chatcmpl-C4sgImMkStzEcgqsTGWNdqkemduvO", "object": "chat.completion", "created": 1755279542, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#include <utility>\n#include <limits>\n#include <algorithm>\n\n// This implementation computes k-NN for 2D points with one warp (32 threads) per query.\n// It uses shared memory tiling for data points and per-warp shared memory buffers:\n// - best[k]: the current k nearest neighbors (sorted ascending by distance)\n// - cand[k]: a buffer of candidates collected from the current/previous tiles\n// - out[k]:  a temporary buffer used during merging\n// The candidate buffer is filled until it reaches k entries; then it is sorted (bitonic sort)\n// and merged with the current best using a parallel merge-path algorithm across the warp.\n//\n// Design and optimization notes:\n// - Each warp handles one query and uses __ballot_sync and __shfl_sync to coordinate writes\n//   into its candidate buffer without atomics.\n// - Tiling of the data points into shared memory provides cache locality and reduces global\n//   memory traffic. The entire block cooperatively loads tiles; then each warp traverses the\n//   tile for its query.\n// - Both \"best\" and \"cand\" are kept in shared memory. \"best\" is always sorted. \"cand\" is sorted\n//   only when flushed (full, or final flush). Merging two sorted arrays leverages \"merge-path\"\n//   partitioning across warp lanes to parallelize merging, keeping only the first k results.\n// - Distances use squared L2 norm. Ties are resolved arbitrarily.\n// - k is assumed to be a power of two in [32, 1024], inclusive.\n// - The kernel uses dynamically allocated shared memory; run_knn() computes the appropriate\n//   shared memory size and sets the corresponding kernel attribute on supported hardware\n//   (e.g., A100/H100) to allow up to ~164KB or more of dynamic shared memory per block.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Choose number of warps per block (tuneable). Using 4 warps/block is a good trade-off\n// between shared memory capacity (for larger k) and occupancy on A100/H100.\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 4\n#endif\n\n// Pair container used internally for (distance, index).\nstruct Candidate {\n    float dist;\n    int   index;\n};\nstatic_assert(sizeof(Candidate) == 8, \"Candidate must be 8 bytes\");\n\n// Utility: squared Euclidean distance between two float2 points.\n__forceinline__ __device__ float squared_distance(const float2& a, const float2& b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    // FMA for potential precision/perf improvement\n    return fmaf(dx, dx, dy * dy);\n}\n\n// Warp-scope bitonic sort on shared-memory array 'arr' of length 'n' (power of two).\n// Sorts ascending by Candidate.dist. Uses only warp-scope sync (no __syncthreads).\n__forceinline__ __device__ void warp_bitonic_sort(Candidate* arr, int n) {\n    const unsigned full_mask = 0xFFFFFFFFu;\n    // Bitonic sort network: O(n log^2 n).\n    for (int size = 2; size <= n; size <<= 1) {\n        // The 'size' stage sets the sort direction for subsequences.\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            // Each thread processes multiple indices spaced by warp size.\n            for (int i = threadIdx.x & (WARP_SIZE - 1); i < n; i += WARP_SIZE) {\n                int j = i ^ stride;\n                if (j > i) {\n                    bool ascending = ((i & size) == 0);\n                    Candidate a = arr[i];\n                    Candidate b = arr[j];\n                    // Compare-exchange for ascending/descending order.\n                    bool swapNeeded = ascending ? (a.dist > b.dist) : (a.dist < b.dist);\n                    if (swapNeeded) {\n                        arr[i] = b;\n                        arr[j] = a;\n                    }\n                }\n            }\n            __syncwarp(full_mask);\n        }\n    }\n}\n\n// Merge-path binary search: find i in [max(0, diag - bCount), min(diag, aCount)]\n// such that A[i-1] <= B[diag-i] and B[diag-i-1] < A[i]. We compare by .dist.\n__forceinline__ __device__ int merge_path_search(const Candidate* A, int aCount,\n                                                 const Candidate* B, int bCount,\n                                                 int diag) {\n    int low  = max(0, diag - bCount);\n    int high = min(diag, aCount);\n    while (low < high) {\n        int mid = (low + high) >> 1;\n        int i = mid;\n        int j = diag - i;\n\n        float a_i     = (i < aCount) ? A[i].dist : CUDART_INF_F;\n        float b_jm1   = (j > 0)      ? B[j - 1].dist : -CUDART_INF_F;\n\n        if (a_i < b_jm1) {\n            low = mid + 1;\n        } else {\n            high = mid;\n        }\n    }\n    return low;\n}\n\n// Parallel merge of two sorted arrays A[0..k-1], B[0..k-1] into out[0..k-1] (top-k only).\n// Each warp lane is assigned a contiguous output range [d0, d1).\n__forceinline__ __device__ void warp_merge_topk(const Candidate* A, const Candidate* B,\n                                                Candidate* out, int k) {\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n\n    // Partition the first k outputs among warp lanes.\n    int chunk = (k + WARP_SIZE - 1) / WARP_SIZE; // ceil(k / 32)\n    int d0 = min(lane * chunk, k);\n    int d1 = min(d0 + chunk, k);\n\n    int i0 = merge_path_search(A, k, B, k, d0);\n    int j0 = d0 - i0;\n\n    int i1 = merge_path_search(A, k, B, k, d1);\n    int j1 = d1 - i1;\n\n    int i = i0;\n    int j = j0;\n\n    for (int out_pos = d0; out_pos < d1; ++out_pos) {\n        float va = (i < k) ? A[i].dist : CUDART_INF_F;\n        float vb = (j < k) ? B[j].dist : CUDART_INF_F;\n        bool takeA = (va <= vb);\n        if (takeA) {\n            out[out_pos] = A[i++];\n        } else {\n            out[out_pos] = B[j++];\n        }\n    }\n}\n\n// Flush candidate buffer by sorting and merging into 'best'.\n// Requires: 'best' is sorted ascending and has length k.\n// - If candCount == k: sort cand[0..k-1], merge with best, write result to out, then copy out->best.\n// - If candCount  < k: pad cand[candCount..k-1] with +inf, then sort, merge, etc.\n// After flushing, candCount is set to 0, and kthDist is updated to best[k-1].dist.\n__forceinline__ __device__ void warp_flush_candidates(Candidate* best,\n                                                      Candidate* cand,\n                                                      Candidate* out,\n                                                      int k,\n                                                      int& candCount,\n                                                      float& kthDist) {\n    const unsigned full_mask = 0xFFFFFFFFu;\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n\n    // Pad with +inf if not full.\n    int cc = __shfl_sync(full_mask, candCount, 0);\n    if (cc < k) {\n        for (int i = lane + cc; i < k; i += WARP_SIZE) {\n            cand[i].dist = CUDART_INF_F;\n            cand[i].index = -1;\n        }\n        __syncwarp(full_mask);\n    }\n\n    // Sort candidate buffer and merge with current best.\n    warp_bitonic_sort(cand, k);\n    __syncwarp(full_mask);\n\n    warp_merge_topk(best, cand, out, k);\n    __syncwarp(full_mask);\n\n    // Copy merged top-k back to best.\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        best[i] = out[i];\n    }\n    __syncwarp(full_mask);\n\n    if (lane == 0) {\n        candCount = 0;\n        kthDist = best[k - 1].dist;\n    }\n    __syncwarp(full_mask);\n}\n\n// Kernel: each warp handles one query. The block cooperatively loads data tiles into shared memory.\n__global__ void knn_kernel(const float2* __restrict__ query,\n                           int query_count,\n                           const float2* __restrict__ data,\n                           int data_count,\n                           std::pair<int, float>* __restrict__ result,\n                           int k,\n                           int tile_capacity_points) {\n    extern __shared__ unsigned char shared_bytes[];\n\n    // Layout of shared memory:\n    // [ float2 tile[tile_capacity_points] ][ Candidate best[WARPS_PER_BLOCK * k] ]\n    // [ Candidate cand[WARPS_PER_BLOCK * k] ][ Candidate out[WARPS_PER_BLOCK * k] ]\n    float2* tile = reinterpret_cast<float2*>(shared_bytes);\n    size_t tile_bytes = static_cast<size_t>(tile_capacity_points) * sizeof(float2);\n    Candidate* smem_candidates = reinterpret_cast<Candidate*>(shared_bytes + tile_bytes);\n\n    const int warps_per_block = blockDim.x / WARP_SIZE;\n    const int warp_id_in_block = threadIdx.x / WARP_SIZE;\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n    const unsigned full_mask = 0xFFFFFFFFu;\n\n    Candidate* best_base = smem_candidates;                                   // size: warps_per_block * k\n    Candidate* cand_base = best_base + warps_per_block * k;                   // size: warps_per_block * k\n    Candidate* out_base  = cand_base + warps_per_block * k;                   // size: warps_per_block * k\n\n    Candidate* best = best_base + warp_id_in_block * k;\n    Candidate* cand = cand_base + warp_id_in_block * k;\n    Candidate* out  = out_base  + warp_id_in_block * k;\n\n    const int warp_global_id = blockIdx.x * warps_per_block + warp_id_in_block;\n    const bool warp_active = (warp_global_id < query_count);\n\n    // Initialize best with +inf distances so it's a valid sorted list.\n    if (warp_active) {\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            best[i].dist = CUDART_INF_F;\n            best[i].index = -1;\n        }\n    }\n    __syncwarp(full_mask);\n\n    float2 q = make_float2(0.f, 0.f);\n    if (warp_active && lane == 0) {\n        q = query[warp_global_id];\n    }\n    // Broadcast query components to all lanes of the warp.\n    if (warp_active) {\n        q.x = __shfl_sync(full_mask, q.x, 0);\n        q.y = __shfl_sync(full_mask, q.y, 0);\n    }\n\n    int candCount = 0;\n    float kthDist = CUDART_INF_F;\n\n    // Process the dataset in tiles.\n    for (int tile_start = 0; tile_start < data_count; tile_start += tile_capacity_points) {\n        int tile_count = min(tile_capacity_points, data_count - tile_start);\n\n        // Block-wide cooperative load of the current tile into shared memory.\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            tile[i] = data[tile_start + i];\n        }\n        __syncthreads();\n\n        if (warp_active) {\n            // Local snapshot of kthDist for gating candidates between flushes.\n            float local_kth = kthDist;\n\n            // Each warp lane iterates over a strided subset of the tile.\n            for (int i = lane; i < tile_count; i += WARP_SIZE) {\n                float2 p = tile[i];\n                int    idx = tile_start + i;\n                float  dist = squared_distance(q, p);\n\n                // Accept only if strictly better than current k-th.\n                bool valid = (dist < local_kth);\n\n                // Warp-scope packing loop: insert as many as possible into cand[],\n                // flushing when full and re-evaluating against the updated threshold.\n                while (true) {\n                    unsigned mask = __ballot_sync(full_mask, valid);\n                    if (mask == 0u) break; // no more lanes to insert this round\n\n                    int cc = __shfl_sync(full_mask, candCount, 0);\n                    int free_slots = k - cc;\n\n                    if (free_slots == 0) {\n                        // Flush: sort cand[0..k-1], merge with best, update kthDist and reset candCount.\n                        warp_flush_candidates(best, cand, out, k, candCount, kthDist);\n                        // Broadcast updated threshold and re-evaluate validity.\n                        local_kth = __shfl_sync(full_mask, kthDist, 0);\n                        valid = (dist < local_kth);\n                        continue;\n                    }\n\n                    int accepted = __popc(mask);\n                    int take = (accepted < free_slots) ? accepted : free_slots;\n\n                    int base = cc;\n                    if (lane == 0) {\n                        candCount = cc + take;\n                    }\n\n                    int rank = __popc(mask & ((1u << lane) - 1));\n                    bool do_write = valid && (rank < take);\n                    if (do_write) {\n                        int pos = base + rank;\n                        cand[pos].dist = dist;\n                        cand[pos].index = idx;\n                    }\n\n                    // If wrote, clear valid; otherwise remain valid for next iteration (after a flush).\n                    valid = valid && !(do_write);\n                }\n            } // end per-tile work\n        }\n\n        __syncthreads();\n    } // end tiles loop\n\n    // Final flush if candidate buffer is non-empty.\n    if (warp_active) {\n        int cc = __shfl_sync(full_mask, candCount, 0);\n        if (cc > 0) {\n            warp_flush_candidates(best, cand, out, k, candCount, kthDist);\n        }\n\n        // Write the result for this query.\n        size_t out_base_idx = static_cast<size_t>(warp_global_id) * static_cast<size_t>(k);\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            int idx = best[i].index;\n            float dist = best[i].dist;\n            result[out_base_idx + i].first = idx;\n            result[out_base_idx + i].second = dist;\n        }\n    }\n}\n\n// Host function to run the k-NN kernel. Computes dynamic shared memory size\n// based on k and the chosen number of warps per block, opts into larger shared\n// memory if available, and launches the kernel.\nvoid run_knn(const float2* query, int query_count,\n             const float2* data, int data_count,\n             std::pair<int, float>* result, int k) {\n    // Choose launch parameters.\n    const int warps_per_block = WARPS_PER_BLOCK;\n    const int threads_per_block = warps_per_block * WARP_SIZE;\n\n    // Shared memory per warp: three k-sized arrays of Candidate (best, cand, out).\n    const size_t per_warp_bytes = static_cast<size_t>(3) * static_cast<size_t>(k) * sizeof(Candidate);\n    const size_t per_block_arrays_bytes = per_warp_bytes * static_cast<size_t>(warps_per_block);\n\n    // Query device shared memory capability.\n    int device = 0;\n    cudaGetDevice(&device);\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&prop, device);\n\n    // Use the opt-in maximum dynamic shared memory size if available (A100/H100).\n    size_t max_dynamic_smem = (prop.sharedMemPerBlockOptin > 0) ? prop.sharedMemPerBlockOptin\n                                                                : prop.sharedMemPerBlock;\n\n    // Compute how many float2 points fit in the remaining shared memory for the tile.\n    size_t tile_bytes_budget = (per_block_arrays_bytes < max_dynamic_smem) ?\n                               (max_dynamic_smem - per_block_arrays_bytes) : 0;\n    int tile_capacity_points = static_cast<int>(tile_bytes_budget / sizeof(float2));\n    if (tile_capacity_points < 1) tile_capacity_points = 1;\n\n    // Total dynamic shared memory per block to request.\n    size_t dynamic_smem_bytes = per_block_arrays_bytes +\n                                static_cast<size_t>(tile_capacity_points) * sizeof(float2);\n\n    // Opt-in to the required dynamic shared memory size (may increase per-block limit).\n    cudaFuncSetAttribute(knn_kernel,\n                         cudaFuncAttributeMaxDynamicSharedMemorySize,\n                         static_cast<int>(dynamic_smem_bytes));\n\n    // Configure grid.\n    int num_blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Launch kernel.\n    knn_kernel<<<num_blocks, threads_per_block, dynamic_smem_bytes>>>(query, query_count,\n                                                                      data, data_count,\n                                                                      result, k,\n                                                                      tile_capacity_points);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 766, "completion_tokens": 23096, "total_tokens": 23862, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 19328, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db7e8608190aed83b72f038a6b0", "custom_id": "knn05-02", "response": {"status_code": 200, "request_id": "d27251843386e2b1db18b8f96e8a79ab", "body": {"id": "chatcmpl-C4sgIjBxOTw4ZYlbWAN32R8nfBzNL", "object": "chat.completion", "created": 1755279542, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <math_constants.h>\n#include <utility>\n\n// This implementation assigns exactly one warp (32 threads) to process one query point.\n// The algorithm proceeds in batches (tiles) of data points that are cached in shared memory.\n// Each warp maintains:\n//   - An \"intermediate result\" of top-k neighbors in registers, distributed across lanes.\n//   - A shared-memory per-warp candidate buffer for k candidates.\n// Whenever the candidate buffer fills up, the warp merges its candidates with the intermediate result.\n// The merge is implemented as a parallel bitonic sort over 2k elements (k old + k new), using the warp cooperatively.\n// At the end, any remaining candidates are merged, and the final top-k (sorted by distance asc) is written to the result.\n//\n// Notes:\n//   - k is assumed to be a power of two between 32 and 1024, inclusive.\n//   - data_count >= k.\n//   - Input arrays (query, data) and the output array (result) are device allocations.\n//   - The kernel allocates no additional device memory, only dynamic shared memory.\n//   - We set a large dynamic shared memory carveout on the kernel launch to accommodate per-warp buffers and cache tiles.\n\n#ifndef KNN_WARPS_PER_BLOCK\n#define KNN_WARPS_PER_BLOCK 4  // 4 warps per block => 128 threads per block; balanced for H100/A100 shared memory limits.\n#endif\n\n// Pair structure used in shared memory (distance as key, index as value).\n// Packed into 8 bytes for efficient shared-memory bandwidth.\nstruct __align__(8) PairDF {\n    float dist;\n    int   idx;\n};\n\n// Utility: warp-wide bitonic sort on PairDF array in shared memory.\n// The array length n must be a power-of-two. We sort ascending by 'dist'.\n// Threads of a single warp cooperatively process the sorting network by striding over indices.\n__device__ __forceinline__ void warp_bitonic_sort(PairDF* arr, int n) {\n    const unsigned full_mask = 0xFFFFFFFFu;\n    const int lane = threadIdx.x & 31;\n\n    // Standard bitonic sort network\n    for (int size = 2; size <= n; size <<= 1) {\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            // Each lane processes multiple indices spaced by warpSize\n            for (int i = lane; i < n; i += warpSize) {\n                int j = i ^ stride;\n                if (j > i) {\n                    bool up = ((i & size) == 0);\n                    PairDF a = arr[i];\n                    PairDF b = arr[j];\n                    // Compare-exchange with direction 'up'\n                    bool swap = (a.dist > b.dist);\n                    if (!up) swap = !swap; // for descending, swap when a < b\n                    if (swap) {\n                        arr[i] = b;\n                        arr[j] = a;\n                    }\n                }\n            }\n            __syncwarp(full_mask);\n        }\n    }\n}\n\n// Merge step: combines current top-k (in registers) and k candidates (in shared memory) into new top-k.\n// Implementation:\n//   - Copy top-k from registers into shared union buffer positions [0, k).\n//   - The candidates are already in union buffer positions [k, 2k).\n//   - Perform a bitonic sort on the 2k union.\n//   - Copy the first k items (best k) back into registers, preserving lane-striping order.\ntemplate <int WARPS_PER_BLOCK_T>\n__device__ __forceinline__ void warp_merge_fullsort(\n    PairDF* union_base_all, int k,\n    float* topkDist, int* topkIdx, int slots_per_lane)\n{\n    const unsigned full_mask = 0xFFFFFFFFu;\n    const int lane = threadIdx.x & 31;\n    const int warp_in_block = threadIdx.x >> 5;\n\n    // This warp's union base: [0, k) holds current top-k, [k, 2k) holds candidates.\n    PairDF* union_base = union_base_all + warp_in_block * (2 * k);\n\n    // Write current top-k from registers into shared union buffer [0, k)\n    for (int t = 0; t < slots_per_lane; ++t) {\n        int pos = lane + t * warpSize; // lane-striped positions\n        union_base[pos].dist = topkDist[t];\n        union_base[pos].idx  = topkIdx[t];\n    }\n    __syncwarp(full_mask);\n\n    // Sort all 2k items in ascending order by distance\n    warp_bitonic_sort(union_base, 2 * k);\n\n    // Copy the best k back to registers, preserving lane-striped order\n    for (int t = 0; t < slots_per_lane; ++t) {\n        int pos = lane + t * warpSize;\n        topkDist[t] = union_base[pos].dist;\n        topkIdx[t]  = union_base[pos].idx;\n    }\n    __syncwarp(full_mask);\n}\n\n// Kernel: one warp per query. Each block has WARPS_PER_BLOCK warps.\n// Dynamic shared memory layout (per block):\n//   - float2 tile[TilePointsCapacity]\n//   - PairDF union_mem[WARPS_PER_BLOCK * (2*k)]  // per-warp union area; second half is the candidate buffer.\n// We pass TilePointsCapacity as a parameter chosen at launch time.\ntemplate <int WARPS_PER_BLOCK_T>\n__global__ void knn2d_warp_kernel(\n    const float2* __restrict__ query,\n    int query_count,\n    const float2* __restrict__ data,\n    int data_count,\n    std::pair<int, float>* __restrict__ result,\n    int k,\n    int tile_points_capacity)\n{\n    const unsigned full_mask = 0xFFFFFFFFu;\n    const int lane = threadIdx.x & 31;\n    const int warp_in_block = threadIdx.x >> 5;\n    const int warp_global = blockIdx.x * WARPS_PER_BLOCK_T + warp_in_block;\n    const bool active = (warp_global < query_count);\n\n    // Shared memory layout setup\n    extern __shared__ unsigned char smem_raw[];\n    // Cache of data points for the block\n    float2* sh_tile = reinterpret_cast<float2*>(smem_raw);\n    // Union memory for all warps (2k per warp); each warp uses [0,k) for current top-k snapshot and [k,2k) for candidates.\n    PairDF* sh_union = reinterpret_cast<PairDF*>(sh_tile + tile_points_capacity);\n\n    // Per-warp pointers into shared memory\n    PairDF* warp_union = sh_union + warp_in_block * (2 * k);\n    PairDF* warp_cand  = warp_union + k; // candidate buffer region [k, 2k)\n\n    // Per-warp in-register top-k storage (distributed across lanes)\n    const int slots_per_lane = k >> 5; // k / 32; k is power of 2 >= 32, so divisible by 32\n    float topkDist[slots_per_lane];\n    int   topkIdx[slots_per_lane];\n\n    // Initialize top-k to +inf and invalid index\n    #pragma unroll\n    for (int t = 0; t < slots_per_lane; ++t) {\n        topkDist[t] = CUDART_INF_F;\n        topkIdx[t]  = -1;\n    }\n\n    // Load this warp's query point and broadcast to all lanes\n    float qx = 0.0f, qy = 0.0f;\n    if (active) {\n        if (lane == 0) {\n            float2 q = query[warp_global];\n            qx = q.x;\n            qy = q.y;\n        }\n        qx = __shfl_sync(full_mask, qx, 0);\n        qy = __shfl_sync(full_mask, qy, 0);\n    }\n\n    // Threshold for pruning: current k-th (worst) distance in the intermediate result.\n    // Starts at +inf (no pruning), updated after each merge.\n    float worstDist = CUDART_INF_F;\n\n    // Per-warp candidate buffer count tracked by lane 0 and broadcast when needed.\n    int candCount = 0;\n\n    // Process the data set in tiles cached by the whole block\n    for (int base = 0; base < data_count; base += tile_points_capacity) {\n        int tileN = data_count - base;\n        if (tileN > tile_points_capacity) tileN = tile_points_capacity;\n\n        // Load tile into shared memory cooperatively across the block\n        for (int i = threadIdx.x; i < tileN; i += blockDim.x) {\n            sh_tile[i] = data[base + i];\n        }\n        __syncthreads();\n\n        if (active) {\n            // Each warp computes distances from its query to all points in the tile.\n            // Lanes stride across the tile to evaluate different points in parallel.\n            for (int tIdx = lane; tIdx < tileN; tIdx += warpSize) {\n                float2 p = sh_tile[tIdx];\n                float dx = p.x - qx;\n                float dy = p.y - qy;\n                float dist = fmaf(dx, dx, dy * dy); // dx*dx + dy*dy\n\n                // Apply pruning based on current worstDist\n                bool isCandidate = (dist < worstDist);\n                unsigned mask = __ballot_sync(full_mask, isCandidate);\n                int pending = __popc(mask);\n\n                // Insert the current batch of candidate lanes into the warp's shared buffer,\n                // handling possible buffer-full condition and performing merges as needed.\n                // We may need at most two iterations for a 32-lane batch:\n                //   - fill remaining space, possibly trigger merge\n                //   - insert leftover lanes after merge\n                while (pending > 0) {\n                    int baseOff = 0;\n                    int take = 0;\n                    if (lane == 0) {\n                        int space = k - candCount;\n                        take = (pending < space) ? pending : space;\n                        baseOff = candCount;\n                        candCount += take;\n                    }\n                    baseOff = __shfl_sync(full_mask, baseOff, 0);\n                    take    = __shfl_sync(full_mask, take,    0);\n\n                    // Compute rank of this lane within the current 'pending' mask\n                    unsigned laneMaskLt = (1u << lane) - 1u;\n                    int rank = __popc(mask & laneMaskLt);\n\n                    // Write selected candidates into shared buffer\n                    if (isCandidate && rank < take) {\n                        int slot = baseOff + rank;\n                        warp_cand[slot].dist = dist;\n                        warp_cand[slot].idx  = base + tIdx;\n                    }\n                    __syncwarp(full_mask);\n\n                    // If buffer filled, merge with intermediate top-k\n                    bool doMerge = false;\n                    if (lane == 0) doMerge = (candCount == k);\n                    doMerge = __shfl_sync(full_mask, doMerge ? 1 : 0, 0);\n\n                    if (doMerge) {\n                        // No need to fill candidates (is exactly k)\n                        // Merge: place current top-k into shared [0,k), candidates already in [k,2k)\n                        warp_merge_fullsort<WARPS_PER_BLOCK_T>(sh_union, k, topkDist, topkIdx, slots_per_lane);\n                        candCount = 0;\n                        // Update pruning threshold (k-th distance is in lane 31, last slot)\n                        float newWorst = (lane == 31) ? topkDist[slots_per_lane - 1] : 0.0f;\n                        worstDist = __shfl_sync(full_mask, newWorst, 31);\n                    }\n\n                    // Remove the first 'take' lanes from the pending set\n                    bool stillPending = (isCandidate && (rank >= take));\n                    mask = __ballot_sync(full_mask, stillPending);\n                    pending = __popc(mask);\n                    isCandidate = stillPending;\n                }\n            }\n        }\n\n        __syncthreads(); // Make sure all warps are done before overwriting the tile\n    }\n\n    if (active) {\n        // After the last tile, if the candidate buffer is non-empty, fill the rest with +inf and merge.\n        if (candCount > 0) {\n            for (int pos = lane + candCount; pos < k; pos += warpSize) {\n                warp_cand[pos].dist = CUDART_INF_F;\n                warp_cand[pos].idx  = -1;\n            }\n            __syncwarp(full_mask);\n            // Merge current top-k with padded candidates\n            warp_merge_fullsort<WARPS_PER_BLOCK_T>(sh_union, k, topkDist, topkIdx, slots_per_lane);\n            candCount = 0;\n            float newWorst = (lane == 31) ? topkDist[slots_per_lane - 1] : 0.0f;\n            worstDist = __shfl_sync(full_mask, newWorst, 31);\n        }\n\n        // Write final top-k for this query to global memory in ascending order by distance.\n        // We output to result[query_id * k + j], where each j is the lane-striped index over k.\n        int outBase = warp_global * k;\n        #pragma unroll\n        for (int t = 0; t < slots_per_lane; ++t) {\n            int j = lane + t * warpSize; // position within [0, k)\n            result[outBase + j].first  = topkIdx[t];\n            result[outBase + j].second = topkDist[t];\n        }\n    }\n}\n\n// Host API: orchestrates shared memory sizing and kernel launch.\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    // Choose execution configuration\n    constexpr int WARPS_PER_BLOCK = KNN_WARPS_PER_BLOCK;\n    const int block_threads = WARPS_PER_BLOCK * 32;\n    const int total_warps = query_count;\n    const int grid_blocks = (total_warps + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK;\n\n    // Inspect device shared memory capacity and set dynamic shared memory size accordingly.\n    int device = 0;\n    cudaGetDevice(&device);\n    cudaDeviceProp prop{};\n    cudaGetDeviceProperties(&prop, device);\n\n    // Maximum dynamic shared memory per block we can opt-in for.\n    size_t max_optin = (prop.sharedMemPerBlockOptin ? prop.sharedMemPerBlockOptin : prop.sharedMemPerBlock);\n\n    // Per-block shared memory usage components:\n    //   - Per-warp union buffer: 2*k PairDF per warp\n    //   - Cached tile of float2 points\n    const size_t per_warp_union_bytes = static_cast<size_t>(2) * static_cast<size_t>(k) * sizeof(PairDF);\n    const size_t per_block_union_bytes = static_cast<size_t>(WARPS_PER_BLOCK) * per_warp_union_bytes;\n\n    // Reserve space for the tile: use as much as available, but keep some headroom (none strictly needed).\n    size_t tile_bytes_available = (max_optin > per_block_union_bytes) ? (max_optin - per_block_union_bytes) : 0;\n    // Choose tile size (in number of points) as a multiple of block_threads for coalesced loading.\n    size_t tile_points_capacity = tile_bytes_available / sizeof(float2);\n    if (tile_points_capacity < static_cast<size_t>(block_threads)) {\n        // Fallback: ensure at least one warp's worth; but by problem constraints we should not hit this path.\n        tile_points_capacity = block_threads;\n    }\n    // Align capacity down to a multiple of block_threads\n    tile_points_capacity = (tile_points_capacity / block_threads) * block_threads;\n    if (tile_points_capacity == 0) {\n        tile_points_capacity = block_threads; // safety\n    }\n\n    // Compute dynamic shared memory size to pass at launch.\n    size_t dynamic_smem_bytes = per_block_union_bytes + tile_points_capacity * sizeof(float2);\n\n    // Increase the kernel's dynamic shared memory limit if necessary.\n    cudaFuncSetAttribute(\n        knn2d_warp_kernel<KNN_WARPS_PER_BLOCK>,\n        cudaFuncAttributeMaxDynamicSharedMemorySize,\n        static_cast<int>(dynamic_smem_bytes));\n\n    // Launch the kernel\n    knn2d_warp_kernel<KNN_WARPS_PER_BLOCK>\n        <<<grid_blocks, block_threads, dynamic_smem_bytes>>>(\n            query, query_count,\n            data, data_count,\n            result,\n            k,\n            static_cast<int>(tile_points_capacity));\n\n    // Optional: synchronize or check for errors (omitted to avoid extra host overhead).\n    // cudaDeviceSynchronize();\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 766, "completion_tokens": 21543, "total_tokens": 22309, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 17920, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db7f4d0819089477a35e343690e", "custom_id": "knn05-03", "response": {"status_code": 200, "request_id": "c60ad5a4b7289bb8deaeaa92a44e73d5", "body": {"id": "chatcmpl-C4sgIiIRuWBAe0whblDmZADYnDTtH", "object": "chat.completion", "created": 1755279542, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n#include <limits>\n\n// Optimized CUDA implementation of k-NN for 2D points.\n// Each query is processed by a single warp (32 threads).\n// The kernel processes the data points in batches cached in shared memory.\n// For each warp (query), we keep:\n//  - An intermediate result of top-k (indices + distances) distributed across the 32 lanes (registers).\n//  - A per-warp candidate buffer of size k stored in shared memory (indices + distances).\n// Candidates that are not closer than the current worst (k-th) distance in the intermediate result are skipped.\n// When the candidate buffer is full (or at the end), we merge it into the intermediate result cooperatively within the warp.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Utility: lane id (0..31)\nstatic __device__ __forceinline__ int lane_id() {\n    return threadIdx.x & (WARP_SIZE - 1);\n}\n\n// Utility: warp id within block\nstatic __device__ __forceinline__ int warp_id_in_block() {\n    return threadIdx.x >> 5;\n}\n\n// Utility: recompute local worst (maximum distance) in this lane's portion\nstatic __device__ __forceinline__ void recompute_local_worst(const float* top_d, int L, float& local_worst_val, int& local_worst_pos) {\n    float wv = top_d[0];\n    int wp = 0;\n    for (int i = 1; i < L; ++i) {\n        float v = top_d[i];\n        if (v > wv) {\n            wv = v;\n            wp = i;\n        }\n    }\n    local_worst_val = wv;\n    local_worst_pos = wp;\n}\n\n// Utility: compute global worst (max) across warp, returning worst value and the owner lane+pos\nstatic __device__ __forceinline__ void warp_argmax(float local_val, int local_pos, float& global_val, int& owner_lane, int& owner_pos, unsigned mask) {\n    int lane = lane_id();\n    float best_val = local_val;\n    int best_lane = lane;\n    int best_pos = local_pos;\n\n    // Warp reduction for max with arg\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        float oth_val = __shfl_down_sync(mask, best_val, offset);\n        int   oth_lane = __shfl_down_sync(mask, best_lane, offset);\n        int   oth_pos  = __shfl_down_sync(mask, best_pos,  offset);\n\n        if (oth_val > best_val) {\n            best_val = oth_val;\n            best_lane = oth_lane;\n            best_pos = oth_pos;\n        }\n    }\n    // Broadcast from lane 0\n    global_val = __shfl_sync(mask, best_val, 0);\n    owner_lane = __shfl_sync(mask, best_lane, 0);\n    owner_pos  = __shfl_sync(mask, best_pos,  0);\n}\n\n// Utility: compute global min (for final ordering) across warp, returning min value and owner lane+pos\nstatic __device__ __forceinline__ void warp_argmin(float local_val, int local_pos, float& global_val, int& owner_lane, int& owner_pos, unsigned mask) {\n    int lane = lane_id();\n    float best_val = local_val;\n    int best_lane = lane;\n    int best_pos = local_pos;\n\n    // Warp reduction for min with arg\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        float oth_val = __shfl_down_sync(mask, best_val, offset);\n        int   oth_lane = __shfl_down_sync(mask, best_lane, offset);\n        int   oth_pos  = __shfl_down_sync(mask, best_pos,  offset);\n\n        if (oth_val < best_val) {\n            best_val = oth_val;\n            best_lane = oth_lane;\n            best_pos = oth_pos;\n        }\n    }\n    // Broadcast from lane 0\n    global_val = __shfl_sync(mask, best_val, 0);\n    owner_lane = __shfl_sync(mask, best_lane, 0);\n    owner_pos  = __shfl_sync(mask, best_pos,  0);\n}\n\n// Merge the per-warp candidate buffer into the warp's distributed top-k.\n// - cand_dists, cand_indices: per-warp candidate arrays in shared memory (size >= count).\n// - count: number of candidates currently in buffer (in shared memory counter).\n// - top_*: per-lane portion of top-k arrays in registers (size L).\n// - local_worst_*: per-lane cached worst info (max of local portion).\n// - worst_threshold: updated to new global worst (k-th distance) after merge.\n// All threads in the warp call this function cooperatively.\nstatic __device__ __forceinline__ void merge_candidates_into_topk(\n    float* cand_dists_base,\n    int*   cand_indices_base,\n    volatile int* cand_count_ptr,\n    float* top_dists, int* top_indices, int L,\n    float& local_worst_val, int& local_worst_pos,\n    float& worst_threshold,\n    unsigned mask)\n{\n    int count = 0;\n    // Lane 0 reads and resets the candidate count\n    if (lane_id() == 0) {\n        count = *cand_count_ptr;\n        *cand_count_ptr = 0;\n    }\n    // Broadcast count to all lanes\n    count = __shfl_sync(mask, count, 0);\n\n    // For each candidate, attempt to insert if it improves the current top-k.\n    for (int i = 0; i < count; ++i) {\n        float cand_d = cand_dists_base[i];\n        int   cand_i = cand_indices_base[i];\n\n        // Compute current global worst (k-th best) and owner of that slot\n        float global_worst_val; int owner_lane; int owner_pos;\n        warp_argmax(local_worst_val, local_worst_pos, global_worst_val, owner_lane, owner_pos, mask);\n\n        // If candidate is better than current worst, replace it\n        if (cand_d < global_worst_val) {\n            if (lane_id() == owner_lane) {\n                top_dists[owner_pos]  = cand_d;\n                top_indices[owner_pos] = cand_i;\n                // Update this lane's local worst, as it changed\n                recompute_local_worst(top_dists, L, local_worst_val, local_worst_pos);\n            }\n        }\n        // All lanes keep participating; implicit warp-synchronous execution\n    }\n\n    // Update worst_threshold for future filtering (global worst after merge)\n    float global_worst_val; int owner_lane_dummy; int owner_pos_dummy;\n    warp_argmax(local_worst_val, local_worst_pos, global_worst_val, owner_lane_dummy, owner_pos_dummy, mask);\n    worst_threshold = global_worst_val;\n}\n\n// Kernel implementing warp-per-query k-NN for 2D points.\n__global__ void knn_kernel(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data,  int data_count,\n    std::pair<int, float>* __restrict__ result,\n    int k,\n    int tile_points) // number of data points cached per block in shared memory per batch\n{\n    extern __shared__ unsigned char shared_raw[];\n    unsigned char* smem = shared_raw;\n\n    const int lane = lane_id();\n    const int warp_in_block = warp_id_in_block();\n    const int warps_per_block = blockDim.x / WARP_SIZE;\n    const int query_idx = blockIdx.x * warps_per_block + warp_in_block;\n\n    if (query_idx >= query_count) return;\n\n    // Lay out shared memory:\n    // [ data tile (float2[tile_points]) ][ candidate counts (int[warps_per_block]) ][ cand dists (float[warps_per_block*k]) ][ cand indices (int[warps_per_block*k]) ]\n    size_t offset = 0;\n\n    float2* sData = reinterpret_cast<float2*>(smem + offset);\n    offset += static_cast<size_t>(tile_points) * sizeof(float2);\n\n    // Align next section to 8 bytes\n    offset = (offset + 7) & ~size_t(7);\n\n    int* sCandCounts = reinterpret_cast<int*>(smem + offset);\n    offset += static_cast<size_t>(warps_per_block) * sizeof(int);\n\n    // Align to 8\n    offset = (offset + 7) & ~size_t(7);\n\n    float* sCandDists = reinterpret_cast<float*>(smem + offset);\n    offset += static_cast<size_t>(warps_per_block) * static_cast<size_t>(k) * sizeof(float);\n\n    int* sCandIdxs = reinterpret_cast<int*>(smem + offset);\n    // offset += warps_per_block * k * sizeof(int); // Not needed further\n\n    // Per-warp view into candidate buffer\n    float* myCandDists = sCandDists + static_cast<size_t>(warp_in_block) * static_cast<size_t>(k);\n    int*   myCandIdxs  = sCandIdxs  + static_cast<size_t>(warp_in_block) * static_cast<size_t>(k);\n    volatile int* myCandCount = sCandCounts + warp_in_block;\n\n    // Initialize candidate count for this warp\n    if (lane == 0) {\n        *myCandCount = 0;\n    }\n    __syncwarp();\n\n    // Load the query point and broadcast within the warp\n    float qx = 0.0f, qy = 0.0f;\n    if (lane == 0) {\n        float2 q = query[query_idx];\n        qx = q.x; qy = q.y;\n    }\n    unsigned full_mask = 0xFFFFFFFFu;\n    qx = __shfl_sync(full_mask, qx, 0);\n    qy = __shfl_sync(full_mask, qy, 0);\n\n    // Each warp maintains its top-k as lane-distributed arrays in registers.\n    // L elements per lane; k is guaranteed to be divisible by 32.\n    const int L = k / WARP_SIZE;\n    float top_dists[32];\n    int   top_indices[32];\n\n    // Initialize intermediate result with +inf distance and -1 index\n    const float INF = CUDART_INF_F;\n    for (int i = 0; i < L; ++i) {\n        top_dists[i] = INF;\n        top_indices[i] = -1;\n    }\n\n    // Local worst for this lane's segment\n    float local_worst_val = INF;\n    int   local_worst_pos = 0;\n\n    // Global worst threshold used for candidate filtering\n    float worst_threshold = INF;\n\n    // Process data in batches loaded into shared memory by the whole block\n    for (int tile_start = 0; tile_start < data_count; tile_start += tile_points) {\n        int tile_count = data_count - tile_start;\n        if (tile_count > tile_points) tile_count = tile_points;\n\n        // Load the tile cooperatively by all threads in the block\n        __syncthreads();\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            sData[i] = data[tile_start + i];\n        }\n        __syncthreads();\n\n        // Each warp processes the tile, computing distances to its query point\n        for (int i = lane; i < tile_count; i += WARP_SIZE) {\n            float2 p = sData[i];\n            float dx = p.x - qx;\n            float dy = p.y - qy;\n            float dist = dx * dx + dy * dy;\n\n            // Filter by current worst threshold of intermediate result\n            int accept = (dist < worst_threshold) ? 1 : 0;\n\n            // Warp-scope append of accepted candidates into per-warp shared buffer\n            unsigned mask = __ballot_sync(full_mask, accept != 0);\n            int num = __popc(mask);\n            if (num > 0) {\n                // Ensure capacity: if adding num would overflow, merge existing candidates first\n                int base = 0;\n                if (lane == 0) {\n                    int count = *myCandCount;\n                    if (count + num > k) {\n                        // Merge all buffered candidates before adding new ones\n                        merge_candidates_into_topk(myCandDists, myCandIdxs, myCandCount,\n                                                   top_dists, top_indices, L,\n                                                   local_worst_val, local_worst_pos,\n                                                   worst_threshold, full_mask);\n                        count = 0;\n                    }\n                    base = count;\n                    *myCandCount = count + num;\n                }\n                base = __shfl_sync(full_mask, base, 0);\n\n                if (accept) {\n                    // Compute my position among active lanes\n                    unsigned lower = mask & ((1u << lane) - 1u);\n                    int offset = __popc(lower);\n                    int pos = base + offset;\n                    myCandDists[pos] = dist;\n                    myCandIdxs[pos]  = tile_start + i;\n                }\n\n                // If buffer just became full, merge it now\n                int need_merge = 0;\n                if (lane == 0) {\n                    need_merge = (*myCandCount >= k) ? 1 : 0;\n                }\n                need_merge = __shfl_sync(full_mask, need_merge, 0);\n                if (need_merge) {\n                    merge_candidates_into_topk(myCandDists, myCandIdxs, myCandCount,\n                                               top_dists, top_indices, L,\n                                               local_worst_val, local_worst_pos,\n                                               worst_threshold, full_mask);\n                }\n            }\n        }\n        // Proceed to next tile; leftover candidates (if any) will be merged later\n    }\n\n    // After processing all tiles, merge any remaining candidates\n    int leftover = 0;\n    if (lane == 0) leftover = *myCandCount;\n    leftover = __shfl_sync(full_mask, leftover, 0);\n    if (leftover > 0) {\n        merge_candidates_into_topk(myCandDists, myCandIdxs, myCandCount,\n                                   top_dists, top_indices, L,\n                                   local_worst_val, local_worst_pos,\n                                   worst_threshold, full_mask);\n    }\n\n    // Final step: write out the k nearest neighbors sorted by ascending distance.\n    // We perform a warp-cooperative selection sort:\n    //  - Repeat k times: find the global minimum among remaining entries,\n    //    write it to output in order, and mark it as used (set to +inf).\n    int out_base = query_idx * k;\n    for (int sel = 0; sel < k; ++sel) {\n        // Each lane finds its local minimum among its L elements\n        float local_min_val = INF;\n        int   local_min_pos = -1;\n        for (int i = 0; i < L; ++i) {\n            float v = top_dists[i];\n            if (v < local_min_val) {\n                local_min_val = v;\n                local_min_pos = i;\n            }\n        }\n\n        // Global argmin across warp\n        float global_min_val; int owner_lane; int owner_pos;\n        warp_argmin(local_min_val, local_min_pos, global_min_val, owner_lane, owner_pos, full_mask);\n\n        // Owner lane writes the result and marks the entry as used\n        if (lane == owner_lane) {\n            int idx = top_indices[owner_pos];\n            // Write (index, distance) to result\n            result[out_base + sel].first  = idx;\n            result[out_base + sel].second = global_min_val;\n            // Mark as used for next iteration\n            top_dists[owner_pos] = INF;\n        }\n        // Implicit warp synchronization is sufficient; all lanes participate uniformly\n    }\n}\n\n// Host function: selects kernel launch configuration and shared memory tile size, then launches the kernel.\nvoid run_knn(const float2* query, int query_count, const float2* data, int data_count, std::pair<int, float>* result, int k)\n{\n    // Choose threads per block: 8 warps (256 threads) per block.\n    const int warps_per_block = 8;\n    const int threads_per_block = warps_per_block * WARP_SIZE;\n\n    // Determine an appropriate tile size for data cached in shared memory,\n    // subject to the device's dynamic shared memory limit and per-warp candidate buffers.\n    int device = 0;\n    cudaGetDevice(&device);\n    cudaDeviceProp prop{};\n    cudaGetDeviceProperties(&prop, device);\n\n    size_t max_dyn_shmem = prop.sharedMemPerBlockOptin ? prop.sharedMemPerBlockOptin : prop.sharedMemPerBlock;\n\n    // Per-block shared memory usage:\n    //   - Candidate counts: warps_per_block * sizeof(int)\n    //   - Candidate buffers: warps_per_block * k * (sizeof(float) + sizeof(int))\n    //   - Data tile: tile_points * sizeof(float2)  <= remaining memory\n    const size_t cand_counts_bytes = static_cast<size_t>(warps_per_block) * sizeof(int);\n    const size_t cand_buffers_bytes = static_cast<size_t>(warps_per_block) * static_cast<size_t>(k) * (sizeof(float) + sizeof(int));\n\n    // Leave some safety margin (optional). Here we use full available memory.\n    size_t remaining = (max_dyn_shmem > cand_counts_bytes + cand_buffers_bytes)\n                     ? (max_dyn_shmem - cand_counts_bytes - cand_buffers_bytes)\n                     : 0;\n\n    // Compute tile_points as large as possible, but at least one block of threads\n    int tile_points = 0;\n    if (remaining >= sizeof(float2)) {\n        tile_points = static_cast<int>(remaining / sizeof(float2));\n    }\n    // Align tile_points down to a multiple of threads_per_block for coalesced loads\n    if (tile_points > 0) {\n        tile_points = (tile_points / threads_per_block) * threads_per_block;\n    }\n    // Ensure a reasonable minimum\n    if (tile_points <= 0) {\n        // Fallback minimal tile equals one block worth of threads\n        tile_points = threads_per_block;\n    }\n\n    // Compute final dynamic shared memory size for kernel launch\n    size_t shmem_bytes = static_cast<size_t>(tile_points) * sizeof(float2)\n                       + cand_counts_bytes\n                       + cand_buffers_bytes;\n\n    // Request the needed dynamic shared memory size\n    cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, static_cast<int>(shmem_bytes));\n\n    // Launch configuration\n    int total_warps = (query_count + warps_per_block - 1) / warps_per_block;\n    dim3 grid(total_warps, 1, 1);\n    dim3 block(threads_per_block, 1, 1);\n\n    // Launch the kernel\n    knn_kernel<<<grid, block, shmem_bytes>>>(query, query_count, data, data_count, result, k, tile_points);\n}\n", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 766, "completion_tokens": 21970, "total_tokens": 22736, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 17856, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db81ec88190be8cd6bfa1992256", "custom_id": "knn05-04", "response": {"status_code": 200, "request_id": "1617a22a4fa2fec6e0b5e39c2aa2f6f1", "body": {"id": "chatcmpl-C4sgI1l0MJQpTvJtW4IgI4WhwERtO", "object": "chat.completion", "created": 1755279542, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n#include <cmath>\n\n// This implementation computes k-NN for 2D points (float2) using one warp per query.\n// It processes the database in shared-memory tiles, keeps an intermediate top-k\n// per warp in shared memory, and accumulates candidates into a per-warp shared\n// buffer. When the buffer fills (or at the end), it is sorted and merged with the\n// intermediate result using a warp-parallel merge-path algorithm. The algorithm\n// uses only dynamic shared memory and no additional device allocations.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Small helper for integer division with rounding up\nstatic inline int div_up(int x, int y) { return (x + y - 1) / y; }\n\n// Device-side utility: next power of two >= x (for x >= 1)\n__device__ __forceinline__ int next_pow2_ceil(int x) {\n    x--;\n    x |= x >> 1;\n    x |= x >> 2;\n    x |= x >> 4;\n    x |= x >> 8;\n    x |= x >> 16;\n    return x + 1;\n}\n\n// Warp-scope bitonic sort of pairs (dist, idx) stored in shared memory.\n// - dist, idx: base pointers to the array for this warp\n// - n: number of valid elements to sort (must be a power of two)\n// The routine sorts ascending by distance. It assumes all threads in the warp\n// participate and uses __syncwarp at each stage.\n__device__ __forceinline__ void warp_bitonic_sort_pairs(float* dist, int* idx, int n) {\n    const unsigned full_mask = 0xFFFFFFFFu;\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n\n    // Bitonic sort network using XOR indexing; n must be a power of two.\n    for (int k = 2; k <= n; k <<= 1) {\n        for (int j = k >> 1; j > 0; j >>= 1) {\n            // Each lane processes multiple elements in stride of warp size.\n            for (int i = lane; i < n; i += WARP_SIZE) {\n                int ixj = i ^ j;\n                if (ixj > i && ixj < n) {\n                    bool up = ((i & k) == 0);\n                    float di = dist[i];\n                    float dj = dist[ixj];\n                    int ii = idx[i];\n                    int ij = idx[ixj];\n                    bool swap = (di > dj) == up;\n                    if (swap) {\n                        dist[i] = dj; idx[i] = ij;\n                        dist[ixj] = di; idx[ixj] = ii;\n                    }\n                }\n            }\n            __syncwarp(full_mask);\n        }\n    }\n}\n\n// Merge-path diagonal search for merging two sorted arrays A (size nA) and B (size nB).\n// Finds the partition i along diagonal 'diag' such that i in [max(0, diag - nB), min(diag, nA)]\n// and B[j-1] <= A[i] with j = diag - i. Returns i; j can be derived as diag - i.\n// This variant uses sentinels -INF/INF via explicit conditionals.\n__device__ __forceinline__ int merge_path_search(const float* A, int nA, const float* B, int nB, int diag) {\n    int i_min = max(0, diag - nB);\n    int i_max = min(diag, nA);\n    while (i_min < i_max) {\n        int i = (i_min + i_max) >> 1;\n        int j = diag - i;\n\n        float a_i      = (i < nA) ? A[i]     : INFINITY;\n        float a_i_minus= (i > 0)  ? A[i - 1] : -INFINITY;\n        float b_j      = (j < nB) ? B[j]     : INFINITY;\n        float b_j_minus= (j > 0)  ? B[j - 1] : -INFINITY;\n\n        // We are looking for the smallest i such that b_{j-1} <= a_i\n        if (b_j_minus > a_i) {\n            i_min = i + 1;\n        } else {\n            i_max = i;\n        }\n    }\n    return i_min;\n}\n\n// Warp-parallel merge of two sorted lists (ascending by distance) producing the first 'k' elements.\n// The output is written to outDist/outIdx. Work is partitioned evenly across the warp using merge-path.\n// nA can be k (current intermediate result), nB is the number of candidates (<= k).\n__device__ __forceinline__ void warp_merge_sorted_topk(\n    const float* A, const int* Aidx, int nA,\n    const float* B, const int* Bidx, int nB,\n    float* outDist, int* outIdx, int k)\n{\n    const unsigned full_mask = 0xFFFFFFFFu;\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n    // k is a power of two and >= 32, so r >= 1\n    int r = k / WARP_SIZE;\n    int out_begin = lane * r;\n    int out_end   = out_begin + r; // exclusive\n\n    int i0 = merge_path_search(A, nA, B, nB, out_begin);\n    int j0 = out_begin - i0;\n    int i1 = merge_path_search(A, nA, B, nB, out_end);\n    int j1 = out_end - i1;\n\n    int ia = i0;\n    int ib = j0;\n\n    // Merge the assigned segment\n    for (int outPos = out_begin; outPos < out_end; ++outPos) {\n        bool takeA = (ia < i1) && ( (ib >= j1) || (A[ia] <= B[ib]) );\n        if (takeA) {\n            outDist[outPos] = A[ia];\n            outIdx[outPos]  = Aidx[ia];\n            ++ia;\n        } else {\n            outDist[outPos] = B[ib];\n            outIdx[outPos]  = Bidx[ib];\n            ++ib;\n        }\n    }\n\n    __syncwarp(full_mask);\n}\n\n// Compute squared Euclidean distance between two float2 points.\n__device__ __forceinline__ float sq_l2(const float2& a, const float2& b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    // Use FMA for better throughput\n    return fmaf(dx, dx, dy * dy);\n}\n\n// The main kernel: one warp processes one query.\n__global__ void knn_warp_kernel(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data,  int data_count,\n    std::pair<int, float>* __restrict__ result,\n    int k, int tile_points)\n{\n    extern __shared__ unsigned char smem_raw[];\n\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n    const int warp_in_block = threadIdx.x >> 5; // / 32\n    const int warps_per_block = blockDim.x >> 5;\n    const int warp_global = blockIdx.x * warps_per_block + warp_in_block;\n    if (warp_global >= query_count) return;\n    const unsigned full_mask = 0xFFFFFFFFu;\n\n    // Shared memory layout\n    // [0] Shared tile of data points: float2 tile[tile_points]\n    // [1] Per-warp arrays: resDist[W*k], resIdx[W*k], bufDist[W*k], bufIdx[W*k], bufCount[W]\n    // The per-warp slices are contiguous segments of size k.\n    unsigned char* p = smem_raw;\n\n    // Align float2 tile base\n    float2* tile = reinterpret_cast<float2*>(p);\n    p += sizeof(float2) * tile_points;\n\n    // Align to 4 bytes for subsequent arrays\n    uintptr_t addr = reinterpret_cast<uintptr_t>(p);\n    addr = (addr + 3) & ~uintptr_t(3);\n    p = reinterpret_cast<unsigned char*>(addr);\n\n    float* resDistAll = reinterpret_cast<float*>(p);\n    p += sizeof(float) * (warps_per_block * k);\n\n    int* resIdxAll = reinterpret_cast<int*>(p);\n    p += sizeof(int) * (warps_per_block * k);\n\n    float* bufDistAll = reinterpret_cast<float*>(p);\n    p += sizeof(float) * (warps_per_block * k);\n\n    int* bufIdxAll = reinterpret_cast<int*>(p);\n    p += sizeof(int) * (warps_per_block * k);\n\n    int* bufCountAll = reinterpret_cast<int*>(p);\n    // p += sizeof(int) * warps_per_block; // Not needed further\n\n    // Per-warp slices\n    float* resDist = resDistAll + warp_in_block * k;\n    int*   resIdx  = resIdxAll  + warp_in_block * k;\n    float* bufDist = bufDistAll + warp_in_block * k;\n    int*   bufIdx  = bufIdxAll  + warp_in_block * k;\n    int&   bufCountRef = bufCountAll[warp_in_block];\n\n    // Load the query point into registers and broadcast to the warp\n    float qx = 0.f, qy = 0.f;\n    if (lane == 0) {\n        float2 q = query[warp_global];\n        qx = q.x; qy = q.y;\n    }\n    qx = __shfl_sync(full_mask, qx, 0);\n    qy = __shfl_sync(full_mask, qy, 0);\n    const float2 qpt = make_float2(qx, qy);\n\n    // Initialize intermediate result with +INF distances and invalid indices.\n    const float INF = INFINITY;\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        resDist[i] = INF;\n        resIdx[i]  = -1;\n    }\n    if (lane == 0) bufCountRef = 0;\n    __syncwarp(full_mask);\n\n    // Helper lambda to flush candidate buffer: sort candidates and merge into res.\n    auto flush_merge = [&](int candCount) {\n        // candCount can be 0..k\n        // If zero, nothing to do.\n        if (candCount <= 0) return;\n\n        // Prepare candidate buffer: pad to next power-of-two length with INF\n        int sortLen = next_pow2_ceil(candCount);\n        // Fill padding with INF to not affect sorting\n        for (int i = lane + candCount; i < sortLen; i += WARP_SIZE) {\n            bufDist[i] = INF;\n            bufIdx[i]  = -1;\n        }\n        __syncwarp(full_mask);\n\n        // Sort the candidate buffer (ascending)\n        warp_bitonic_sort_pairs(bufDist, bufIdx, sortLen);\n\n        // Merge top-k from res (size k) and buf (size candCount) into buf as output\n        warp_merge_sorted_topk(resDist, resIdx, k, bufDist, bufIdx, candCount, bufDist, bufIdx, k);\n\n        // Swap: buf now holds the new intermediate result; reuse old res as the next buffer\n        __syncwarp(full_mask);\n        float* tmpDist = resDist; resDist = bufDist; bufDist = tmpDist;\n        int*   tmpIdx  = resIdx;  resIdx  = bufIdx;  bufIdx  = tmpIdx;\n\n        // Reset candidate count\n        if (lane == 0) bufCountRef = 0;\n        __syncwarp(full_mask);\n    };\n\n    // Process the data in tiles loaded by the entire block\n    for (int base = 0; base < data_count; base += tile_points) {\n        int tileCount = min(tile_points, data_count - base);\n\n        // Cooperative load of the tile into shared memory\n        for (int i = threadIdx.x; i < tileCount; i += blockDim.x) {\n            tile[i] = data[base + i];\n        }\n        __syncthreads(); // Ensure the tile is fully loaded before any warp uses it\n\n        // Each warp processes its assigned query against the shared tile\n        for (int j = lane; j < tileCount; j += WARP_SIZE) {\n            float2 pnt = tile[j];\n            float d = sq_l2(qpt, pnt);\n            int   idx = base + j;\n\n            // Check against current threshold (k-th distance in intermediate result)\n            // We always compare to the current intermediate result; candidates do not affect\n            // the threshold until merged.\n            float threshold = resDist[k - 1];\n            bool accept = (d < threshold);\n\n            // Warp-scope ballot of accepted candidates\n            unsigned mask = __ballot_sync(full_mask, accept);\n            int accepted = __popc(mask);\n\n            if (accepted > 0) {\n                // If adding all accepted would overflow the buffer, flush first (if buffer not empty).\n                int need_merge = 0;\n                if (lane == 0) {\n                    int count = bufCountRef;\n                    need_merge = (count > 0) && (count + accepted > k);\n                }\n                need_merge = __shfl_sync(full_mask, need_merge, 0);\n                if (need_merge) {\n                    // Flush existing buffer\n                    int prevCount = 0;\n                    if (lane == 0) prevCount = bufCountRef;\n                    prevCount = __shfl_sync(full_mask, prevCount, 0);\n                    flush_merge(prevCount);\n                    // Update threshold and recompute acceptance after merge\n                    threshold = resDist[k - 1];\n                    accept = (d < threshold);\n                    mask = __ballot_sync(full_mask, accept);\n                    accepted = __popc(mask);\n                    if (accepted == 0) {\n                        // No longer a candidate after improved threshold; continue\n                        continue;\n                    }\n                }\n\n                // Reserve positions in candidate buffer\n                int basePos = 0;\n                if (lane == 0) {\n                    basePos = bufCountRef;\n                    bufCountRef = basePos + accepted;\n                }\n                basePos = __shfl_sync(full_mask, basePos, 0);\n\n                // Compute each accepting lane's offset using prefix popcount\n                int rank = __popc(mask & ((1u << lane) - 1u));\n                if (accept) {\n                    int pos = basePos + rank;\n                    bufDist[pos] = d;\n                    bufIdx[pos]  = idx;\n                }\n                __syncwarp(full_mask);\n\n                // If buffer is exactly full now, flush it\n                int do_flush = 0;\n                if (lane == 0) do_flush = (bufCountRef == k);\n                do_flush = __shfl_sync(full_mask, do_flush, 0);\n                if (do_flush) {\n                    flush_merge(k);\n                }\n            }\n        }\n\n        __syncthreads(); // Ensure all warps are done with the tile before loading the next one\n    }\n\n    // After all tiles, flush any remaining candidates\n    int remaining = 0;\n    if (lane == 0) remaining = bufCountRef;\n    remaining = __shfl_sync(full_mask, remaining, 0);\n    if (remaining > 0) {\n        flush_merge(remaining);\n    }\n\n    // Write out the final top-k for this query\n    size_t outBase = static_cast<size_t>(warp_global) * static_cast<size_t>(k);\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        result[outBase + i].first  = resIdx[i];\n        result[outBase + i].second = resDist[i];\n    }\n}\n\n// Host function to select launch parameters and run the kernel\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Query device shared memory capability to choose warps per block and tile size.\n    int device = 0;\n    cudaGetDevice(&device);\n    cudaDeviceProp prop{};\n    cudaGetDeviceProperties(&prop, device);\n\n    // Maximum dynamic shared memory per block (opt-in) if supported; fallback to default.\n    int maxOptinSmem = 0;\n    cudaDeviceGetAttribute(&maxOptinSmem, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    size_t maxSmem = maxOptinSmem > 0 ? static_cast<size_t>(maxOptinSmem) : static_cast<size_t>(prop.sharedMemPerBlock);\n\n    // Choose warps per block: try a few options, prefer more warps if shared memory allows.\n    int chosenWarps = 4; // start optimistic\n    for (; chosenWarps >= 1; --chosenWarps) {\n        // Per-block shared memory footprint for per-warp buffers: 16 bytes per entry, 2*k entries per warp = 16*k bytes per warp\n        size_t perWarpSmem = static_cast<size_t>(16 * k);\n        // Reserve a minimum tile of at least 2048 points (16 KB) if possible; adjust later precisely\n        size_t overhead = perWarpSmem * chosenWarps + sizeof(int) * chosenWarps;\n        if (overhead + sizeof(float2) * 1024 <= maxSmem) break;\n    }\n    if (chosenWarps < 1) chosenWarps = 1;\n\n    // Compute the maximum tile size that fits in shared memory with the chosen number of warps\n    size_t perWarpSmem = static_cast<size_t>(16 * k);\n    size_t baseOverhead = perWarpSmem * chosenWarps + sizeof(int) * chosenWarps;\n\n    // Leave a small safety margin\n    const size_t safety = 0;\n\n    size_t maxTileBytes = 0;\n    if (maxSmem > baseOverhead + safety) {\n        maxTileBytes = maxSmem - baseOverhead - safety;\n    } else {\n        maxTileBytes = sizeof(float2) * 256; // minimal fallback\n    }\n\n    int tile_points = static_cast<int>(maxTileBytes / sizeof(float2));\n    // Cap tile_points to a reasonable size and at least 1024 for efficiency if possible\n    tile_points = max(256, min(tile_points, 16384));\n    // Ensure tile_points is not zero\n    if (tile_points <= 0) tile_points = 256;\n\n    // Recompute actual shared memory size required\n    size_t smemBytes = sizeof(float2) * static_cast<size_t>(tile_points)\n                     + perWarpSmem * chosenWarps // resDist+resIdx+bufDist+bufIdx (16*k bytes per warp)\n                     + sizeof(int) * chosenWarps; // bufCount\n\n    // Request opt-in larger shared memory if needed/available\n    cudaFuncSetAttribute(knn_warp_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, static_cast<int>(min(smemBytes, maxSmem)));\n\n    dim3 block(chosenWarps * WARP_SIZE, 1, 1);\n    dim3 grid(div_up(query_count, chosenWarps), 1, 1);\n\n    knn_warp_kernel<<<grid, block, smemBytes>>>(query, query_count, data, data_count, result, k, tile_points);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 766, "completion_tokens": 20046, "total_tokens": 20812, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 15808, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db871ac8190b2b336c9a95633fe", "custom_id": "knn05-05", "response": {"status_code": 200, "request_id": "c83228535adb0853fd42bd7ae5aa0b97", "body": {"id": "chatcmpl-C4simuT8cZ0rzTADIiGSXx9u56T0d", "object": "chat.completion", "created": 1755279696, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cfloat>\n#include <utility>\n\n// This implementation maps one warp (32 threads) to one query point.\n// Each warp maintains its private intermediate top-k (indices and distances) in registers,\n// and a per-warp candidate buffer of size k in shared memory. Data points are processed\n// in tiles cached in shared memory by the whole block. Candidates whose distances are\n// smaller than the current k-th best distance (threshold) are appended to the buffer.\n// Whenever the buffer becomes full, it is merged with the intermediate result using a\n// warp-parallel merge-path procedure. The candidate buffer is sorted in shared memory\n// by a warp-only bitonic sort. The intermediate result is kept globally sorted ascending,\n// which allows efficient parallel merge and fast threshold retrieval.\n//\n// Constraints leveraged:\n// - k is a power of two between 32 and 1024 inclusive.\n// - data_count >= k\n// - Arrays are device-allocated, and we allocate no additional device memory.\n// - We use dynamic shared memory per block for the data tile cache and per-warp buffers.\n//\n// Notes:\n// - To avoid large register pressure for 2k items in a merge, we copy the current best\n//   (size k) into per-warp shared memory temporarily during merge. Thus, per warp we\n//   need 2*k pairs (candidate + best tmp) in shared memory during merge.\n// - We select WARPS_PER_BLOCK conservatively (4) so that even at k=1024 we remain well\n//   within A100's per-block shared memory limit after opting-in to the maximum dynamic\n//   shared memory (164KB on A100, 228KB on H100).\n// - We specialize the kernel for K in {32, 64, 128, 256, 512, 1024} via templates.\n//   At runtime, we dispatch to the appropriate specialization based on k.\n//\n// Implementation details:\n// - Pair layout is compatible with std::pair<int, float> for device writes.\n// - All warp collectives use a full mask (0xffffffff). Inactive warps (queries >= count)\n//   still participate in ballots but never satisfy the accept condition, so they don't\n//   write to buffers.\n// - The bitonic sort is implemented as a warp-serial algorithm over K elements using\n//   32 threads. It uses __syncwarp barriers between stages.\n// - The parallel merge uses merge-path to partition the first K elements of the merge\n//   of (best, sorted candidates) across 32 lanes. Each lane merges a small consecutive\n//   slice (K/32 items) sequentially and writes directly into its per-lane register chunk.\n//\n// Tuning knobs chosen:\n// - WARPS_PER_BLOCK = 4 (128 threads per block)\n// - Tile size is chosen at runtime to maximally utilize the available shared memory,\n//   after accounting for per-warp buffers.\n\n#ifndef KNN_WARPS_PER_BLOCK\n#define KNN_WARPS_PER_BLOCK 4\n#endif\n\n// Output pair type with the same memory layout as std::pair<int, float>.\nstruct __align__(8) PairIF {\n    int   first;   // index\n    float second;  // distance\n};\n\n// Utility: CUDA check macro (used in host function).\nstatic inline void __checkCuda(cudaError_t err, const char* stmt, const char* file, int line) {\n#if !defined(NDEBUG)\n    if (err != cudaSuccess) {\n        fprintf(stderr, \"CUDA error %s at %s:%d: %s\\n\", stmt, file, line, cudaGetErrorString(err));\n        abort();\n    }\n#else\n    (void)err; (void)stmt; (void)file; (void)line;\n#endif\n}\n#define CUDA_CHECK(x) __checkCuda((x), #x, __FILE__, __LINE__)\n\ntemplate<int K>\nstruct KnnTraits {\n    static constexpr int WARP_SIZE = 32;\n    static constexpr int LANES     = 32;\n    static constexpr int CHUNK     = K / LANES; // number of elements per lane\n    static_assert((K % WARP_SIZE) == 0, \"K must be a multiple of 32.\");\n};\n\n// Warp-wide bitonic sort for K elements stored in shared memory as PairIF.\n// Sort ascending by 'second' (distance).\ntemplate<int K>\n__device__ __forceinline__\nvoid warp_bitonic_sort_pairs(PairIF* arr) {\n    constexpr unsigned FULL_MASK = 0xFFFFFFFFu;\n    const unsigned lane = threadIdx.x & 31;\n\n    // Classic bitonic sort network: size is the total sequence length of the bitonic merge,\n    // stride is the distance between compare-exchange pairs at the current stage.\n    for (unsigned size = 2; size <= (unsigned)K; size <<= 1) {\n        bool up = true; // direction determined per index i via (i & size) == 0\n        // For each bitonic merge stage, do a log(size) pass.\n        for (unsigned stride = size >> 1; stride > 0; stride >>= 1) {\n            // Partition the K indices across 32 lanes, each lane processes i = lane, lane+32, ...\n            for (unsigned i = lane; i < (unsigned)K; i += 32u) {\n                unsigned j = i ^ stride;\n                if (j > i && j < (unsigned)K) {\n                    bool dir = ((i & size) == 0); // true => ascending\n                    PairIF a = arr[i];\n                    PairIF b = arr[j];\n                    // Compare distances\n                    bool swap_needed = dir ? (a.second > b.second) : (a.second < b.second);\n                    if (swap_needed) {\n                        // Swap\n                        arr[i] = b;\n                        arr[j] = a;\n                    }\n                }\n            }\n            __syncwarp(FULL_MASK);\n        }\n    }\n}\n\n// Merge-path partition search for merging first diag elements from A and B.\n// A and B are sorted ascending arrays of length nA and nB (here both = K).\n// Returns ai, bi such that ai + bi = diag and A[ai-1] <= B[bi] and B[bi-1] < A[ai].\n// A and B are in shared memory.\n__device__ __forceinline__\nvoid merge_path_search(const PairIF* __restrict__ A, const PairIF* __restrict__ B,\n                       int nA, int nB, int diag, int& ai, int& bi) {\n    // Clamp the search range for ai\n    int lo = max(0, diag - nB);\n    int hi = min(diag, nA);\n    // Sentinels: we will manually handle boundaries when reading values.\n    while (lo <= hi) {\n        int mid = (lo + hi) >> 1;\n        int a_idx = mid;\n        int b_idx = diag - mid;\n\n        float a_left  = (a_idx > 0)     ? A[a_idx - 1].second : -CUDART_INF_F;\n        float b_left  = (b_idx > 0)     ? B[b_idx - 1].second : -CUDART_INF_F;\n        float a_right = (a_idx < nA)    ? A[a_idx].second     : CUDART_INF_F;\n        float b_right = (b_idx < nB)    ? B[b_idx].second     : CUDART_INF_F;\n\n        if (a_left <= b_right && b_left < a_right) {\n            ai = a_idx;\n            bi = b_idx;\n            return;\n        }\n        if (a_left > b_right) {\n            hi = mid - 1;\n        } else {\n            lo = mid + 1;\n        }\n    }\n    // Fallback (shouldn't trigger): place all in A or B.\n    ai = min(diag, nA);\n    bi = diag - ai;\n}\n\n// Merge the first K elements from the sorted arrays A (best_tmp) and B (cand sorted)\n// into per-lane register arrays best_idx/best_dist, using merge-path partitioning.\n// Each lane writes exactly CHUNK (=K/32) outputs.\ntemplate<int K>\n__device__ __forceinline__\nvoid warp_merge_topk_from_shared(const PairIF* __restrict__ A,\n                                 const PairIF* __restrict__ B,\n                                 int* __restrict__ best_idx, float* __restrict__ best_dist) {\n    constexpr unsigned FULL_MASK = 0xFFFFFFFFu;\n    constexpr int CHUNK = KnnTraits<K>::CHUNK;\n    const int lane = threadIdx.x & 31;\n\n    // Each lane is responsible for output positions [d0, d1)\n    int d0 = lane * CHUNK;\n    int d1 = d0 + CHUNK;\n\n    int a0, b0, a1, b1;\n    merge_path_search(A, B, K, K, d0, a0, b0);\n    merge_path_search(A, B, K, K, d1, a1, b1);\n\n    // Sequentially merge the assigned segment\n    int ia = a0;\n    int ib = b0;\n    for (int t = 0; t < CHUNK; ++t) {\n        // Boundaries\n        bool takeA = false;\n        float va = CUDART_INF_F;\n        float vb = CUDART_INF_F;\n        if (ia < a1) va = A[ia].second;\n        if (ib < b1) vb = B[ib].second;\n        if (va <= vb) {\n            takeA = true;\n        }\n        if (takeA) {\n            best_dist[t] = va;\n            best_idx[t]  = A[ia].first;\n            ++ia;\n        } else {\n            best_dist[t] = vb;\n            best_idx[t]  = B[ib].first;\n            ++ib;\n        }\n    }\n    __syncwarp(FULL_MASK);\n}\n\n// Merge utility: given per-lane register arrays best_idx/best_dist (globally sorted ascending),\n// and a per-warp candidate buffer cand (length 'cand_count' <= K), perform:\n// - pad cand to K with +inf\n// - sort cand ascending in shared memory via warp bitonic\n// - copy current best (registers) into best_tmp (shared)\n// - merge-path to produce the first K elements of merge(best_tmp, cand) back into registers\ntemplate<int K>\n__device__ __forceinline__\nvoid warp_merge_buffer_with_best(PairIF* __restrict__ cand,\n                                 PairIF* __restrict__ best_tmp,\n                                 int    cand_count,\n                                 int*   __restrict__ best_idx,\n                                 float* __restrict__ best_dist) {\n    constexpr unsigned FULL_MASK = 0xFFFFFFFFu;\n    constexpr int CHUNK = KnnTraits<K>::CHUNK;\n    const int lane = threadIdx.x & 31;\n\n    // 1) Pad candidate buffer to length K with +inf distances.\n    for (int i = lane; i < K; i += 32) {\n        if (i >= cand_count) {\n            cand[i].first  = -1;\n            cand[i].second = CUDART_INF_F;\n        }\n    }\n    __syncwarp(FULL_MASK);\n\n    // 2) Sort candidate buffer ascending by distance.\n    warp_bitonic_sort_pairs<K>(cand);\n\n    // 3) Copy current best from per-lane registers into shared memory contiguous array.\n    // Layout: lane t stores its CHUNK items at positions [t*CHUNK .. t*CHUNK + CHUNK-1]\n    int base = lane * CHUNK;\n    for (int j = 0; j < CHUNK; ++j) {\n        best_tmp[base + j].first  = best_idx[j];\n        best_tmp[base + j].second = best_dist[j];\n    }\n    __syncwarp(FULL_MASK);\n\n    // 4) Merge first K elements from (best_tmp, cand) into new best arrays in registers.\n    warp_merge_topk_from_shared<K>(best_tmp, cand, best_idx, best_dist);\n}\n\n// Kernel: One warp per query; WARPS_PER_BLOCK warps per block.\ntemplate<int K, int WARPS_PER_BLOCK>\n__global__ void knn_kernel_2d(const float2* __restrict__ query,\n                              int query_count,\n                              const float2* __restrict__ data,\n                              int data_count,\n                              PairIF* __restrict__ result,\n                              int tile_elems) {\n    constexpr unsigned FULL_MASK = 0xFFFFFFFFu;\n    constexpr int WARP_SIZE = 32;\n    constexpr int CHUNK = KnnTraits<K>::CHUNK;\n\n    extern __shared__ unsigned char smem_raw[];\n    // Shared memory layout: [tile (float2) | cand[W*K] (PairIF) | best_tmp[W*K] (PairIF)]\n    float2* tile = reinterpret_cast<float2*>(smem_raw);\n    PairIF* base_cand = reinterpret_cast<PairIF*>(tile + tile_elems);\n    PairIF* base_best = base_cand + WARPS_PER_BLOCK * K;\n\n    const int lane      = threadIdx.x & (WARP_SIZE - 1);\n    const int warp_in_b = threadIdx.x >> 5;\n    const int warp_id   = blockIdx.x * WARPS_PER_BLOCK + warp_in_b; // query id handled by this warp\n    const bool warp_active = (warp_id < query_count);\n\n    // Per-warp pointers to shared memory buffers\n    PairIF* cand     = base_cand + warp_in_b * K;\n    PairIF* best_tmp = base_best + warp_in_b * K;\n\n    // Load the query point once (by lane 0) and broadcast to warp\n    float qx = 0.0f, qy = 0.0f;\n    if (lane == 0) {\n        if (warp_active) {\n            float2 q = query[warp_id];\n            qx = q.x; qy = q.y;\n        }\n    }\n    qx = __shfl_sync(FULL_MASK, qx, 0);\n    qy = __shfl_sync(FULL_MASK, qy, 0);\n\n    // Per-warp intermediate top-k in registers (distributed across lanes)\n    int   best_idx[CHUNK];\n    float best_dist[CHUNK];\n    for (int j = 0; j < CHUNK; ++j) {\n        best_idx[j]  = -1;\n        best_dist[j] = CUDART_INF_F;\n    }\n    // Current threshold (k-th best) for candidate filtering\n    float threshold = CUDART_INF_F;\n\n    // Candidate buffer fill count (replicated across lanes; updated by lane 0)\n    int cand_count = 0;\n\n    // Process data in tiles\n    for (int tile_base = 0; tile_base < data_count; tile_base += tile_elems) {\n        int tile_count = min(tile_elems, data_count - tile_base);\n\n        // Load tile into shared memory cooperatively by the whole block\n        for (int t = threadIdx.x; t < tile_count; t += blockDim.x) {\n            tile[t] = data[tile_base + t];\n        }\n        __syncthreads();\n\n        // Each warp processes the cached tile: compute distances and push candidates\n        for (int i = lane; i < tile_count; i += WARP_SIZE) {\n            // Compute squared L2 distance to the query\n            float2 p = tile[i];\n            float dx = p.x - qx;\n            float dy = p.y - qy;\n            float dist = dx * dx + dy * dy;\n            int   idx  = tile_base + i;\n\n            // Evaluate candidate under current threshold and warp activity\n            bool accept = warp_active && (dist < threshold);\n\n            // Warp-cooperative append to the candidate buffer with overflow handling\n            // If we overflow buffer capacity K, we merge buffer with best, update threshold,\n            // and retry the leftover accepted candidates (if any) under the new threshold.\n            unsigned mask = __ballot_sync(FULL_MASK, accept);\n            int naccept = __popc(mask);\n\n            while (naccept > 0) {\n                int capacity = K - cand_count;\n                if (capacity == 0) {\n                    // Merge buffer with current best (buffer full)\n                    warp_merge_buffer_with_best<K>(cand, best_tmp, K, best_idx, best_dist);\n                    cand_count = 0;\n                    // Update threshold to the new k-th best, which is the last element of global sorted best\n                    float last = (lane == 31) ? best_dist[CHUNK - 1] : -CUDART_INF_F;\n                    threshold = __shfl_sync(FULL_MASK, last, 31);\n                    // Re-evaluate acceptance under new (smaller) threshold\n                    accept = warp_active && (dist < threshold);\n                    mask = __ballot_sync(FULL_MASK, accept);\n                    naccept = __popc(mask);\n                    continue;\n                }\n\n                int take = min(capacity, naccept);\n\n                // Compute per-lane rank among accepted lanes\n                unsigned lane_mask_lt = (1u << lane) - 1u;\n                int rank_in_mask = __popc(mask & lane_mask_lt);\n                bool take_this = accept && (rank_in_mask < take);\n\n                // Broadcast base offset (previous cand_count)\n                int base = __shfl_sync(FULL_MASK, cand_count, 0);\n\n                if (take_this) {\n                    int pos = base + rank_in_mask;\n                    cand[pos].first  = idx;\n                    cand[pos].second = dist;\n                }\n\n                // Update cand_count by lane 0\n                if (lane == 0) {\n                    cand_count += take;\n                }\n                cand_count = __shfl_sync(FULL_MASK, cand_count, 0);\n\n                // Remove taken lanes from mask and continue if leftovers remain\n                if (take == naccept) {\n                    // All accepted written, we're done for this i\n                    naccept = 0;\n                } else {\n                    // Some leftover accepted still need to be written, buffer now full -> merge\n                    // Mark taken lanes as not accepted for the next round\n                    accept = accept && (rank_in_mask >= take);\n                    mask = __ballot_sync(FULL_MASK, accept);\n                    naccept = __popc(mask);\n                    // Buffer should be full here; merge to free it\n                    if (cand_count == K) {\n                        warp_merge_buffer_with_best<K>(cand, best_tmp, K, best_idx, best_dist);\n                        cand_count = 0;\n                        float last = (lane == 31) ? best_dist[CHUNK - 1] : -CUDART_INF_F;\n                        threshold = __shfl_sync(FULL_MASK, last, 31);\n                    }\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    // After processing all tiles, merge any remaining candidates in the buffer\n    if (cand_count > 0) {\n        warp_merge_buffer_with_best<K>(cand, best_tmp, cand_count, best_idx, best_dist);\n        cand_count = 0;\n        float last = (lane == 31) ? best_dist[CHUNK - 1] : -CUDART_INF_F;\n        threshold = __shfl_sync(FULL_MASK, last, 31);\n        (void)threshold;\n    }\n\n    // Write final results for this query (globally sorted ascending by distance)\n    if (warp_active) {\n        PairIF* out = result + warp_id * K;\n        int base = lane * CHUNK;\n        for (int j = 0; j < CHUNK; ++j) {\n            out[base + j].first  = best_idx[j];\n            out[base + j].second = best_dist[j];\n        }\n    }\n}\n\n// Host-side launcher: choose specialization based on k, configure shared memory and launch.\nstatic inline int round_down_pow2(int x) {\n    int p = 1;\n    while ((p << 1) <= x) p <<= 1;\n    return p;\n}\n\ntemplate<int K>\nvoid launch_knn_kernel(const float2* query, int query_count,\n                       const float2* data, int data_count,\n                       PairIF* result,\n                       int device_id) {\n    constexpr int WARPS_PER_BLOCK = KNN_WARPS_PER_BLOCK;\n    constexpr int WARP_SIZE = 32;\n    constexpr size_t PER_WARP_BYTES = (size_t)(2 * K) * sizeof(PairIF); // cand + best_tmp\n    const int threads_per_block = WARPS_PER_BLOCK * WARP_SIZE;\n\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, device_id));\n\n    // Opt-in to the maximum dynamic shared memory per block for this kernel specialization.\n    int optin_smem = 0;\n#if CUDART_VERSION >= 11000\n    CUDA_CHECK(cudaDeviceGetAttribute(&optin_smem, cudaDevAttrMaxSharedMemoryPerBlockOptin, device_id));\n#else\n    optin_smem = prop.sharedMemPerBlock;\n#endif\n    // Set the function attribute to allow using up to optin_smem bytes of dynamic shared memory.\n    CUDA_CHECK(cudaFuncSetAttribute(knn_kernel_2d<K, WARPS_PER_BLOCK>,\n                                    cudaFuncAttributeMaxDynamicSharedMemorySize,\n                                    optin_smem));\n\n    // Determine tile size to fit within shared memory limit\n    // smem_total = tile_elems*sizeof(float2) + WARPS_PER_BLOCK*(2*K*sizeof(PairIF))\n    size_t per_block_warp_bytes = WARPS_PER_BLOCK * PER_WARP_BYTES;\n    size_t max_tile_bytes = (optin_smem > (int)per_block_warp_bytes)\n                                ? (optin_smem - per_block_warp_bytes)\n                                : 0;\n    // Choose tile_elems as a multiple of 256 for good load balance, but at least 1024.\n    int tile_elems = (int)(max_tile_bytes / sizeof(float2));\n    tile_elems = max(1024, (tile_elems / 256) * 256);\n    // Clamp tile_elems to data_count to avoid excessive smem request for small data.\n    tile_elems = min(tile_elems, data_count > 0 ? data_count : 1);\n\n    // Final dynamic shared memory size requested at launch\n    size_t dyn_smem = (size_t)tile_elems * sizeof(float2) + per_block_warp_bytes;\n\n    // Compute grid configuration\n    int warps_needed = (query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK;\n    dim3 grid(warps_needed, 1, 1);\n    dim3 block(threads_per_block, 1, 1);\n\n    // Launch\n    knn_kernel_2d<K, WARPS_PER_BLOCK><<<grid, block, dyn_smem>>>(\n        query, query_count, data, data_count, result, tile_elems\n    );\n}\n\nextern \"C\"\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data, int data_count,\n             std::pair<int, float> *result, int k) {\n    if (query_count <= 0 || data_count <= 0) return;\n    // Cast the result pointer to our layout-compatible pair type\n    PairIF* result_if = reinterpret_cast<PairIF*>(result);\n\n    int device_id = 0;\n    CUDA_CHECK(cudaGetDevice(&device_id));\n\n    // Dispatch based on k (power of two between 32 and 1024 inclusive)\n    switch (k) {\n        case 32:   launch_knn_kernel<32 >(query, query_count, data, data_count, result_if, device_id); break;\n        case 64:   launch_knn_kernel<64 >(query, query_count, data, data_count, result_if, device_id); break;\n        case 128:  launch_knn_kernel<128>(query, query_count, data, data_count, result_if, device_id); break;\n        case 256:  launch_knn_kernel<256>(query, query_count, data, data_count, result_if, device_id); break;\n        case 512:  launch_knn_kernel<512>(query, query_count, data, data_count, result_if, device_id); break;\n        case 1024: launch_knn_kernel<1024>(query, query_count, data, data_count, result_if, device_id); break;\n        default:\n            // Fallback: round down to nearest supported power of two (shouldn't happen per spec).\n            // This ensures we never crash, but the result will be the k' nearest with k' < k.\n            {\n                int kk = round_down_pow2(k);\n                if      (kk >= 1024) launch_knn_kernel<1024>(query, query_count, data, data_count, result_if, device_id);\n                else if (kk >= 512)  launch_knn_kernel<512 >(query, query_count, data, data_count, result_if, device_id);\n                else if (kk >= 256)  launch_knn_kernel<256 >(query, query_count, data, data_count, result_if, device_id);\n                else if (kk >= 128)  launch_knn_kernel<128 >(query, query_count, data, data_count, result_if, device_id);\n                else if (kk >= 64)   launch_knn_kernel<64  >(query, query_count, data, data_count, result_if, device_id);\n                else                 launch_knn_kernel<32  >(query, query_count, data, data_count, result_if, device_id);\n            }\n            break;\n    }\n    CUDA_CHECK(cudaGetLastError());\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 766, "completion_tokens": 24410, "total_tokens": 25176, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 18944, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db810f08190a13a1b8b7264f25a", "custom_id": "knn05-06", "response": {"status_code": 200, "request_id": "6a8e161c0f86cfcc3364779e04b8e87f", "body": {"id": "chatcmpl-C4sjxuBwyWUjP3YqnVudFa4AfD6AW", "object": "chat.completion", "created": 1755279769, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <stdint.h>\n\n// This implementation computes k-NN for 2D points using one warp (32 threads) per query.\n// It processes the data in shared-memory tiles. Each warp maintains two per-query buffers\n// in shared memory:\n//   - intermediate result (k best pairs so far)\n//   - candidate buffer (up to k new candidate pairs)\n// When the candidate buffer fills (or at the end), the warp merges the intermediate\n// result and candidate buffer by performing an in-place bitonic sort over the combined\n// 2k elements in shared memory, then keeps the first k elements as the new intermediate result.\n//\n// The kernel uses only per-block dynamic shared memory (no additional device allocations).\n// It assumes k is a power of two between 32 and 1024 inclusive, and data_count >= k.\n\nstruct IdxDist {\n    int   idx;\n    float dist;\n};\n\nstatic __device__ __forceinline__ float sqr(float x) { return x * x; }\n\nstatic __device__ __forceinline__ bool pairGreater(int ai, float ad, int bi, float bd) {\n    // Return true if (ad, ai) > (bd, bi) lexicographically (distance primary, index secondary)\n    return (ad > bd) || ((ad == bd) && (ai > bi));\n}\n\nstatic __device__ __forceinline__ unsigned lane_id() {\n    return threadIdx.x & 31;\n}\n\nstatic __device__ __forceinline__ unsigned warp_id_in_block() {\n    return threadIdx.x >> 5;\n}\n\nstatic __device__ __forceinline__ unsigned full_mask() {\n    return 0xFFFFFFFFu;\n}\n\n// Accessors for the combined array of size 2k, where positions [0..k-1] are in res buffers\n// and positions [k..2k-1] are in cand buffers. This lets us run a bitonic sort\n// across 2k elements without allocating an additional array.\nstatic __device__ __forceinline__\nvoid get_pair_2k(int pos, int k,\n                 const int* __restrict__ res_idx, const float* __restrict__ res_dist,\n                 const int* __restrict__ cand_idx, const float* __restrict__ cand_dist,\n                 int &idx, float &dist) {\n    if (pos < k) {\n        idx = res_idx[pos];\n        dist = res_dist[pos];\n    } else {\n        int p = pos - k;\n        idx = cand_idx[p];\n        dist = cand_dist[p];\n    }\n}\n\nstatic __device__ __forceinline__\nvoid set_pair_2k(int pos, int k,\n                 int* __restrict__ res_idx, float* __restrict__ res_dist,\n                 int* __restrict__ cand_idx, float* __restrict__ cand_dist,\n                 int idx, float dist) {\n    if (pos < k) {\n        res_idx[pos]  = idx;\n        res_dist[pos] = dist;\n    } else {\n        int p = pos - k;\n        cand_idx[p]  = idx;\n        cand_dist[p] = dist;\n    }\n}\n\n// In-place bitonic sort of the combined array of length N = 2*k stored across two buffers.\n// After completion, res buffers [0..k-1] contain the k smallest pairs in ascending order.\n// This uses 32 threads of the current warp cooperatively; threads operate on strided indices.\nstatic __device__ __forceinline__\nvoid bitonic_sort_combined_2k(int k,\n                              int* __restrict__ res_idx, float* __restrict__ res_dist,\n                              int* __restrict__ cand_idx, float* __restrict__ cand_dist) {\n    const int N = 2 * k;\n    const unsigned lane = lane_id();\n\n    // Standard bitonic sort network for N elements (N is power of two).\n    // Using warp-synchronous programming with __syncwarp barriers.\n    for (int size = 2; size <= N; size <<= 1) {\n        // The direction for this size block is ascending when ((i & size) == 0)\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            for (int i = lane; i < N; i += warpSize) {\n                int j = i ^ stride;\n                if (j > i) {\n                    bool up = ((i & size) == 0);\n                    int ai, bi;\n                    float ad, bd;\n                    get_pair_2k(i, k, res_idx, res_dist, cand_idx, cand_dist, ai, ad);\n                    get_pair_2k(j, k, res_idx, res_dist, cand_idx, cand_dist, bi, bd);\n\n                    bool swap_needed = up ? pairGreater(ai, ad, bi, bd) : pairGreater(bi, bd, ai, ad);\n                    if (swap_needed) {\n                        // Swap pairs at i and j\n                        set_pair_2k(i, k, res_idx, res_dist, cand_idx, cand_dist, bi, bd);\n                        set_pair_2k(j, k, res_idx, res_dist, cand_idx, cand_dist, ai, ad);\n                    }\n                }\n            }\n            __syncwarp();\n        }\n    }\n}\n\n// Flush the candidate buffer by merging it with the intermediate result using a\n// bitonic sort over the 2k combined elements. Any unused candidate slots should\n// already be padded with (idx=-1, dist=INF) by the caller if cand_count < k.\nstatic __device__ __forceinline__\nvoid flush_and_merge_warp(int k,\n                          int* __restrict__ res_idx, float* __restrict__ res_dist,\n                          int* __restrict__ cand_idx, float* __restrict__ cand_dist) {\n    bitonic_sort_combined_2k(k, res_idx, res_dist, cand_idx, cand_dist);\n    // After sort, res buffers contain the k smallest elements in ascending order.\n}\n\n// Kernel: one warp handles one query.\n__global__ void knn_warp_kernel(const float2* __restrict__ query,\n                                int query_count,\n                                const float2* __restrict__ data,\n                                int data_count,\n                                IdxDist* __restrict__ result,\n                                int k,\n                                int tile_points,\n                                int warps_per_block) {\n    extern __shared__ unsigned char smem_raw[];\n\n    // Layout shared memory:\n    // [tile_points * sizeof(float2)] tile buffer for data points (shared by the entire block)\n    // Then per-warp private regions:\n    //   res_idx[warps_per_block][k]\n    //   res_dist[warps_per_block][k]\n    //   cand_idx[warps_per_block][k]\n    //   cand_dist[warps_per_block][k]\n    //   cand_count[warps_per_block]\n    //\n    // Total dynamic shared memory is set accordingly by the host.\n    unsigned char* ptr = smem_raw;\n\n    // Tile of data points for the whole block\n    float2* tile = reinterpret_cast<float2*>(ptr);\n    ptr += sizeof(float2) * tile_points;\n\n    // Align to 16 bytes for good measure\n    uintptr_t addr = reinterpret_cast<uintptr_t>(ptr);\n    addr = (addr + 15u) & ~uintptr_t(15u);\n    ptr = reinterpret_cast<unsigned char*>(addr);\n\n    // Per-warp buffers\n    int*   res_idx_all  = reinterpret_cast<int*>(ptr);\n    ptr += sizeof(int) * warps_per_block * k;\n\n    float* res_dist_all = reinterpret_cast<float*>(ptr);\n    ptr += sizeof(float) * warps_per_block * k;\n\n    int*   cand_idx_all = reinterpret_cast<int*>(ptr);\n    ptr += sizeof(int) * warps_per_block * k;\n\n    float* cand_dist_all = reinterpret_cast<float*>(ptr);\n    ptr += sizeof(float) * warps_per_block * k;\n\n    int* cand_count_all = reinterpret_cast<int*>(ptr);\n    // No need to advance ptr further\n\n    const unsigned lane   = lane_id();\n    const unsigned warp_b = warp_id_in_block();\n    const unsigned warp_g = (blockIdx.x * warps_per_block) + warp_b;\n    const bool warp_active = (warp_g < static_cast<unsigned>(query_count));\n\n    // Per-warp base pointers into shared memory buffers\n    int*   res_idx  = res_idx_all  + warp_b * k;\n    float* res_dist = res_dist_all + warp_b * k;\n    int*   cand_idx  = cand_idx_all  + warp_b * k;\n    float* cand_dist = cand_dist_all + warp_b * k;\n    int&   cand_count = cand_count_all[warp_b];\n\n    // Initialize per-warp structures\n    if (lane == 0) {\n        cand_count = 0;\n    }\n    // Initialize intermediate result with +INF distances so any real candidate is better.\n    if (warp_active) {\n        for (int i = lane; i < k; i += warpSize) {\n            res_idx[i]  = -1;\n            res_dist[i] = CUDART_INF_F;\n        }\n    }\n    __syncwarp();\n\n    // Load the query point for this warp\n    float qx = 0.0f, qy = 0.0f;\n    if (warp_active) {\n        float2 q = query[warp_g];\n        qx = q.x; qy = q.y;\n    }\n\n    // Current threshold: the k-th smallest distance so far (last element of res_dist).\n    float threshold = CUDART_INF_F;\n\n    // Process data in tiles\n    for (int base = 0; base < data_count; base += tile_points) {\n        const int count = min(tile_points, data_count - base);\n\n        // Load data tile into shared memory cooperatively by the whole block\n        for (int i = threadIdx.x; i < count; i += blockDim.x) {\n            tile[i] = data[base + i];\n        }\n        __syncthreads();\n\n        // Each active warp processes its query against all points in the tile\n        if (warp_active) {\n            for (int t = lane; t < count; t += warpSize) {\n                float2 p = tile[t];\n                float dx = p.x - qx;\n                float dy = p.y - qy;\n                float d2 = fmaf(dx, dx, dy * dy); // dx*dx + dy*dy (use FMA for better perf)\n\n                // Fast reject by current threshold\n                int global_idx = base + t;\n                unsigned keep_mask = __ballot_sync(full_mask(), d2 < threshold);\n                if (keep_mask == 0u) {\n                    continue; // no lane keeps this iteration\n                }\n\n                int adds = __popc(keep_mask);\n\n                // If buffer would overflow, flush it first, then re-evaluate with updated threshold\n                int local_cand_count = 0;\n                if (lane == 0) local_cand_count = cand_count;\n                local_cand_count = __shfl_sync(full_mask(), local_cand_count, 0);\n\n                if (local_cand_count + adds > k) {\n                    // Pad remaining candidate slots with +INF so that combined size is exactly k\n                    for (int i = lane; i < k; i += warpSize) {\n                        if (i >= local_cand_count) {\n                            cand_idx[i]  = -1;\n                            cand_dist[i] = CUDART_INF_F;\n                        }\n                    }\n                    __syncwarp();\n\n                    // Merge candidate buffer with intermediate result\n                    flush_and_merge_warp(k, res_idx, res_dist, cand_idx, cand_dist);\n\n                    if (lane == 0) cand_count = 0;\n                    __syncwarp();\n\n                    // Update threshold\n                    threshold = res_dist[k - 1];\n\n                    // Recompute keep mask under tighter threshold\n                    keep_mask = __ballot_sync(full_mask(), d2 < threshold);\n                    adds = __popc(keep_mask);\n                    if (adds == 0) {\n                        continue;\n                    }\n                }\n\n                // Reserve slots in candidate buffer\n                int base_off = 0;\n                if (lane == 0) {\n                    base_off = cand_count;\n                    cand_count = base_off + adds;\n                }\n                base_off = __shfl_sync(full_mask(), base_off, 0);\n\n                // Compute per-lane offset among kept lanes\n                unsigned lt_mask = (lane == 0) ? 0u : ((1u << lane) - 1u);\n                int my_off = __popc(keep_mask & lt_mask);\n\n                // Write kept candidates\n                if (d2 < threshold) {\n                    int pos = base_off + my_off;\n                    // pos is guaranteed < k due to earlier flush if needed\n                    cand_idx[pos]  = global_idx;\n                    cand_dist[pos] = d2;\n                }\n            }\n        }\n\n        __syncthreads(); // ensure all warps finished with tile before overwriting it\n    }\n\n    // Final flush for remaining candidates (if any)\n    if (warp_active) {\n        int final_count = 0;\n        if (lane == 0) final_count = cand_count;\n        final_count = __shfl_sync(full_mask(), final_count, 0);\n\n        if (final_count > 0) {\n            // Pad the remaining candidate slots with +INF\n            for (int i = lane; i < k; i += warpSize) {\n                if (i >= final_count) {\n                    cand_idx[i]  = -1;\n                    cand_dist[i] = CUDART_INF_F;\n                }\n            }\n            __syncwarp();\n\n            flush_and_merge_warp(k, res_idx, res_dist, cand_idx, cand_dist);\n            if (lane == 0) cand_count = 0;\n            __syncwarp();\n        }\n\n        // Write the final k nearest neighbors for this query\n        for (int i = lane; i < k; i += warpSize) {\n            int out_index = warp_g * k + i;\n            result[out_index].idx  = res_idx[i];\n            result[out_index].dist = res_dist[i];\n        }\n    }\n}\n\n// Host entry point. Chooses hyper-parameters and launches the kernel.\n// The result pointer is std::pair<int,float>* on the host; we reinterpret it as IdxDist* for device.\n// It assumes all device memory allocations and copies are managed by the caller.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data, int data_count,\n             std::pair<int, float> *result, int k) {\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Query available dynamic shared memory (opt-in) and choose launch parameters.\n    int device = 0;\n    cudaGetDevice(&device);\n\n    int maxOptinSmem = 0;\n    cudaDeviceGetAttribute(&maxOptinSmem, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    if (maxOptinSmem <= 0) {\n        // Fallback to legacy limit if opt-in not supported\n        cudaDeviceGetAttribute(&maxOptinSmem, cudaDevAttrMaxSharedMemoryPerBlock, device);\n    }\n\n    // Choose warps per block (default 8) and adjust if needed to fit in shared memory.\n    int warps_per_block = 8; // 8 warps -> 256 threads per block, good balance on A100/H100\n    const int warp_threads = 32;\n    const int threads_per_block = warps_per_block * warp_threads;\n\n    // Per-warp shared memory usage:\n    // res_idx[k] + res_dist[k] + cand_idx[k] + cand_dist[k] = 4*k + 4*k + 4*k + 4*k bytes = 16*k bytes\n    const size_t per_warp_bytes = static_cast<size_t>(16) * static_cast<size_t>(k);\n    // Additional per-warp small overhead for counters (int)\n    const size_t per_warp_overhead = sizeof(int);\n\n    // Reserve some safety margin for alignment\n    const size_t safety_bytes = 256;\n\n    // Compute per-block bytes for per-warp buffers\n    auto block_perwarp_bytes = [&](int warps) -> size_t {\n        return warps * (per_warp_bytes + per_warp_overhead);\n    };\n\n    // Choose tile_points to maximize cache reuse while fitting in shared memory:\n    // tile_bytes = tile_points * sizeof(float2)\n    // total_smem = tile_bytes + block_perwarp_bytes + safety <= maxOptinSmem\n    int tile_points = 4096; // start with 4K points (32 KB), a good default\n    while (true) {\n        size_t tile_bytes = static_cast<size_t>(tile_points) * sizeof(float2);\n        size_t total = tile_bytes + block_perwarp_bytes(warps_per_block) + safety_bytes;\n        if (total <= static_cast<size_t>(maxOptinSmem)) break;\n\n        // If too big, reduce tile size first, but keep it at least 1024\n        if (tile_points > 1024) {\n            tile_points = max(1024, tile_points / 2);\n            continue;\n        }\n        // If still too big, reduce warps per block and reset tile size\n        if (warps_per_block > 1) {\n            warps_per_block /= 2;\n            tile_points = 4096; // try again with larger tile\n            continue;\n        }\n        // As a last resort, reduce tile even further\n        while (true) {\n            size_t total2 = static_cast<size_t>(tile_points) * sizeof(float2) + block_perwarp_bytes(warps_per_block) + safety_bytes;\n            if (total2 <= static_cast<size_t>(maxOptinSmem) || tile_points <= 32) break;\n            tile_points /= 2;\n        }\n        break;\n    }\n\n    // Round tile_points up/down to a multiple of 32 for better coalescing\n    tile_points = (tile_points / 32) * 32;\n    if (tile_points < 32) tile_points = 32;\n\n    // Compute dynamic shared memory size\n    size_t dyn_smem_bytes = sizeof(float2) * static_cast<size_t>(tile_points)\n                          + block_perwarp_bytes(warps_per_block)\n                          + safety_bytes;\n\n    // Opt-in to use large dynamic shared memory if necessary\n    cudaFuncSetAttribute(knn_warp_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize,\n                         static_cast<int>(dyn_smem_bytes));\n\n    // Compute grid size: one warp per query\n    int warps_total = (query_count + 0) / 1; // one warp per query\n    int blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Launch kernel\n    IdxDist* out = reinterpret_cast<IdxDist*>(result);\n    dim3 block(threads_per_block);\n    dim3 grid(blocks);\n\n    knn_warp_kernel<<<grid, block, dyn_smem_bytes>>>(\n        query, query_count, data, data_count, out, k, tile_points, warps_per_block\n    );\n\n    // Users of this function are expected to synchronize or use the stream they provided externally.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 766, "completion_tokens": 23188, "total_tokens": 23954, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 19072, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db8120c8190b7d3910dda47b302", "custom_id": "knn05-07", "response": {"status_code": 200, "request_id": "f7917a1ef0c271498b04f97c143a984d", "body": {"id": "chatcmpl-C4sj3tyt4Q42JPPknYKLnfDfUW0hK", "object": "chat.completion", "created": 1755279713, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n#include <cfloat>\n\n// Optimized CUDA implementation of k-NN (squared Euclidean distance) for 2D points.\n// Design decisions, invariants, and algorithmic steps are thoroughly documented inline.\n//\n// High-level summary:\n//\n// - One warp (32 threads) processes one query point.\n// - The block loads the data points into shared memory in batches (\"tiles\") so that\n//   all warps in the block can reuse the cached points to compute distances.\n// - Each warp maintains two private shared-memory arrays of size k for its query:\n//     (a) \"best\"  (intermediate top-k results) at combined[0..k-1].\n//     (b) \"cands\" (candidate buffer)           at combined[k..2k-1].\n//   So per warp we allocate combined[2*k] elements (distance,index).\n// - When the candidate buffer fills (contains k elements), the warp merges \"best\"\n//   and \"cands\" by sorting the 2k elements in-place using a warp-parallel bitonic sort\n//   over shared memory, then keeps the first k as the new \"best\" and resets \"cands\".\n// - After the last batch, any remaining candidates are merged.\n// - The kernel does not allocate device memory; only uses shared memory (dynamic).\n// - k is a power of two in [32, 1024]. We rely on 2k also being a power-of-two,\n//   which is required by the bitonic network.\n// - We choose 8 warps per block (256 threads) by default, appropriate for A100/H100.\n//   Dynamic shared memory usage is computed at launch to fit device limits and k.\n//\n// Memory layout in dynamic shared memory per block:\n//\n//   [ tile float2 array of size tile_capacity ]\n//   [ combined arrays per warp (warp_count * 2*k elements of PairDfIdx) ]\n//   [ candidate counters per warp (warp_count ints) ]\n//\n// Where PairDfIdx is 8 bytes (float distance, int index).\n//\n// Notes on synchronization:\n// - __syncthreads() is used to coordinate block-wide tile loads.\n// - Within a warp, __syncwarp() is used to coordinate updates to per-warp shared arrays.\n// - Warp-level ballot/popc/shfl are used to: compact accepted candidates into the buffer,\n//   assign write positions without atomics, and coordinate merging when the buffer is full.\n//\n// The output array 'result' uses std::pair<int,float>, which we treat as a POD-like\n// pair of two 4-byte fields (index, squared distance). We map it to a simple struct\n// PairIF with identical layout and write to it from device code.\n\nstatic_assert(sizeof(std::pair<int,float>) == 8, \"std::pair<int,float> is expected to be 8 bytes\");\n\nstruct PairIF { int first; float second; }; // must match std::pair<int,float> layout\n\n// Internal pair type holding (distance, index) for sorting by distance.\nstruct PairDfIdx {\n    float dist;\n    int   idx;\n};\n\n// Return lane id within the warp\n__device__ __forceinline__ int lane_id() {\n    return threadIdx.x & 31;\n}\n\n// Warp-scope bitonic sort (ascending by dist) for an array in shared memory.\n// - 'arr' points to the warp-private array of length 'N' located in shared memory.\n// - 'N' must be a power of two. We use N = 2*k with k in [32, 1024], so N in [64, 2048].\n// - Only the calling warp operates on 'arr'. Multiple warps can call this on disjoint regions.\n__device__ __forceinline__ void warp_bitonic_sort_shared_ascending(PairDfIdx* arr, int N) {\n    const unsigned FULL_MASK = 0xffffffffu;\n    const int lane = lane_id();\n\n    // Classic bitonic network using the \"ixj = i ^ j\" formulation.\n    for (int k = 2; k <= N; k <<= 1) {\n        // For each merge stage, j starts at k/2 and halves to 1\n        for (int j = k >> 1; j > 0; j >>= 1) {\n            // Each lane processes multiple indices 'i' spaced by warp size\n            for (int i = lane; i < N; i += warpSize) {\n                int ixj = i ^ j;\n                if (ixj > i) {\n                    // Determine sort direction for this sequence\n                    bool ascending = ((i & k) == 0);\n                    float ai = arr[i].dist;\n                    float aj = arr[ixj].dist;\n\n                    // Compare and swap based on direction\n                    bool swap_needed = ascending ? (ai > aj) : (ai < aj);\n                    if (swap_needed) {\n                        PairDfIdx tmp = arr[i];\n                        arr[i] = arr[ixj];\n                        arr[ixj] = tmp;\n                    }\n                }\n            }\n            __syncwarp(FULL_MASK);\n        }\n    }\n}\n\n// Merge \"best\" (arr[0..k-1]) with \"cands\" (arr[k..k+cand_count-1]) and keep top-k (smallest distances).\n// This sorts arr[0..2k-1] ascending by distance, after padding unused candidates with +INF.\n// After return: arr[0..k-1] holds the new best, and cand_count is reset to 0.\n__device__ __forceinline__ void warp_merge_topk(PairDfIdx* arr_combined, int k, int warp_local_id, int* s_cand_count) {\n    const unsigned FULL_MASK = 0xffffffffu;\n    const int lane = lane_id();\n\n    int cand_count = 0;\n    if (lane == 0) cand_count = s_cand_count[warp_local_id];\n    cand_count = __shfl_sync(FULL_MASK, cand_count, 0);\n\n    // Pad the unused portion of the candidate buffer with +INF so that sorting is well-defined.\n    for (int i = lane + cand_count; i < k; i += warpSize) {\n        arr_combined[k + i].dist = FLT_MAX;\n        arr_combined[k + i].idx  = -1;\n    }\n    __syncwarp(FULL_MASK);\n\n    // Sort the 2*k elements; ascending order by distance\n    warp_bitonic_sort_shared_ascending(arr_combined, 2 * k);\n    __syncwarp(FULL_MASK);\n\n    // Reset candidate counter\n    if (lane == 0) s_cand_count[warp_local_id] = 0;\n    __syncwarp(FULL_MASK);\n}\n\n// Attempt to append a candidate (dist, idx) to the warp's candidate buffer (arr_combined[k..2k-1]).\n// If the buffer is full, perform a merge and then possibly re-evaluate the candidate against\n// the tightened threshold before retrying. This function is warp-synchronous: all lanes call it\n// with their own (maybe inactive) candidate. It internally compacts active lanes and assigns\n// write offsets without atomics.\n//\n// Arguments:\n//   arr_combined      -> warp's shared memory region of size 2*k: [0..k-1]=best, [k..2k-1]=cands\n//   k                 -> top-k\n//   warp_local_id     -> warp index within block\n//   s_cand_count      -> per-warp candidate counts in shared memory\n//   take              -> whether this lane's candidate initially qualifies (dist < current threshold)\n//   dist, idx         -> candidate distance and index\n__device__ __forceinline__ void warp_append_candidate_with_merge(\n    PairDfIdx* arr_combined, int k, int warp_local_id, int* s_cand_count,\n    bool take, float dist, int idx)\n{\n    const unsigned FULL_MASK = 0xffffffffu;\n    const int lane = lane_id();\n\n    unsigned active = __ballot_sync(FULL_MASK, take);\n    while (active) {\n        // Remaining capacity in the candidate buffer\n        int ccount = 0;\n        if (lane == 0) ccount = s_cand_count[warp_local_id];\n        ccount = __shfl_sync(FULL_MASK, ccount, 0);\n        int remain = k - ccount;\n\n        if (remain <= 0) {\n            // Buffer full: merge best and cands to tighten threshold\n            warp_merge_topk(arr_combined, k, warp_local_id, s_cand_count);\n            // New threshold equals the k-th element's distance after merge (best[k-1])\n            float new_thr = arr_combined[k - 1].dist;\n            // Re-evaluate candidates that weren't inserted yet\n            bool still_take = take && (dist < new_thr);\n            active = __ballot_sync(FULL_MASK, still_take);\n            take = still_take;\n            continue;\n        }\n\n        // Number of active lanes wanting to write; take as many as we can this round\n        int active_count = __popc(active);\n        int takeN = (active_count < remain) ? active_count : remain;\n\n        // Position among active lanes for this lane\n        int pos = __popc(active & ((1u << lane) - 1));\n\n        // Base write index in candidate buffer\n        int base = 0;\n        if (lane == 0) base = s_cand_count[warp_local_id];\n        base = __shfl_sync(FULL_MASK, base, 0);\n\n        // Write candidates (only first 'takeN' active lanes write)\n        if (take && pos < takeN) {\n            arr_combined[k + base + pos].dist = dist;\n            arr_combined[k + base + pos].idx  = idx;\n        }\n        __syncwarp(FULL_MASK);\n\n        // Update count\n        if (lane == 0) s_cand_count[warp_local_id] = base + takeN;\n        __syncwarp(FULL_MASK);\n\n        // If some active lanes weren't written, keep them for the next iteration\n        if (active_count > takeN) {\n            bool keep = take && (pos >= takeN);\n            active = __ballot_sync(FULL_MASK, keep);\n            take = keep;\n        } else {\n            break;\n        }\n    }\n}\n\n// Main kernel. Each warp handles one query. The block processes a set of queries equal to the number of warps per block.\n__global__ void knn_kernel_2d(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data,  int data_count,\n    PairIF* __restrict__ result, int k, int tile_capacity)\n{\n    const unsigned FULL_MASK = 0xffffffffu;\n\n    const int lane = lane_id();\n    const int warp_local_id = threadIdx.x >> 5;\n    const int warps_per_block = blockDim.x >> 5;\n\n    const int query_idx = blockIdx.x * warps_per_block + warp_local_id;\n    if (query_idx >= query_count)\n        return;\n\n    // Dynamic shared memory layout:\n    // [ tile float2 array of length tile_capacity ]\n    // [ per-warp combined arrays (warps_per_block * 2*k of PairDfIdx) ]\n    // [ per-warp candidate counts (warps_per_block ints) ]\n    extern __shared__ unsigned char smem_raw[];\n    float2* tile = reinterpret_cast<float2*>(smem_raw);\n\n    // Pointer to per-warp combined arrays\n    PairDfIdx* combined_all = reinterpret_cast<PairDfIdx*>(tile + tile_capacity);\n    // Pointer to per-warp candidate counters\n    int* s_cand_count = reinterpret_cast<int*>(combined_all + (warps_per_block * 2 * k));\n\n    // This warp's combined array: [0..k-1]=best, [k..2k-1]=cands\n    PairDfIdx* combined = combined_all + warp_local_id * (2 * k);\n    PairDfIdx* best     = combined;         // convenience alias\n    // PairDfIdx* cands = combined + k;     // alias (not strictly necessary as we index combined[k+...])\n\n    // Load the query point; lane 0 loads from global, broadcasts to others\n    float2 q;\n    if (lane == 0) q = query[query_idx];\n    q.x = __shfl_sync(FULL_MASK, q.x, 0);\n    q.y = __shfl_sync(FULL_MASK, q.y, 0);\n\n    // Initialize best array with +INF and indices -1\n    for (int i = lane; i < k; i += warpSize) {\n        best[i].dist = FLT_MAX;\n        best[i].idx  = -1;\n    }\n    // Initialize candidate counter\n    if (lane == 0) s_cand_count[warp_local_id] = 0;\n    __syncwarp(FULL_MASK);\n\n    // Process data in tiles\n    for (int base = 0; base < data_count; base += tile_capacity) {\n        int tile_n = data_count - base;\n        if (tile_n > tile_capacity) tile_n = tile_capacity;\n\n        // Load tile into shared memory (all threads in block cooperate)\n        for (int i = threadIdx.x; i < tile_n; i += blockDim.x) {\n            tile[i] = data[base + i];\n        }\n        __syncthreads(); // ensure tile is fully loaded\n\n        // Each warp computes distances from its query to all points in the tile\n        // and feeds candidates into its shared candidate buffer.\n        // We skip candidates that are not closer than the current k-th best distance.\n        for (int t = lane; t < tile_n; t += warpSize) {\n            float2 p = tile[t];\n            float dx = p.x - q.x;\n            float dy = p.y - q.y;\n            float dist = dx * dx + dy * dy;\n\n            // Read current threshold (k-th best distance); since 'best' remains sorted ascending\n            // after each merge, threshold is best[k-1].dist. It may tighten when merges occur.\n            float thr = best[k - 1].dist;\n\n            bool take = (dist < thr);\n            int  idx  = base + t;\n\n            // Attempt to append this candidate; if buffer is full, it will merge and retry if needed\n            warp_append_candidate_with_merge(combined, k, warp_local_id, s_cand_count, take, dist, idx);\n        }\n\n        __syncwarp(FULL_MASK);\n        __syncthreads(); // allow the block to reuse smem for the next tile\n    }\n\n    // After last batch: merge any remaining candidates\n    int final_count = 0;\n    if (lane == 0) final_count = s_cand_count[warp_local_id];\n    final_count = __shfl_sync(FULL_MASK, final_count, 0);\n    if (final_count > 0) {\n        warp_merge_topk(combined, k, warp_local_id, s_cand_count);\n    }\n    __syncwarp(FULL_MASK);\n\n    // Write out results for this query: best[0..k-1] are sorted ascending by distance.\n    // Row-major storage: for query i, result[i*k + j] holds the j-th nearest neighbor.\n    for (int i = lane; i < k; i += warpSize) {\n        PairIF out;\n        out.first  = best[i].idx;\n        out.second = best[i].dist; // squared distance\n        result[query_idx * k + i] = out;\n    }\n}\n\n// Host interface: run_knn wrapper as requested. It configures launch parameters and dynamic shared memory.\n// - query: device pointer to float2 query points\n// - query_count: number of query points\n// - data: device pointer to float2 data points\n// - data_count: number of data points\n// - result: device pointer to std::pair<int,float> output array of size query_count * k\n// - k: power of two, 32..1024\n//\n// We choose 8 warps per block (256 threads). Dynamic shared memory is sized as:\n//   tile_capacity * sizeof(float2) +\n//   warps_per_block * (2 * k * sizeof(PairDfIdx)) +\n//   warps_per_block * sizeof(int)\n//\n// tile_capacity is chosen to fit within the device's opt-in maximum shared memory per block.\n// We set the kernel's max dynamic shared memory attribute accordingly.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data,  int data_count,\n             std::pair<int, float> *result, int k)\n{\n    // Basic launch config\n    const int warps_per_block = 8;                         // 8 warps -> 256 threads per block\n    const int threads_per_block = warps_per_block * 32;\n\n    // Query how much dynamic shared memory we can use (opt-in)\n    int device = 0;\n    cudaGetDevice(&device);\n    cudaDeviceProp prop{};\n    cudaGetDeviceProperties(&prop, device);\n\n    // Max dynamic shared memory we can request for this kernel\n    int maxDynSmem = prop.sharedMemPerBlockOptin ? prop.sharedMemPerBlockOptin : prop.sharedMemPerBlock;\n\n    // Per-warp shared memory for combined arrays (2*k) of PairDfIdx (8 bytes each) and one int counter\n    const size_t bytes_per_pair = sizeof(PairDfIdx); // 8 bytes\n    const size_t per_warp_combined_bytes = 2ull * static_cast<size_t>(k) * bytes_per_pair;\n    const size_t per_warp_counter_bytes  = sizeof(int);\n    const size_t block_combined_bytes = warps_per_block * (per_warp_combined_bytes + per_warp_counter_bytes);\n\n    // Choose tile_capacity to fit within maxDynSmem\n    int tile_capacity = 0;\n    if (maxDynSmem > static_cast<int>(block_combined_bytes)) {\n        size_t remaining = maxDynSmem - block_combined_bytes;\n        tile_capacity = static_cast<int>(remaining / sizeof(float2));\n    } else {\n        tile_capacity = 0;\n    }\n\n    // Ensure at least one element in the tile (avoid zero shared tile)\n    if (tile_capacity < 1) tile_capacity = 1;\n\n    // To avoid excessively large tiles, optionally cap tile size (e.g., 8192 points)\n    const int tile_cap_upper = 8192;\n    if (tile_capacity > tile_cap_upper) tile_capacity = tile_cap_upper;\n\n    // Compute total dynamic shared memory in bytes we will request\n    const size_t dynamic_smem_bytes =\n        static_cast<size_t>(tile_capacity) * sizeof(float2) + block_combined_bytes;\n\n    // Set the kernel attribute to allow this much dynamic shared memory (clamped to device maximum)\n    int attr_value = static_cast<int>(dynamic_smem_bytes);\n    if (attr_value > maxDynSmem) attr_value = maxDynSmem;\n    cudaFuncSetAttribute(knn_kernel_2d, cudaFuncAttributeMaxDynamicSharedMemorySize, attr_value);\n\n    // Prefer larger shared memory carveout if supported (helps for smem-heavy kernels)\n    cudaFuncSetAttribute(knn_kernel_2d, cudaFuncAttributePreferredSharedMemoryCarveout, 100);\n\n    // Grid size: one warp per query; each block has 'warps_per_block' warps\n    const int total_warps = (query_count + 0) / 1; // just for clarity\n    const int blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Launch kernel\n    PairIF* result_as_pairif = reinterpret_cast<PairIF*>(result);\n    knn_kernel_2d<<<blocks, threads_per_block, dynamic_smem_bytes>>>(\n        query, query_count, data, data_count, result_as_pairif, k, tile_capacity);\n\n    // The caller is assumed to handle stream synchronization or cudaDeviceSynchronize as needed.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 766, "completion_tokens": 24298, "total_tokens": 25064, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 20032, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db847808190b72f342897407bba", "custom_id": "knn05-08", "response": {"status_code": 200, "request_id": "97742a6d1ab1dea295dc01f7cbecd0a4", "body": {"id": "chatcmpl-C4snXj6uPm3xfaREtbLfyDx4s33V3", "object": "chat.completion", "created": 1755279991, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// This implementation assigns one warp (32 threads) per query.\n// It processes the data points in tiles loaded into shared memory,\n// maintains an intermediate top-k result per query in registers (distributed across warp lanes),\n// buffers up to k candidate neighbors per warp in shared memory, and whenever the\n// buffer is full (or at the end of processing), it sorts the buffer and merges it\n// with the intermediate result using a parallel merge-path algorithm.\n// The output contains indices of the nearest data points and their distances (squared L2).\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Choose a conservative configuration that fits A100's 164 KB shared memory limit for all k in [32, 1024].\n// - Warps per block: 4 (128 threads per block)\n// - Per-warp shared memory: 3 * k pairs (dist+idx) = 24*k bytes\n// - Tile size (data points per block): 6144 float2 = 48 KB\n// Total: 4*24k + 48KB <= 164KB for k <= 1024\nstatic constexpr int WARPS_PER_BLOCK = 4;\nstatic constexpr int THREADS_PER_BLOCK = WARPS_PER_BLOCK * WARP_SIZE;\nstatic constexpr int TILE_POINTS = 6144; // 48 KB of float2\n\nstruct PairIF { int first; float second; };\n\n__device__ __forceinline__ unsigned lane_id() {\n    return threadIdx.x & (WARP_SIZE - 1);\n}\n__device__ __forceinline__ unsigned warp_id_in_block() {\n    return threadIdx.x >> 5;\n}\n__device__ __forceinline__ unsigned lane_mask_lt() {\n    // Mask with bits set for lanes less than current lane\n    return (1u << lane_id()) - 1u;\n}\n\n__device__ __forceinline__ float2 ld_float2(const float2* ptr) {\n    // Use regular global load; modern GPUs will cache automatically.\n    return *ptr;\n}\n\n__device__ __forceinline__ void warp_sync() {\n#if __CUDA_ARCH__ >= 700\n    __syncwarp();\n#else\n    // For older arch, no-op; but our target hardware is H100/A100 (sm80+)\n#endif\n}\n\n// Bitonic sort for pairs (dist, idx) stored in shared memory.\n// N must be a power of two. Sorting order: ascending by distance.\n__device__ void bitonic_sort_pairs_shared(float* dist, int* idx, int N) {\n    const unsigned full_mask = 0xFFFFFFFFu;\n    unsigned lane = lane_id();\n    for (int size = 2; size <= N; size <<= 1) {\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            for (int i = lane; i < N; i += WARP_SIZE) {\n                int j = i ^ stride;\n                if (j > i) {\n                    bool ascending = ((i & size) == 0);\n                    float di = dist[i];\n                    float dj = dist[j];\n                    int ii = idx[i];\n                    int ij = idx[j];\n                    bool cond = ascending ? (di > dj) : (di < dj);\n                    if (cond) {\n                        dist[i] = dj; dist[j] = di;\n                        idx[i]  = ij; idx[j] = ii;\n                    }\n                }\n            }\n            warp_sync();\n        }\n    }\n    (void)full_mask;\n}\n\n// Merge-path partition search: find a where\n// A[a-1] <= B[b] and B[b-1] <= A[a], with a + b = diag.\n// Arrays are sorted ascending. Uses sentinels -INF/+INF for out-of-bounds.\n__device__ int merge_path_search(const float* A, int nA, const float* B, int nB, int diag) {\n    int low = max(0, diag - nB);\n    int high = min(diag, nA);\n    while (low <= high) {\n        int a = (low + high) >> 1;\n        int b = diag - a;\n        float A_left = (a > 0)    ? A[a - 1] : -CUDART_INF_F;\n        float A_right = (a < nA)  ? A[a]     : CUDART_INF_F;\n        float B_left = (b > 0)    ? B[b - 1] : -CUDART_INF_F;\n        float B_right = (b < nB)  ? B[b]     : CUDART_INF_F;\n        if (A_left <= B_right && B_left <= A_right) {\n            return a;\n        } else if (A_left > B_right) {\n            high = a - 1;\n        } else {\n            low = a + 1;\n        }\n    }\n    // Fallback; should not reach here for valid inputs\n    return min(max(0, low), nA);\n}\n\n// Merge top-k of two sorted arrays A (nA=k) and B (nB<=k) into output (k elements).\n// A_in_dist/idx: input A (size k)\n// B_dist/idx:    input B (size k, with +INF padding in tail if nB < k)\n// out_dist/idx:  output buffer (size k)\n// Uses one warp cooperatively; each lane merges CHUNK = k / 32 outputs.\n__device__ void merge_topk_warp(const float* A_in_dist, const int* A_in_idx,\n                                const float* B_dist, const int* B_idx,\n                                float* out_dist, int* out_idx,\n                                int k) {\n    const int lane = lane_id();\n    const int CHUNK = k / WARP_SIZE;\n    const int out_begin = lane * CHUNK;\n    const int out_end   = out_begin + CHUNK;\n\n    // Compute partitions for this lane\n    int a_begin = merge_path_search(A_in_dist, k, B_dist, k, out_begin);\n    int b_begin = out_begin - a_begin;\n    int a_end   = merge_path_search(A_in_dist, k, B_dist, k, out_end);\n    int b_end   = out_end - a_end;\n\n    int i = a_begin;\n    int j = b_begin;\n    for (int out_pos = out_begin; out_pos < out_end; ++out_pos) {\n        // Choose the smaller current element\n        float a_val = (i < a_end) ? A_in_dist[i] : CUDART_INF_F;\n        float b_val = (j < b_end) ? B_dist[j]    : CUDART_INF_F;\n        if (a_val <= b_val) {\n            out_dist[out_pos] = a_val;\n            out_idx [out_pos] = A_in_idx[i];\n            ++i;\n        } else {\n            out_dist[out_pos] = b_val;\n            out_idx [out_pos] = B_idx[j];\n            ++j;\n        }\n    }\n    warp_sync();\n}\n\n// Kernel: one warp per query.\n__global__ void knn_kernel(const float2* __restrict__ query,\n                           int query_count,\n                           const float2* __restrict__ data,\n                           int data_count,\n                           PairIF* __restrict__ result,\n                           int k) {\n    extern __shared__ unsigned char shared_bytes[];\n    // Shared memory layout:\n    // [0 .. TILE_POINTS-1] float2 tile_data\n    // Then per-warp segments, each consisting of:\n    //   cand_dist[k], cand_idx[k],\n    //   A_in_dist[k], A_in_idx[k],\n    //   out_dist[k],  out_idx[k]\n    // Then cand_count[WARPS_PER_BLOCK]\n    float2* tile_points = reinterpret_cast<float2*>(shared_bytes);\n    size_t tile_bytes = size_t(TILE_POINTS) * sizeof(float2);\n    unsigned char* p = shared_bytes + tile_bytes;\n\n    size_t per_warp_bytes = size_t(k) * (sizeof(float) + sizeof(int)) * 3; // 3 arrays of pairs\n    unsigned wid = warp_id_in_block();\n    unsigned lane = lane_id();\n    unsigned full_mask = 0xFFFFFFFFu;\n\n    unsigned char* warp_base = p + wid * per_warp_bytes;\n\n    float* cand_dist = reinterpret_cast<float*>(warp_base);\n    int*   cand_idx  = reinterpret_cast<int*>(warp_base + size_t(k) * sizeof(float));\n\n    float* A_in_dist = reinterpret_cast<float*>(warp_base + size_t(k) * (sizeof(float) + sizeof(int)));\n    int*   A_in_idx  = reinterpret_cast<int*>(reinterpret_cast<unsigned char*>(A_in_dist) + size_t(k) * sizeof(float));\n\n    float* out_dist  = reinterpret_cast<float*>(reinterpret_cast<unsigned char*>(A_in_idx) + size_t(k) * sizeof(int));\n    int*   out_idx   = reinterpret_cast<int*>(reinterpret_cast<unsigned char*>(out_dist) + size_t(k) * sizeof(float));\n\n    // cand_count array placed after all per-warp buffers\n    int* cand_count_base = reinterpret_cast<int*>(p + WARPS_PER_BLOCK * per_warp_bytes);\n    int& cand_count = cand_count_base[wid];\n\n    // Assign one warp per query\n    int query_index = int(blockIdx.x) * WARPS_PER_BLOCK + int(wid);\n    bool warp_active = (query_index < query_count);\n\n    // Initialize warp-private intermediate result in registers (distributed across lanes)\n    const int CHUNK = k / WARP_SIZE;\n    float res_dist[32]; // maximum CHUNK is 32; we will use only first CHUNK\n    int   res_idx[32];\n    for (int i = 0; i < CHUNK; ++i) {\n        res_dist[i] = CUDART_INF_F;\n        res_idx[i]  = -1;\n    }\n    float kth_threshold = CUDART_INF_F;\n\n    // Initialize candidate buffer in shared memory for this warp\n    if (lane == 0) cand_count = 0;\n    warp_sync();\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        cand_dist[i] = CUDART_INF_F;\n        cand_idx [i] = -1;\n    }\n    warp_sync();\n\n    // Load query point for this warp\n    float qx = 0.0f, qy = 0.0f;\n    if (warp_active) {\n        float2 q = query[query_index];\n        qx = q.x; qy = q.y;\n    }\n    // Broadcast qx, qy within the warp to ensure they are defined in all lanes\n    qx = __shfl_sync(full_mask, qx, 0);\n    qy = __shfl_sync(full_mask, qy, 0);\n\n    // Helper lambdas capture by reference 'cand_count', 'cand_dist', 'cand_idx', 'A_in_*', 'out_*', 'res_*', 'k', 'kth_threshold'\n    auto sort_candidates_and_merge = [&]() {\n        // Sort candidate buffer (size k with +INF padding) in shared memory\n        bitonic_sort_pairs_shared(cand_dist, cand_idx, k);\n\n        // Copy current intermediate result from registers to shared memory A_in_*\n        // Global order corresponds to concatenation of lanes in lane order.\n        int base = int(lane) * CHUNK;\n        for (int i = 0; i < CHUNK; ++i) {\n            A_in_dist[base + i] = res_dist[i];\n            A_in_idx [base + i] = res_idx[i];\n        }\n        warp_sync();\n\n        // Merge top-k of A_in_* and cand_* into out_*\n        merge_topk_warp(A_in_dist, A_in_idx, cand_dist, cand_idx, out_dist, out_idx, k);\n\n        // Copy merged result back into registers\n        for (int i = 0; i < CHUNK; ++i) {\n            res_dist[i] = out_dist[base + i];\n            res_idx [i] = out_idx [base + i];\n        }\n        warp_sync();\n\n        // Update kth_threshold (k-th nearest distance), which is the last element of the global list\n        float tail = res_dist[CHUNK - 1];\n        kth_threshold = __shfl_sync(full_mask, tail, WARP_SIZE - 1);\n\n        // Reset candidate buffer\n        if (lane == 0) cand_count = 0;\n        warp_sync();\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            cand_dist[i] = CUDART_INF_F;\n            cand_idx [i] = -1;\n        }\n        warp_sync();\n    };\n\n    auto try_append_candidate = [&](float d, int idx) {\n        // Try to append this candidate, merging as needed; re-evaluate against threshold after each merge.\n        while (true) {\n            // If not better than current k-th, skip\n            if (!(d < kth_threshold)) break;\n\n            unsigned mask = __ballot_sync(full_mask, d < kth_threshold);\n            int num = __popc(mask);\n            if (num == 0) break; // nobody has a valid candidate now\n\n            // Reserve space or merge\n            int start = 0;\n            if (lane == 0) start = cand_count;\n            start = __shfl_sync(full_mask, start, 0);\n            int space = k - start;\n            if (space >= num) {\n                // Append all valid candidates\n                int rank = __popc(mask & lane_mask_lt());\n                int pos = start + rank;\n                if (d < kth_threshold) {\n                    cand_dist[pos] = d;\n                    cand_idx [pos] = idx;\n                }\n                if (lane == 0) cand_count = start + num;\n                warp_sync();\n                break;\n            } else {\n                // Not enough space: merge current buffer with intermediate result\n                sort_candidates_and_merge();\n                // After merge, kth_threshold is updated; loop to re-evaluate acceptance\n            }\n        }\n    };\n\n    // Iterate over data in tiles loaded into shared memory by entire block\n    for (int base = 0; base < data_count; base += TILE_POINTS) {\n        int tile_count = min(TILE_POINTS, data_count - base);\n\n        // Load tile\n        for (int t = threadIdx.x; t < tile_count; t += blockDim.x) {\n            tile_points[t] = ld_float2(data + base + t);\n        }\n        __syncthreads();\n\n        // Each active warp processes the tile\n        if (warp_active) {\n            for (int j = lane; j < tile_count; j += WARP_SIZE) {\n                float2 p = tile_points[j];\n                float dx = qx - p.x;\n                float dy = qy - p.y;\n                float d = fmaf(dx, dx, dy * dy); // squared L2\n                int   idx = base + j;\n\n                // Try to append this candidate into the warp's buffer\n                try_append_candidate(d, idx);\n            }\n        }\n\n        __syncthreads(); // ensure all warps finished using tile before it's overwritten\n    }\n\n    // After last tile, flush remaining candidates\n    if (warp_active) {\n        int count = 0;\n        if (lane == 0) count = cand_count;\n        count = __shfl_sync(full_mask, count, 0);\n        if (count > 0) {\n            sort_candidates_and_merge();\n        }\n\n        // Write out final top-k for this query in row-major order\n        int base_out = query_index * k;\n        int global_base = int(lane) * CHUNK;\n        for (int i = 0; i < CHUNK; ++i) {\n            int out_pos = base_out + global_base + i;\n            result[out_pos].first  = res_idx[i];\n            result[out_pos].second = res_dist[i];\n        }\n    }\n}\n\n// Host function: launch the kernel with appropriate configuration and shared memory size.\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    dim3 block(THREADS_PER_BLOCK);\n    dim3 grid((query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK);\n\n    // Dynamic shared memory size:\n    // Tile: TILE_POINTS * sizeof(float2)\n    // Per-warp: 3 * k * (sizeof(float) + sizeof(int))\n    // Counts: WARPS_PER_BLOCK * sizeof(int)\n    size_t tile_bytes = size_t(TILE_POINTS) * sizeof(float2);\n    size_t per_warp_bytes = size_t(k) * (sizeof(float) + sizeof(int)) * 3;\n    size_t smem_bytes = tile_bytes + WARPS_PER_BLOCK * per_warp_bytes + WARPS_PER_BLOCK * sizeof(int);\n\n    // Opt-in to large dynamic shared memory if necessary\n    cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, int(smem_bytes));\n\n    PairIF* out = reinterpret_cast<PairIF*>(result);\n    knn_kernel<<<grid, block, smem_bytes>>>(query, query_count, data, data_count, out, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 766, "completion_tokens": 21952, "total_tokens": 22718, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 18240, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db88a808190ba0c518f5cd58f20", "custom_id": "knn05-09", "response": {"status_code": 200, "request_id": "1a19e92218cc431489cafdc0badec6a2", "body": {"id": "chatcmpl-C4sjEftEA6pxU55kCUkw2Qpkbl8XU", "object": "chat.completion", "created": 1755279724, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda.h>\n#include <cuda_runtime.h>\n#include <utility>\n#include <cfloat>\n#include <cmath>\n\n// This implementation assigns a single warp (32 threads) to each query. The kernel iterates\n// over the data points in large batches cached in shared memory. Each warp maintains:\n// - A \"private\" intermediate result (top-k indices and distances) per query.\n// - A per-warp shared-memory candidate buffer of size k (indices and distances).\n// The warp processes the cached batch in tiles of warp-size, filters candidates using the\n// current threshold (k-th smallest distance in the intermediate result), and appends them\n// into the candidate buffer. When the candidate buffer becomes full, it is sorted and\n// merged (in parallel using the warp) with the intermediate result. After all batches,\n// any remaining candidates are merged as well.\n//\n// The intermediate result and candidate buffer both use shared memory so that multiple\n// threads in a warp can cooperate during sort/merge. No additional device allocations\n// are performed. The kernel uses dynamic shared memory sized by the host launcher.\n//\n// Requirements observed:\n// - One warp per query (32 threads).\n// - Batch-based processing; each batch is cached by the whole block.\n// - Per-query buffer for k candidates in shared memory.\n// - When the buffer is full, merge candidates with the intermediate result using the warp.\n// - After last batch, merge any remaining candidates.\n// - k is a power of two, 32 <= k <= 1024.\n//\n// Notes on shared memory usage per block (WARPS_PER_BLOCK = 4):\n// - Two arrays per warp for intermediate (idx/dist) and two for candidates => 16*k bytes/warp.\n// - For k=1024: 16*1024 = 16384 bytes per warp => 65536 bytes for 4 warps.\n// - Remaining shared memory is used for the batch cache of float2 points.\n// - Dynamic shared memory size is selected by the host to stay within the device limit\n//   (opt-in maximum, e.g., ~164KB on A100, higher on H100).\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 4\n#endif\n\n// Utility: lane id and warp id within block\n__device__ __forceinline__ int lane_id() { return threadIdx.x & (WARP_SIZE - 1); }\n__device__ __forceinline__ int warp_id_in_block() { return threadIdx.x >> 5; }\n\n// Squared Euclidean distance between 2D points\n__device__ __forceinline__ float l2_sq_dist(const float2 a, const float2 b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    // fmaf(dx, dx, dy*dy) may fuse multiply-add on capable architectures\n    return fmaf(dx, dx, dy * dy);\n}\n\n// In-warp bitonic sort on pairs (dist, idx) stored in shared memory.\n// 'n' must be a power of two. This sorts ascending by distance.\n// All 32 threads in the warp cooperate; the arrays are per-warp disjoint regions in smem.\n__device__ void warp_bitonic_sort_pairs(float* dist, int* idx, int n, unsigned mask) {\n    int lane = lane_id();\n\n    // Standard bitonic sorting network over indices 0..n-1\n    for (int size = 2; size <= n; size <<= 1) {\n        // For each stage, perform log2(size) passes with decreasing stride\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            __syncwarp(mask);\n            // Each lane processes multiple indices i strided by warp size\n            for (int i = lane; i < n; i += WARP_SIZE) {\n                int j = i ^ stride;\n                if (j > i && j < n) {\n                    bool up = ((i & size) == 0); // ascending for this subsequence\n                    float di = dist[i];\n                    float dj = dist[j];\n                    int ii = idx[i];\n                    int ij = idx[j];\n\n                    // Compare-and-swap depending on desired order\n                    if (up) {\n                        if (di > dj) {\n                            dist[i] = dj; dist[j] = di;\n                            idx[i] = ij; idx[j] = ii;\n                        }\n                    } else {\n                        if (di < dj) {\n                            dist[i] = dj; dist[j] = di;\n                            idx[i] = ij; idx[j] = ii;\n                        }\n                    }\n                }\n            }\n        }\n    }\n    __syncwarp(mask);\n}\n\n// Merge-path search to partition the merge of A and B at diagonal 'diag'.\n// Both A and B are ascending arrays of length lenA=lenB=k.\n// Returns (i, j) such that i+j = diag and the merge path invariants hold.\n// This is used to parallelize the merge across warp lanes.\n__device__ __forceinline__ int2 merge_path_search(const float* A, int lenA,\n                                                  const float* B, int lenB,\n                                                  int diag) {\n    int low = max(0, diag - lenB);\n    int high = min(diag, lenA);\n\n    while (low <= high) {\n        int i = (low + high) >> 1;\n        int j = diag - i;\n\n        float A_im1 = (i > 0)     ? A[i - 1] : -CUDART_INF_F;\n        float B_j   = (j < lenB)  ? B[j]     :  CUDART_INF_F;\n\n        if (A_im1 > B_j) {\n            high = i - 1;\n        } else {\n            float B_jm1 = (j > 0)    ? B[j - 1] : -CUDART_INF_F;\n            float A_i   = (i < lenA) ? A[i]     :  CUDART_INF_F;\n            if (B_jm1 > A_i) {\n                low = i + 1;\n            } else {\n                low = i;\n                break;\n            }\n        }\n    }\n\n    int i = min(max(0, low), lenA);\n    int j = diag - i;\n    return make_int2(i, j);\n}\n\n// Merge the two ascending arrays A and B (length k each) into the first k elements of Out.\n// The output is the k smallest elements of the 2k-length union.\n// All 32 lanes in the warp cooperate, each lane produces a contiguous subrange.\n// Out can alias A or B; the function reads only from A and B, writes to Out.\n__device__ void warp_merge_topk_ascending(const float* A_dist, const int* A_idx,\n                                          const float* B_dist, const int* B_idx,\n                                          float* Out_dist, int* Out_idx,\n                                          int k, unsigned mask) {\n    int lane = lane_id();\n    // Split the first 'k' outputs evenly among warp lanes\n    int outputs_per_lane = (k + WARP_SIZE - 1) / WARP_SIZE;\n    int out_start = lane * outputs_per_lane;\n    int out_end = min(out_start + outputs_per_lane, k);\n    if (out_start >= out_end) {\n        __syncwarp(mask);\n        return;\n    }\n\n    // Find (i,j) starting positions for out_start\n    int2 start = merge_path_search(A_dist, k, B_dist, k, out_start);\n    int i = start.x;\n    int j = start.y;\n    int out_pos = out_start;\n\n    // Merge until out_end\n    while (out_pos < out_end) {\n        float a = (i < k) ? A_dist[i] : CUDART_INF_F;\n        float b = (j < k) ? B_dist[j] : CUDART_INF_F;\n\n        if (a <= b) {\n            Out_dist[out_pos] = a;\n            Out_idx[out_pos] = A_idx[i];\n            ++i;\n        } else {\n            Out_dist[out_pos] = b;\n            Out_idx[out_pos] = B_idx[j];\n            ++j;\n        }\n        ++out_pos;\n    }\n    __syncwarp(mask);\n}\n\n// Flush the per-warp candidate buffer by sorting it and merging with the intermediate result.\n// After merge, the new intermediate result is stored in A (by swapping pointers if needed).\n// candCount is reset to 0 and the threshold (k-th distance) is updated.\n// A_dist/A_idx and B_dist/B_idx are references (pointers passed by reference) so we can swap them.\n__device__ void warp_flush_and_merge(float*& A_dist, int*& A_idx,\n                                     float*& B_dist, int*& B_idx,\n                                     volatile int* candCountPtr,\n                                     volatile float* thrPtr,\n                                     int k, unsigned mask) {\n    // Read candidate count (lane 0) and broadcast\n    int lane = lane_id();\n    int candCount = 0;\n    if (lane == 0) candCount = *candCountPtr;\n    candCount = __shfl_sync(mask, candCount, 0);\n\n    // Pad remaining candidate slots with +inf so we can sort a full power-of-two length\n    for (int i = lane + candCount; i < k; i += WARP_SIZE) {\n        B_dist[i] = CUDART_INF_F;\n        B_idx[i] = -1;\n    }\n    __syncwarp(mask);\n\n    // Sort candidates ascending by distance\n    warp_bitonic_sort_pairs(B_dist, B_idx, k, mask);\n\n    // Merge A and B into the first k outputs placed into B, then swap A<->B\n    warp_merge_topk_ascending(A_dist, A_idx, B_dist, B_idx, B_dist, B_idx, k, mask);\n    __syncwarp(mask);\n\n    // Swap pointers so that A holds the new intermediate result\n    if (lane == 0) {\n        float* tmpD = A_dist; A_dist = B_dist; B_dist = tmpD;\n        int*   tmpI = A_idx;  A_idx  = B_idx;  B_idx  = tmpI;\n    }\n    // Broadcast the swapped pointers within the warp\n    A_dist = (float*)__shfl_sync(mask, (unsigned long long)A_dist, 0);\n    A_idx  = (int*)__shfl_sync(mask, (unsigned long long)A_idx, 0);\n    B_dist = (float*)__shfl_sync(mask, (unsigned long long)B_dist, 0);\n    B_idx  = (int*)__shfl_sync(mask, (unsigned long long)B_idx, 0);\n    __syncwarp(mask);\n\n    // Update threshold (k-th smallest, i.e., last element in ascending array)\n    float new_thr = 0.0f;\n    if (lane == 0) new_thr = A_dist[k - 1];\n    new_thr = __shfl_sync(mask, new_thr, 0);\n    if (lane == 0) {\n        *thrPtr = new_thr;\n        *candCountPtr = 0;\n    }\n    __syncwarp(mask);\n}\n\n// Kernel: one warp per query\n__global__ void knn_kernel(const float2* __restrict__ query, int query_count,\n                           const float2* __restrict__ data, int data_count,\n                           std::pair<int, float>* __restrict__ result,\n                           int k, int batch_size) {\n    extern __shared__ unsigned char smem[];\n    const unsigned full_mask = 0xFFFFFFFFu;\n\n    // Dynamic shared memory layout:\n    // [0, batch_size * sizeof(float2)) -> cached data points\n    // Then per-warp arrays laid out contiguously:\n    // - A_idx [WARPS_PER_BLOCK * k]\n    // - A_dist[WARPS_PER_BLOCK * k]\n    // - B_idx [WARPS_PER_BLOCK * k]\n    // - B_dist[WARPS_PER_BLOCK * k]\n    // - candCount[WARPS_PER_BLOCK]\n    // - thr[WARPS_PER_BLOCK]\n\n    size_t offset = 0;\n    auto smem_align = [&](size_t a) {\n        offset = (offset + a - 1) & ~(a - 1);\n    };\n    auto alloc = [&](size_t bytes) -> unsigned char* {\n        unsigned char* ptr = smem + offset;\n        offset += bytes;\n        return ptr;\n    };\n\n    smem_align(alignof(float2));\n    float2* sh_points = reinterpret_cast<float2*>(alloc((size_t)batch_size * sizeof(float2)));\n\n    smem_align(alignof(int));\n    int* sh_A_idx_all = reinterpret_cast<int*>(alloc((size_t)WARPS_PER_BLOCK * k * sizeof(int)));\n    smem_align(alignof(float));\n    float* sh_A_dist_all = reinterpret_cast<float*>(alloc((size_t)WARPS_PER_BLOCK * k * sizeof(float)));\n    smem_align(alignof(int));\n    int* sh_B_idx_all = reinterpret_cast<int*>(alloc((size_t)WARPS_PER_BLOCK * k * sizeof(int)));\n    smem_align(alignof(float));\n    float* sh_B_dist_all = reinterpret_cast<float*>(alloc((size_t)WARPS_PER_BLOCK * k * sizeof(float)));\n\n    smem_align(alignof(int));\n    int* sh_candCount = reinterpret_cast<int*>(alloc((size_t)WARPS_PER_BLOCK * sizeof(int)));\n    smem_align(alignof(float));\n    float* sh_threshold = reinterpret_cast<float*>(alloc((size_t)WARPS_PER_BLOCK * sizeof(float)));\n\n    const int lane = lane_id();\n    const int warp = warp_id_in_block();\n    const int query_global = blockIdx.x * WARPS_PER_BLOCK + warp;\n    const bool warp_active = (query_global < query_count);\n\n    // Per-warp slice base offset into shared arrays\n    const int warp_offset = warp * k;\n\n    // Pointers to this warp's arrays\n    float* A_dist = sh_A_dist_all + warp_offset;\n    int*   A_idx  = sh_A_idx_all  + warp_offset;\n    float* B_dist = sh_B_dist_all + warp_offset;\n    int*   B_idx  = sh_B_idx_all  + warp_offset;\n\n    // Initialize per-warp intermediate result to +inf and indices to -1\n    if (warp_active) {\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            A_dist[i] = CUDART_INF_F;\n            A_idx[i]  = -1;\n        }\n        if (lane == 0) {\n            sh_candCount[warp] = 0;\n            sh_threshold[warp] = CUDART_INF_F;\n        }\n    }\n    __syncthreads(); // synchronize entire block before starting batch processing\n\n    // Load query point (broadcast within the warp)\n    float qx = 0.0f, qy = 0.0f;\n    if (warp_active) {\n        float2 q = query[query_global];\n        qx = q.x;\n        qy = q.y;\n    }\n    // Broadcast qx, qy to ensure all lanes have valid registers even if only lane 0 loaded\n    qx = __shfl_sync(full_mask, qx, 0);\n    qy = __shfl_sync(full_mask, qy, 0);\n\n    // Process data in batches cached into shared memory\n    for (int base = 0; base < data_count; base += batch_size) {\n        const int this_batch = min(batch_size, data_count - base);\n\n        // All threads in the block cooperatively load the batch into shared memory\n        for (int i = threadIdx.x; i < this_batch; i += blockDim.x) {\n            sh_points[i] = data[base + i];\n        }\n        __syncthreads();\n\n        // Each warp processes the cached points in tiles of 32\n        if (warp_active) {\n            for (int t = 0; t < this_batch; t += WARP_SIZE) {\n                int idx_local = t + lane;\n                bool valid = (idx_local < this_batch);\n\n                // Compute squared distance for this lane's point in the tile\n                float dist = 0.0f;\n                int   gidx = -1;\n                if (valid) {\n                    float2 p = sh_points[idx_local];\n                    dist = l2_sq_dist(p, make_float2(qx, qy));\n                    gidx = base + idx_local;\n                }\n\n                // A lane's candidate is \"pending\" until stored into the candidate buffer or discarded\n                bool pending = valid;\n\n                // Inner loop: while there are pending candidates for this tile, try to push them\n                // into the candidate buffer; if full, flush and merge, then retry with updated threshold.\n                while (true) {\n                    // Read current threshold\n                    float thr = sh_threshold[warp];\n\n                    // Decide which lanes have a qualifying pending candidate\n                    bool qualifies = pending && (dist < thr);\n                    unsigned mask = __ballot_sync(full_mask, qualifies);\n                    int count = __popc(mask);\n\n                    // Load current candidate count (lane 0) and broadcast\n                    int candCount = 0;\n                    if (lane == 0) candCount = sh_candCount[warp];\n                    candCount = __shfl_sync(full_mask, candCount, 0);\n                    int free_slots = k - candCount;\n\n                    if (free_slots == 0) {\n                        // Candidate buffer is full: flush and merge, then retry\n                        warp_flush_and_merge(A_dist, A_idx, B_dist, B_idx,\n                                             &sh_candCount[warp], &sh_threshold[warp], k, full_mask);\n                        // Continue; threshold and candCount updated\n                        continue;\n                    }\n\n                    if (count == 0) {\n                        // No more candidates from this tile under current threshold\n                        break;\n                    }\n\n                    // Compute per-lane prefix among qualifiers\n                    unsigned lane_mask_lt = (1u << lane) - 1u;\n                    int prefix = __popc(mask & lane_mask_lt);\n\n                    if (count <= free_slots) {\n                        // All qualifiers fit into the buffer\n                        int write_base = candCount;\n                        if (qualifies) {\n                            int write_pos = write_base + prefix;\n                            B_dist[write_pos] = dist;\n                            B_idx[write_pos]  = gidx;\n                            // Mark as no longer pending (written)\n                            pending = false;\n                        }\n                        if (lane == 0) sh_candCount[warp] = candCount + count;\n                        __syncwarp(full_mask);\n                        break; // tile consumed under current threshold\n                    } else {\n                        // Partial fill: write only the first 'free_slots' qualifiers\n                        bool push_now = qualifies && (prefix < free_slots);\n                        unsigned mask_partial = __ballot_sync(full_mask, push_now);\n\n                        int write_base = candCount;\n                        int write_prefix = __popc(mask_partial & lane_mask_lt);\n\n                        if (push_now) {\n                            int write_pos = write_base + write_prefix;\n                            B_dist[write_pos] = dist;\n                            B_idx[write_pos]  = gidx;\n                            // Mark as no longer pending (written)\n                            pending = false;\n                        }\n                        if (lane == 0) sh_candCount[warp] = k; // buffer is now full\n                        __syncwarp(full_mask);\n\n                        // Flush and merge, then loop to try pushing remaining pending candidates\n                        warp_flush_and_merge(A_dist, A_idx, B_dist, B_idx,\n                                             &sh_candCount[warp], &sh_threshold[warp], k, full_mask);\n                        // Continue: threshold updated, candCount reset to 0\n                    }\n                } // while pending loop\n            } // for each tile in batch\n        }\n\n        __syncthreads(); // sync before next batch is loaded\n    } // for each batch\n\n    // After processing all batches, flush any remaining candidates\n    if (warp_active) {\n        int candCount = 0;\n        if (lane == 0) candCount = sh_candCount[warp];\n        candCount = __shfl_sync(full_mask, candCount, 0);\n        if (candCount > 0) {\n            warp_flush_and_merge(A_dist, A_idx, B_dist, B_idx,\n                                 &sh_candCount[warp], &sh_threshold[warp], k, full_mask);\n        }\n\n        // Write the final sorted top-k (ascending distances) to the result array\n        // Each lane writes a strided subset for coalescing and to utilize all lanes.\n        size_t out_base = static_cast<size_t>(query_global) * static_cast<size_t>(k);\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            std::pair<int, float> outp;\n            outp.first = A_idx[i];\n            outp.second = A_dist[i];\n            result[out_base + i] = outp;\n        }\n    }\n}\n\n// Host launcher. Chooses block size, grid size, dynamic shared memory size, and batch size.\n// The kernel uses WARPS_PER_BLOCK warps per block (compile-time constant).\nvoid run_knn(const float2* query, int query_count,\n             const float2* data, int data_count,\n             std::pair<int, float>* result, int k) {\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Ensure k is a power of two in [32, 1024]\n    // (Assumed by problem statement; no runtime check needed, but keeping a guard.)\n    if (k < 32 || k > 1024 || (k & (k - 1)) != 0) return;\n\n    // Device attributes for dynamic shared memory\n    int device = 0;\n    cudaGetDevice(&device);\n\n    int max_smem_optin = 0;\n    cudaDeviceGetAttribute(&max_smem_optin, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    if (max_smem_optin <= 0) {\n        // Fallback to default if opt-in not reported; typical default is 48KB.\n        cudaDeviceProp prop{};\n        cudaGetDeviceProperties(&prop, device);\n        max_smem_optin = prop.sharedMemPerBlock; // may be conservative\n    }\n\n    // Opt-in to the maximum dynamic shared memory size\n    cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, max_smem_optin);\n    // Prefer shared memory carveout if supported (100% shared mem)\n    cudaFuncSetAttribute(knn_kernel, cudaFuncAttributePreferredSharedMemoryCarveout, 100);\n\n    const int warps_per_block = WARPS_PER_BLOCK;\n    const int threads_per_block = warps_per_block * WARP_SIZE;\n\n    // Compute per-block shared memory usage for per-warp arrays\n    // Two sets (A and B) of (idx + dist) per warp: 2 * k * (4 + 4) bytes per warp => 16*k per warp\n    size_t per_warp_bytes = (size_t)k * (sizeof(int) + sizeof(float)) * 2; // 8*k bytes per set\n    size_t per_block_arrays = per_warp_bytes * warps_per_block * 2;        // A and B => 16*k*warps bytes\n\n    // Additional small arrays for candCount and threshold per warp\n    size_t per_block_small = warps_per_block * (sizeof(int) + sizeof(float)); // negligible but accounted\n\n    // Remaining shared memory goes to the batch cache (float2 points)\n    size_t overhead = per_block_arrays + per_block_small;\n\n    // Choose batch size as large as possible within the dynamic shared memory limit\n    size_t max_batch_bytes = (overhead < (size_t)max_smem_optin) ? (size_t)max_smem_optin - overhead : 0;\n    // At least 1 point per batch; prefer multiples of warp size for better coalescing\n    int batch_size = 0;\n    if (max_batch_bytes >= sizeof(float2)) {\n        batch_size = static_cast<int>(max_batch_bytes / sizeof(float2));\n        // Round down to nearest multiple of WARP_SIZE to simplify tile processing\n        if (batch_size > WARP_SIZE) batch_size = (batch_size / WARP_SIZE) * WARP_SIZE;\n        if (batch_size <= 0) batch_size = WARP_SIZE;\n    } else {\n        // If no space for caching, set a minimal batch size of 32 to keep algorithmic structure\n        // (the kernel will still function; caching array will be small)\n        batch_size = WARP_SIZE;\n    }\n\n    // Compute actual dynamic shared memory size used for launch\n    size_t dyn_smem_bytes = overhead + (size_t)batch_size * sizeof(float2);\n\n    // Grid configuration: one warp per query\n    int blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Launch kernel\n    void* args[] = {\n        (void*)&query,\n        (void*)&query_count,\n        (void*)&data,\n        (void*)&data_count,\n        (void*)&result,\n        (void*)&k,\n        (void*)&batch_size\n    };\n    cudaLaunchKernel((const void*)&knn_kernel,\n                     dim3(blocks), dim3(threads_per_block),\n                     nullptr, (size_t)dyn_smem_bytes, nullptr, args, nullptr);\n\n    // Optional sync or error checking can be performed by the caller as desired.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 766, "completion_tokens": 23310, "total_tokens": 24076, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 17856, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db866f88190bfe1782bc83579ce", "custom_id": "knn05-10", "response": {"status_code": 200, "request_id": "79849b95f4ebfd88ad0fca2393b42e94", "body": {"id": "chatcmpl-C4sqCwrrwc1Cv8U0N0YF6ip2XPnwf", "object": "chat.completion", "created": 1755280156, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// This implementation computes k-nearest neighbors (k-NN) for 2D points using a warp-per-query design.\n// - One warp (32 threads) processes one query.\n// - All data points are processed in batches (tiles) loaded into shared memory by the whole block.\n// - Each warp maintains:\n//    * An intermediate top-k result (indices + distances) as a \"private\" copy distributed across its 32 lanes in registers.\n//      Each lane stores k/32 entries (with k in [32, 1024], power-of-two, so k/32 <= 32).\n//    * A per-warp candidate buffer of size k (indices + distances) in shared memory.\n// - For each data batch:\n//    * The block loads a tile of points to shared memory.\n//    * Each warp computes distances from its query to all points in the tile and selectively appends better candidates\n//      (distances smaller than the current k-th best) to its buffer using warp-aggregated push.\n//    * When a buffer is full, the warp merges its candidates with the intermediate top-k.\n// - After all batches, if a buffer is non-empty, it merges remaining candidates.\n// - Finally, each warp sorts its intermediate top-k in ascending distance (selection via warp reductions) and writes the\n//   results as std::pair<int,float> to the output.\n//\n// Design details and optimizations:\n// - Distances are squared L2. Single-precision floats with FMA used where possible (fmaf).\n// - Intermediate top-k is stored entirely in per-lane registers (arrays of length up to 32).\n// - The k-th best distance (\"worst\" within the current top-k) is tracked warp-wide and used as threshold.\n// - Merging: candidates are processed sequentially per warp; replacement targets are found by:\n//     (a) each lane keeps (and recomputes when changed) the max of its local segment,\n//     (b) warp-wide argmax reduction finds the global worst to replace.\n//   This yields small per-candidate overhead with k/32 <= 32.\n// - Candidate buffer append uses warp-aggregated compaction with ballot/popc/rank and safe handling when the buffer\n//   overflows; flushing triggers a merge followed by re-evaluation of remaining candidates against the new threshold.\n// - Shared memory usage per block = warpsPerBlock * k * sizeof(Candidate) + tilePoints * sizeof(float2).\n//   Host code queries device limits and chooses warpsPerBlock and tilePoints that fit. It also opts-in to high dynamic\n//   shared memory when supported (A100/H100).\n// - Threads per block = warpsPerBlock * 32. A good default is 8 warps per block (256 threads), with fallbacks to 4/2/1\n//   if shared memory is tight for large k or constrained devices.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\nstruct Candidate {\n    float dist;\n    int   idx;\n};\n\n// Utility: per-warp lane id and warp id within a block\nstatic __device__ __forceinline__ int lane_id() {\n    return threadIdx.x & (WARP_SIZE - 1);\n}\nstatic __device__ __forceinline__ int warp_id_in_block() {\n    return threadIdx.x >> 5;\n}\n\n// Utility: return a mask of bits less than lane id (for prefix rank)\nstatic __device__ __forceinline__ unsigned lane_mask_lt() {\n    return (1u << lane_id()) - 1u;\n}\n\n// Warp-wide argmax reduction on values, returning both maximum value and owning lane.\n// Ties are broken in favor of higher lane indices to keep deterministic behavior.\nstruct ArgMaxPair {\n    float val;\n    int lane;\n};\nstatic __device__ __forceinline__ ArgMaxPair warp_argmax(float v, unsigned mask) {\n    ArgMaxPair a{v, lane_id()};\n    // Tree reduction using shfl_down. Only uses active lanes specified by 'mask'.\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        float v_other = __shfl_down_sync(mask, a.val, offset);\n        int   l_other = __shfl_down_sync(mask, a.lane, offset);\n        if (v_other > a.val || (v_other == a.val && l_other > a.lane)) {\n            a.val  = v_other;\n            a.lane = l_other;\n        }\n    }\n    // Broadcast the final result to all lanes (from lane 0 after reduction)\n    a.val  = __shfl_sync(mask, a.val, 0);\n    a.lane = __shfl_sync(mask, a.lane, 0);\n    return a;\n}\n\n// Warp-wide max reduction (returns maximum value)\nstatic __device__ __forceinline__ float warp_max(float v, unsigned mask) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        float v_other = __shfl_down_sync(mask, v, offset);\n        v = v > v_other ? v : v_other;\n    }\n    return __shfl_sync(mask, v, 0);\n}\n\n// Merge a warp's candidate buffer (size 'bufCount') into the warp's private top-k.\n// - resDist/resIdx are register arrays local to each lane with length 'itemsPerLane' (k/32).\n// - The function updates warpWorstDist to the new k-th best distance after merging.\n// - bufCount is set to 0 at the end.\ntemplate <int WARPS_PER_BLOCK>\nstatic __device__ __forceinline__\nvoid merge_buffer_with_topk(Candidate* cand_base, int k, int& bufCount,\n                            float* resDist, int* resIdx, int itemsPerLane,\n                            float& warpWorstDist) {\n    const unsigned full_mask = 0xFFFFFFFFu;\n    // Compute each lane's local maximum (worst in its segment).\n    float localWorst = resDist[0];\n    int localWorstPos = 0;\n    #pragma unroll\n    for (int i = 1; i < 32; ++i) {\n        if (i >= itemsPerLane) break;\n        float v = resDist[i];\n        if (v > localWorst) { localWorst = v; localWorstPos = i; }\n    }\n    // Initialize warp-wide worst\n    ArgMaxPair am = warp_argmax(localWorst, full_mask);\n    warpWorstDist = am.val;\n\n    // Sequentially process candidates in the buffer. Each iteration reads one candidate and attempts replacement.\n    for (int i = 0; i < bufCount; ++i) {\n        int reader_lane = i & (WARP_SIZE - 1);\n        float candDist = __shfl_sync(full_mask, cand_base[i].dist, reader_lane);\n        int   candIdx  = __shfl_sync(full_mask, cand_base[i].idx,  reader_lane);\n\n        // Skip if not better than current k-th best\n        if (!(candDist < warpWorstDist)) continue;\n\n        // Find which lane owns the global worst entry\n        ArgMaxPair gw = warp_argmax(localWorst, full_mask);\n        int ownerLane = gw.lane;\n        int ownerPos  = __shfl_sync(full_mask, localWorstPos, ownerLane);\n\n        // Owner lane replaces its worst entry with the candidate and recomputes its local worst\n        if (lane_id() == ownerLane) {\n            resDist[ownerPos] = candDist;\n            resIdx[ownerPos]  = candIdx;\n            // Recompute local worst after update\n            float lw = resDist[0];\n            int lp = 0;\n            #pragma unroll\n            for (int t = 1; t < 32; ++t) {\n                if (t >= itemsPerLane) break;\n                float val = resDist[t];\n                if (val > lw) { lw = val; lp = t; }\n            }\n            localWorst = lw;\n            localWorstPos = lp;\n        }\n        // Update global worst\n        warpWorstDist = warp_max(localWorst, full_mask);\n    }\n    // Clear the buffer\n    if (lane_id() == 0) bufCount = 0;\n    __syncwarp();\n}\n\n// The main kernel template. WARPS_PER_BLOCK must be 1, 2, 4, or 8 (256 threads = 8 warps is a good default).\ntemplate <int WARPS_PER_BLOCK>\n__global__ void knn_kernel(const float2* __restrict__ query, int query_count,\n                           const float2* __restrict__ data,  int data_count,\n                           std::pair<int, float>* __restrict__ result,\n                           int k, int tilePoints) {\n    static_assert(WARPS_PER_BLOCK >= 1 && WARPS_PER_BLOCK <= 8, \"WARPS_PER_BLOCK in [1,8]\");\n    static_assert(WARP_SIZE == 32, \"This kernel assumes warp size is 32\");\n\n    const int lane = lane_id();\n    const int warpInBlock = warp_id_in_block();\n    const int warpGlobal = blockIdx.x * WARPS_PER_BLOCK + warpInBlock;\n\n    const unsigned full_mask = 0xFFFFFFFFu;\n\n    // Shared memory layout:\n    // [ per-warp candidate buffers (WARPS_PER_BLOCK * k entries of Candidate) ] [ data tile (tilePoints float2) ]\n    extern __shared__ unsigned char smem_raw[];\n    Candidate* smemCandidates = reinterpret_cast<Candidate*>(smem_raw);\n    Candidate* warpCandBase   = smemCandidates + warpInBlock * k;\n    float2*    sData          = reinterpret_cast<float2*>(smemCandidates + WARPS_PER_BLOCK * k);\n\n    const bool activeWarp = (warpGlobal < query_count);\n\n    // Load the query point and broadcast within the warp.\n    float qx = 0.0f, qy = 0.0f;\n    if (activeWarp) {\n        if (lane == 0) {\n            float2 q = query[warpGlobal];\n            qx = q.x; qy = q.y;\n        }\n        qx = __shfl_sync(full_mask, qx, 0);\n        qy = __shfl_sync(full_mask, qy, 0);\n    }\n\n    // Items per lane in the intermediate top-k\n    const int itemsPerLane = k >> 5; // since k is a power-of-two >= 32, divisible by 32\n    // Private top-k per warp, distributed across lanes, held in registers.\n    // Each lane stores itemsPerLane entries.\n    float resDist[32];\n    int   resIdx[32];\n\n    // Initialize intermediate results to +inf dist and invalid index.\n    const float INF = CUDART_INF_F;\n    #pragma unroll\n    for (int i = 0; i < 32; ++i) {\n        resDist[i] = INF;\n        resIdx[i]  = -1;\n    }\n\n    // Candidate buffer count (per warp), maintained by lane 0 and broadcast as needed\n    int bufCount = 0;\n\n    // Current k-th best distance in the intermediate result (initially INF)\n    float warpWorstDist = INF;\n\n    // Process data in tiles loaded into shared memory by the whole block\n    for (int tileStart = 0; tileStart < data_count; tileStart += tilePoints) {\n        int count = data_count - tileStart;\n        if (count > tilePoints) count = tilePoints;\n\n        // Cooperative load of the data tile\n        for (int i = threadIdx.x; i < count; i += blockDim.x) {\n            sData[i] = data[tileStart + i];\n        }\n        __syncthreads();\n\n        // Each active warp processes the tile: compute distances and push candidates\n        if (activeWarp) {\n            for (int pos = lane; pos < count; pos += WARP_SIZE) {\n                float2 p = sData[pos];\n                float dx = p.x - qx;\n                float dy = p.y - qy;\n                float dist = fmaf(dx, dx, dy * dy);\n                int   idx  = tileStart + pos;\n\n                // Only consider if strictly better than current k-th best\n                bool pending = (dist < warpWorstDist);\n\n                // Warp-aggregated append with buffer-full handling and re-evaluation after merges\n                while (true) {\n                    unsigned mask = __ballot_sync(full_mask, pending);\n                    int nActive = __popc(mask);\n                    if (nActive == 0) break;\n\n                    int base = 0;\n                    int bufCountWarp = __shfl_sync(full_mask, bufCount, 0);\n                    int slotsLeft = k - bufCountWarp;\n\n                    if (slotsLeft == 0) {\n                        // Merge buffer into top-k to free space and tighten the threshold\n                        merge_buffer_with_topk<WARPS_PER_BLOCK>(warpCandBase, k, bufCount, resDist, resIdx, itemsPerLane, warpWorstDist);\n                        // Re-evaluate with the new threshold\n                        pending = (dist < warpWorstDist);\n                        continue;\n                    }\n\n                    int emitCount = (nActive < slotsLeft) ? nActive : slotsLeft;\n                    int rank = __popc(mask & lane_mask_lt());\n                    bool willWrite = pending && (rank < emitCount);\n\n                    if (lane == 0) base = bufCount;\n                    base = __shfl_sync(full_mask, base, 0);\n\n                    if (willWrite) {\n                        warpCandBase[base + rank].dist = dist;\n                        warpCandBase[base + rank].idx  = idx;\n                    }\n\n                    if (lane == 0) bufCount = base + emitCount;\n                    __syncwarp();\n\n                    // If all active lanes wrote, we're done with this candidate.\n                    if (nActive <= slotsLeft) {\n                        pending = false;\n                        break;\n                    }\n\n                    // Otherwise, update 'pending' to keep only those that didn't fit; the buffer is now full -> next loop merges.\n                    pending = pending && (rank >= emitCount);\n                }\n            }\n        }\n\n        __syncthreads();\n    }\n\n    // After the final tile, merge remaining buffered candidates\n    if (activeWarp && bufCount > 0) {\n        merge_buffer_with_topk<WARPS_PER_BLOCK>(warpCandBase, k, bufCount, resDist, resIdx, itemsPerLane, warpWorstDist);\n    }\n\n    // Final step: output the top-k results sorted by ascending distance.\n    // We perform a selection sort using warp-wide reductions:\n    //   - each lane scans its local segment to find the current minimum,\n    //   - a warp reduction finds the global minimum and its owner,\n    //   - the owner marks it as used by setting its distance to INF,\n    //   - lane 0 writes the selected pair to global memory.\n    if (activeWarp) {\n        // Local arrays are already in resDist/resIdx.\n        auto* out = result + warpGlobal * k;\n\n        for (int sel = 0; sel < k; ++sel) {\n            // Local min and its position\n            float localMin = resDist[0];\n            int localPos = 0;\n            #pragma unroll\n            for (int i = 1; i < 32; ++i) {\n                if (i >= itemsPerLane) break;\n                float v = resDist[i];\n                if (v < localMin) { localMin = v; localPos = i; }\n            }\n\n            // Warp-wide argmin via argmax on negative values to avoid another helper\n            // or use max with inverted sense: We simulate argmin by flipping sign and argmax\n            float negLocalMin = -localMin;\n            ArgMaxPair gm = warp_argmax(negLocalMin, full_mask);\n            int ownerLane = gm.lane;\n            int ownerPos  = __shfl_sync(full_mask, localPos, ownerLane);\n            float bestDist = -gm.val;\n            int bestIdx    = __shfl_sync(full_mask, resIdx[ownerPos], ownerLane);\n\n            if (lane == 0) {\n                out[sel].first  = bestIdx;\n                out[sel].second = bestDist;\n            }\n\n            // Mark selected item as used by setting distance to INF\n            if (lane == ownerLane) {\n                resDist[ownerPos] = INF;\n                // resIdx[ownerPos] may remain as-is; distance INF prevents reselection.\n            }\n            __syncwarp();\n        }\n    }\n}\n\n// Helper to choose a launch configuration and dynamic shared memory size that fits, then launch the kernel.\nstatic inline void launch_knn_kernel(const float2* query, int query_count,\n                                     const float2* data,  int data_count,\n                                     std::pair<int, float>* result, int k) {\n    // Candidate buffers require warpsPerBlock * k candidates, each 8 bytes.\n    // Data tile requires tilePoints * 8 bytes.\n    // We try WARPS_PER_BLOCK in {8,4,2,1} to fit into device limits.\n    int device = 0;\n    cudaGetDevice(&device);\n\n    int maxDynSmemOptin = 0;\n    int maxDynSmemDefault = 0;\n    cudaDeviceGetAttribute(&maxDynSmemOptin,   cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    cudaDeviceGetAttribute(&maxDynSmemDefault, cudaDevAttrMaxSharedMemoryPerBlock,      device);\n\n    int maxDynSmem = maxDynSmemOptin > 0 ? maxDynSmemOptin : maxDynSmemDefault;\n    if (maxDynSmem <= 0) {\n        // Fallback to a conservative number if querying fails (rare)\n        maxDynSmem = 96 * 1024;\n    }\n\n    // Try different WARPS_PER_BLOCK to fit shared memory constraints.\n    int chosenWarps = 0;\n    int blockThreads = 0;\n    int tilePoints   = 0;\n    size_t dynSmem   = 0;\n\n    auto try_config = [&](int warpsPerBlock) {\n        size_t candBytes = static_cast<size_t>(warpsPerBlock) * static_cast<size_t>(k) * sizeof(Candidate);\n        if (candBytes + sizeof(float2) > static_cast<size_t>(maxDynSmem)) return false;\n        int maxTilePts = static_cast<int>((maxDynSmem - candBytes) / sizeof(float2));\n        if (maxTilePts <= 0) return false;\n        // Round tile points down to multiple of blockDim.x for coalesced loads\n        int threads = warpsPerBlock * WARP_SIZE;\n        int rounded = (maxTilePts / threads) * threads;\n        if (rounded <= 0) return false;\n\n        chosenWarps = warpsPerBlock;\n        blockThreads = threads;\n        tilePoints = rounded;\n        dynSmem = candBytes + static_cast<size_t>(tilePoints) * sizeof(float2);\n        return true;\n    };\n\n    if (!try_config(8)) {\n        if (!try_config(4)) {\n            if (!try_config(2)) {\n                if (!try_config(1)) {\n                    // As a last resort, fallback to minimal viable tilePoints with 1 warp/block\n                    chosenWarps = 1;\n                    blockThreads = WARP_SIZE;\n                    size_t candBytes = static_cast<size_t>(chosenWarps) * static_cast<size_t>(k) * sizeof(Candidate);\n                    int minTilePts = blockThreads; // at least one per thread\n                    size_t needed = candBytes + static_cast<size_t>(minTilePts) * sizeof(float2);\n                    // If still not fitting, clamp to device default max\n                    if (needed > static_cast<size_t>(maxDynSmem)) {\n                        // Reduce tile points to fit\n                        int maxTilePts = static_cast<int>((maxDynSmem - candBytes) / sizeof(float2));\n                        if (maxTilePts < blockThreads) maxTilePts = blockThreads; // even if exceeding, will set attr\n                        tilePoints = maxTilePts / blockThreads * blockThreads;\n                        dynSmem = candBytes + static_cast<size_t>(tilePoints) * sizeof(float2);\n                    } else {\n                        tilePoints = minTilePts;\n                        dynSmem = needed;\n                    }\n                }\n            }\n        }\n    }\n\n    // Grid size: one warp per query\n    int gridBlocks = (query_count + chosenWarps - 1) / chosenWarps;\n\n    // Opt-in to large dynamic shared memory if supported\n    // Set preferred shared memory carveout to maximize shared memory\n    // These calls are best-effort; ignore errors if not supported.\n    if (maxDynSmemOptin > 0) {\n        cudaFuncSetAttribute(knn_kernel<8>, cudaFuncAttributeMaxDynamicSharedMemorySize, maxDynSmem);\n        cudaFuncSetAttribute(knn_kernel<4>, cudaFuncAttributeMaxDynamicSharedMemorySize, maxDynSmem);\n        cudaFuncSetAttribute(knn_kernel<2>, cudaFuncAttributeMaxDynamicSharedMemorySize, maxDynSmem);\n        cudaFuncSetAttribute(knn_kernel<1>, cudaFuncAttributeMaxDynamicSharedMemorySize, maxDynSmem);\n#if defined(cudaFuncAttributePreferredSharedMemoryCarveout)\n        cudaFuncSetAttribute(knn_kernel<8>, cudaFuncAttributePreferredSharedMemoryCarveout, 100);\n        cudaFuncSetAttribute(knn_kernel<4>, cudaFuncAttributePreferredSharedMemoryCarveout, 100);\n        cudaFuncSetAttribute(knn_kernel<2>, cudaFuncAttributePreferredSharedMemoryCarveout, 100);\n        cudaFuncSetAttribute(knn_kernel<1>, cudaFuncAttributePreferredSharedMemoryCarveout, 100);\n#endif\n    }\n\n    // Launch with the chosen configuration\n    switch (chosenWarps) {\n        case 8:\n            knn_kernel<8><<<gridBlocks, 8 * WARP_SIZE, dynSmem>>>(query, query_count, data, data_count, result, k, tilePoints);\n            break;\n        case 4:\n            knn_kernel<4><<<gridBlocks, 4 * WARP_SIZE, dynSmem>>>(query, query_count, data, data_count, result, k, tilePoints);\n            break;\n        case 2:\n            knn_kernel<2><<<gridBlocks, 2 * WARP_SIZE, dynSmem>>>(query, query_count, data, data_count, result, k, tilePoints);\n            break;\n        default:\n            knn_kernel<1><<<gridBlocks, 1 * WARP_SIZE, dynSmem>>>(query, query_count, data, data_count, result, k, tilePoints);\n            break;\n    }\n}\n\n// Public API entry point\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    // Assumptions: pointers are device memory; k is power-of-two in [32, 1024]; data_count >= k.\n    // The kernel is designed to handle large arrays efficiently with tiling and warp-per-query processing.\n    launch_knn_kernel(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 766, "completion_tokens": 23457, "total_tokens": 24223, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 18496, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db84ba8819084d639c85fe2b233", "custom_id": "knn06-01", "response": {"status_code": 200, "request_id": "4219f5cdc108e4f1c4c6d581141a7008", "body": {"id": "chatcmpl-C4slyPVRpIeIUg7xkgEn2TpbsGjlC", "object": "chat.completion", "created": 1755279894, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cfloat>\n#include <utility>\n\n// This implementation assigns one warp (32 threads) to each query point.\n// The block cooperatively loads input data points into shared memory tiles.\n// Each warp maintains:\n//  - An intermediate result (its current top-k neighbors) privately in registers (distributed across threads).\n//  - A per-warp candidate buffer of capacity k in shared memory, plus a shared counter updated via atomicAdd.\n//\n// The algorithm processes the data in batches (tiles) cached in shared memory.\n// Each warp computes distances from its query to all points in a tile and filters by current max_distance\n// (distance of the k-th neighbor). Points closer than max_distance are buffered as candidates.\n// When the per-warp buffer fills up, the warp merges the buffered candidates into its intermediate result.\n// After processing all tiles, the warp merges any remaining candidates, then gathers and sorts its top-k\n// results (ascending by distance) in shared memory and writes them to the output.\n//\n// Notes:\n//  - k is a power of two in [32, 1024]. We use k/32 elements per thread to store the warp's top-k list.\n//  - Shared memory usage per block (worst-case with k=1024, 8 warps and tile of 4096 points) is ~96 KB.\n//  - The kernel uses warp-level intrinsics (__shfl_sync, __ballot_sync, __syncwarp) for communication.\n//  - The output is written into a std::pair<int,float> array; we use a POD alias with identical layout.\n\nstruct PairIF { int first; float second; };\n\n// Utilities\nstatic inline __device__ unsigned lane_id() { return threadIdx.x & 31; }\n\ntemplate <typename T>\nstatic inline __device__ T warp_bcast(T v, int src_lane, unsigned mask = 0xFFFFFFFFu) {\n    return __shfl_sync(mask, v, src_lane);\n}\n\nstatic inline __device__ float warp_reduce_max(float v, unsigned mask = 0xFFFFFFFFu) {\n    // Full-warp reduction to maximum\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        float oth = __shfl_down_sync(mask, v, offset);\n        v = fmaxf(v, oth);\n    }\n    // Broadcast max to all lanes (in lane 0 after reduction)\n    float maxv = __shfl_sync(mask, v, 0);\n    return maxv;\n}\n\n// Compute warp-wide argmax of values, returning the lane that holds the maximum value.\n// Tie-breaker: smaller lane index wins to keep behavior deterministic.\nstatic inline __device__ int warp_argmax_lane(float local_val, int lane, unsigned mask = 0xFFFFFFFFu) {\n    int max_lane = lane;\n    float max_val = local_val;\n    // Reduce by comparing against values from other lanes\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        float oth_val  = __shfl_down_sync(mask, max_val,  offset);\n        int   oth_lane = __shfl_down_sync(mask, max_lane, offset);\n        bool take_other = (oth_val > max_val) || ((oth_val == max_val) && (oth_lane < max_lane));\n        max_val  = take_other ? oth_val  : max_val;\n        max_lane = take_other ? oth_lane : max_lane;\n    }\n    // Broadcast winner lane to all lanes\n    int winner_lane = __shfl_sync(mask, max_lane, 0);\n    return winner_lane;\n}\n\n// Merge the candidate buffer (cand_count entries) into the top-k list stored in registers (best_dist/best_idx).\n// - k: total top-k size\n// - kpt: number of elements per thread (k/32)\n// - best_dist/idx: per-thread arrays of size kpt\n// - cand_dist/idx: per-warp arrays in shared memory of size k\n// - warp_maxdist: in/out distance of the current k-th neighbor (global worst among the top-k)\ntemplate <int KPT_MAX>\nstatic inline __device__ void merge_candidates_warp(\n    int k,\n    int cand_count,\n    const float* __restrict__ cand_dist,\n    const int*   __restrict__ cand_idx,\n    float (&best_dist)[KPT_MAX],\n    int   (&best_idx)[KPT_MAX],\n    int kpt,\n    float &warp_maxdist\n) {\n    const unsigned full_mask = 0xFFFFFFFFu;\n    const int lane = lane_id();\n\n    if (cand_count <= 0) return;\n\n    // Process candidates sequentially with warp cooperation\n    for (int c = 0; c < cand_count; ++c) {\n        float cd = cand_dist[c];\n        int   ci = cand_idx[c];\n\n        // Only attempt to insert if strictly better than current k-th\n        if (cd < warp_maxdist) {\n            // Each lane computes its local maximum among its kpt entries and remembers the slot index\n            float local_max = -CUDART_INF_F;\n            int local_slot = 0;\n            #pragma unroll\n            for (int s = 0; s < KPT_MAX; ++s) {\n                if (s < kpt) {\n                    float v = best_dist[s];\n                    if (v > local_max) {\n                        local_max = v;\n                        local_slot = s;\n                    }\n                }\n            }\n\n            // Find which lane currently holds the global maximum\n            int max_owner_lane = warp_argmax_lane(local_max, lane, full_mask);\n            float max_val = warp_bcast(local_max, max_owner_lane, full_mask);\n\n            // If the candidate is better than the current global worst, replace it\n            if (cd < max_val) {\n                // Only the owner lane updates its own arrays at the recorded slot\n                if (lane == max_owner_lane) {\n                    best_dist[local_slot] = cd;\n                    best_idx[local_slot]  = ci;\n                }\n                // Recompute the new global worst (warp_maxdist) after replacement\n                float new_local_max = -CUDART_INF_F;\n                #pragma unroll\n                for (int s = 0; s < KPT_MAX; ++s) {\n                    if (s < kpt) {\n                        new_local_max = fmaxf(new_local_max, best_dist[s]);\n                    }\n                }\n                warp_maxdist = warp_reduce_max(new_local_max, full_mask);\n            }\n        }\n    }\n}\n\n// Bitonic sort (ascending by distance) over per-warp arrays in shared memory.\n// length == k is a power of two (32..1024).\nstatic inline __device__ void bitonic_sort_pairs_shared(float* dist, int* idx, int length) {\n    const unsigned full_mask = 0xFFFFFFFFu;\n    const int lane = lane_id();\n    for (int size = 2; size <= length; size <<= 1) {\n        // \"size\" is the size of the bitonic sequence\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            for (int i = lane; i < length; i += 32) {\n                int partner = i ^ stride;\n                if (partner > i) {\n                    bool up = ((i & size) == 0); // ascending in the first half\n                    float a = dist[i];\n                    float b = dist[partner];\n                    int ia = idx[i];\n                    int ib = idx[partner];\n                    bool swap = up ? (a > b) : (a < b);\n                    if (swap) {\n                        dist[i]      = b;\n                        dist[partner]= a;\n                        idx[i]       = ib;\n                        idx[partner] = ia;\n                    }\n                }\n            }\n            __syncwarp(full_mask);\n        }\n    }\n}\n\ntemplate<int WARPS_PER_BLOCK, int TILE_POINTS>\n__global__ void knn_kernel(\n    const float2* __restrict__ query,\n    int query_count,\n    const float2* __restrict__ data,\n    int data_count,\n    int k,\n    PairIF* __restrict__ result\n) {\n    const unsigned full_mask = 0xFFFFFFFFu;\n    const int lane = lane_id();\n    const int warp_in_block = threadIdx.x >> 5;\n    const int global_warp_id = blockIdx.x * WARPS_PER_BLOCK + warp_in_block;\n    const bool warp_active = (global_warp_id < query_count);\n    const int kpt = k >> 5; // k / 32\n    const int KPT_MAX = 1024 / 32; // 32\n\n    extern __shared__ unsigned char smem_raw[];\n    auto align8 = [](size_t x) { return (x + 7) & ~size_t(7); };\n\n    // Lay out shared memory:\n    size_t smem_off = 0;\n\n    // Tile of data points\n    float2* tile = reinterpret_cast<float2*>(smem_raw + smem_off);\n    smem_off += TILE_POINTS * sizeof(float2);\n    smem_off = align8(smem_off);\n\n    // Per-warp candidate buffers (indices and distances)\n    int* cand_idx_base   = reinterpret_cast<int*>(smem_raw + smem_off);\n    smem_off += WARPS_PER_BLOCK * size_t(k) * sizeof(int);\n    smem_off = align8(smem_off);\n\n    float* cand_dist_base = reinterpret_cast<float*>(smem_raw + smem_off);\n    smem_off += WARPS_PER_BLOCK * size_t(k) * sizeof(float);\n    smem_off = align8(smem_off);\n\n    int* cand_count_base = reinterpret_cast<int*>(smem_raw + smem_off);\n    smem_off += WARPS_PER_BLOCK * sizeof(int);\n    // No further align needed\n\n    // Pointers to this warp's candidate buffer\n    int*   cand_idx  = cand_idx_base  + warp_in_block * k;\n    float* cand_dist = cand_dist_base + warp_in_block * k;\n    volatile int* cand_count = cand_count_base;\n\n    // Initialize per-warp candidate count\n    if (lane == 0) cand_count[warp_in_block] = 0;\n\n    // Private intermediate result in registers (distributed across lanes)\n    int   best_idx[32];   // KPT_MAX = 32 (max k/32)\n    float best_dist[32];\n    #pragma unroll\n    for (int i = 0; i < 32; ++i) {\n        if (i < kpt) {\n            best_idx[i] = -1;\n            best_dist[i] = CUDART_INF_F;\n        }\n    }\n\n    // Load query point for this warp\n    float2 q = make_float2(0.0f, 0.0f);\n    if (warp_active) {\n        q = query[global_warp_id];\n    }\n\n    // Initialize warp-level max distance (k-th neighbor distance)\n    float warp_maxdist = CUDART_INF_F;\n\n    // Process data in tiles\n    for (int base = 0; base < data_count; base += TILE_POINTS) {\n        int tile_size = data_count - base;\n        if (tile_size > TILE_POINTS) tile_size = TILE_POINTS;\n\n        // Block-cooperative load of tile into shared memory\n        for (int i = threadIdx.x; i < tile_size; i += blockDim.x) {\n            tile[i] = data[base + i];\n        }\n        __syncthreads();\n\n        if (warp_active) {\n            // Each warp processes all points in the tile\n            for (int j = lane; j < tile_size; j += 32) {\n                float2 p = tile[j];\n                float dx = q.x - p.x;\n                float dy = q.y - p.y;\n                float dist = dx * dx + dy * dy;\n\n                // Determine which lanes have candidates closer than current max\n                bool is_active = (dist < warp_maxdist);\n                unsigned active_mask = __ballot_sync(full_mask, is_active);\n                if (active_mask) {\n                    int leader = __ffs(active_mask) - 1;\n\n                    int local_rank = 0;\n                    if (is_active) {\n                        unsigned prior = active_mask & ((1u << lane) - 1u);\n                        local_rank = __popc(prior);\n                    }\n                    int num_active = __popc(active_mask);\n\n                    int base_pos = 0;\n                    if (lane == leader) {\n                        base_pos = atomicAdd((int*)&cand_count[warp_in_block], num_active);\n                    }\n                    base_pos = __shfl_sync(active_mask, base_pos, leader);\n\n                    int remaining = k - base_pos;\n                    int take = remaining > 0 ? (remaining < num_active ? remaining : num_active) : 0;\n                    int take_bcast = __shfl_sync(active_mask, take, leader);\n\n                    // Write the first 'take' candidates into the buffer\n                    if (is_active && local_rank < take_bcast) {\n                        int pos = base_pos + local_rank;\n                        cand_idx[pos]  = base + j;\n                        cand_dist[pos] = dist;\n                    }\n\n                    // If buffer is full, merge it with intermediate result\n                    bool need_merge = (base_pos + num_active) >= k;\n                    unsigned merge_mask = __ballot_sync(full_mask, need_merge);\n                    if (merge_mask) {\n                        __syncwarp(full_mask);\n                        if (lane == 0) {\n                            // Clamp count to k (should be >= k now)\n                            int cc = cand_count[warp_in_block];\n                            if (cc > k) cand_count[warp_in_block] = k;\n                        }\n                        __syncwarp(full_mask);\n\n                        int cc = cand_count[warp_in_block];\n                        merge_candidates_warp<32>(k, cc, cand_dist, cand_idx, best_dist, best_idx, kpt, warp_maxdist);\n\n                        if (lane == 0) {\n                            cand_count[warp_in_block] = 0;\n                        }\n                        __syncwarp(full_mask);\n\n                        // Handle leftover candidates from this iteration that didn't fit before merge\n                        bool had_leftover = is_active && (local_rank >= take_bcast) && (dist < warp_maxdist);\n                        unsigned leftover_mask = __ballot_sync(full_mask, had_leftover);\n                        if (leftover_mask) {\n                            int leader2 = __ffs(leftover_mask) - 1;\n                            int n2 = __popc(leftover_mask);\n                            int local_rank2 = 0;\n                            if (had_leftover) {\n                                unsigned prior2 = leftover_mask & ((1u << lane) - 1u);\n                                local_rank2 = __popc(prior2);\n                            }\n                            int base2 = 0;\n                            if (lane == leader2) {\n                                base2 = atomicAdd((int*)&cand_count[warp_in_block], n2);\n                            }\n                            base2 = __shfl_sync(leftover_mask, base2, leader2);\n                            if (had_leftover) {\n                                int pos2 = base2 + local_rank2;\n                                cand_idx[pos2]  = base + j;\n                                cand_dist[pos2] = dist;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n\n        __syncthreads(); // ensure all warps done with the tile before loading next\n    }\n\n    // After processing all tiles, merge any remaining candidates\n    if (warp_active) {\n        int cc = cand_count[warp_in_block];\n        if (cc > 0) {\n            merge_candidates_warp<32>(k, cc, cand_dist, cand_idx, best_dist, best_idx, kpt, warp_maxdist);\n            if (lane == 0) cand_count[warp_in_block] = 0;\n            __syncwarp(full_mask);\n        }\n\n        // Gather the top-k from registers into the per-warp shared buffer (reuse it for sorting)\n        // Layout: position pos = s * 32 + lane for s in [0, kpt)\n        #pragma unroll\n        for (int s = 0; s < 32; ++s) {\n            if (s < kpt) {\n                int pos = s * 32 + lane;\n                cand_idx[pos]  = best_idx[s];\n                cand_dist[pos] = best_dist[s];\n            }\n        }\n        __syncwarp(full_mask);\n\n        // Sort ascending by distance using bitonic sort\n        bitonic_sort_pairs_shared(cand_dist, cand_idx, k);\n\n        // Write results to global memory\n        PairIF* out = result + global_warp_id * k;\n        for (int pos = lane; pos < k; pos += 32) {\n            out[pos].first  = cand_idx[pos];\n            out[pos].second = cand_dist[pos];\n        }\n    }\n}\n\n// Host interface\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    // Choose algorithm parameters\n    constexpr int WARPS_PER_BLOCK = 8;   // 8 warps -> 256 threads per block\n    constexpr int TILE_POINTS     = 4096; // 4096 points per shared-memory tile (32 KB)\n\n    // Grid configuration\n    int blocks = (query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK;\n    dim3 block_dim(WARPS_PER_BLOCK * 32, 1, 1);\n    dim3 grid_dim(blocks, 1, 1);\n\n    // Compute dynamic shared memory size\n    auto align8 = [](size_t x) { return (x + 7) & ~size_t(7); };\n    size_t smem_size = 0;\n    smem_size += TILE_POINTS * sizeof(float2);\n    smem_size = align8(smem_size);\n    smem_size += WARPS_PER_BLOCK * size_t(k) * sizeof(int);\n    smem_size = align8(smem_size);\n    smem_size += WARPS_PER_BLOCK * size_t(k) * sizeof(float);\n    smem_size = align8(smem_size);\n    smem_size += WARPS_PER_BLOCK * sizeof(int);\n\n    // Set attribute to allow large dynamic shared memory usage if needed\n    cudaFuncSetAttribute(knn_kernel<WARPS_PER_BLOCK, TILE_POINTS>,\n                         cudaFuncAttributeMaxDynamicSharedMemorySize,\n                         static_cast<int>(smem_size));\n\n    // Launch kernel\n    PairIF* result_if = reinterpret_cast<PairIF*>(result);\n    knn_kernel<WARPS_PER_BLOCK, TILE_POINTS><<<grid_dim, block_dim, smem_size>>>(\n        query, query_count, data, data_count, k, result_if);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 879, "completion_tokens": 21188, "total_tokens": 22067, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 17216, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db84c6c8190b511a61ba1009b4f", "custom_id": "knn06-02", "response": {"status_code": 200, "request_id": "ea6e15a2600300ae960a18ea3662abcf", "body": {"id": "chatcmpl-C4ssxhCX4kTWhpiqZOgCq7PXM2C7s", "object": "chat.completion", "created": 1755280327, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// Optimized CUDA k-NN (2D, squared L2) using one warp (32 threads) per query.\n// - Processes data in shared-memory batches (\"tiles\") loaded by the block.\n// - Each warp maintains:\n//   * An intermediate result of k nearest neighbors (distributed across lanes in registers).\n//   * A shared-memory candidate buffer of size k with an atomic counter.\n//   * A shared max_distance (distance of the current k-th nearest neighbor).\n// - Candidates closer than max_distance are added to the buffer using atomicAdd for position allocation.\n// - When the buffer becomes full, it is merged into the intermediate result cooperatively by the warp.\n// - After all tiles, any remaining candidates in the buffer are merged, then results are sorted per query and written out.\n\n#ifndef KNN_THREADS_PER_BLOCK\n#define KNN_THREADS_PER_BLOCK 256  // 8 warps per block\n#endif\n\n#ifndef KNN_TILE_POINTS\n#define KNN_TILE_POINTS 4096       // Number of data points cached per block tile (floats2 => 32 KB for 4096)\n#endif\n\nstatic __device__ __forceinline__ int lane_id() { return threadIdx.x & 31; }\nstatic __device__ __forceinline__ int warp_id() { return threadIdx.x >> 5; }\nstatic __device__ __forceinline__ int warps_per_block() { return blockDim.x >> 5; }\n\nstatic __device__ __forceinline__ unsigned full_mask() { return 0xFFFFFFFFu; }\n\n// Compute the local (per-lane) maximum value and its position within a small array of length chunk (chunk in [1,32])\nstatic __device__ __forceinline__\nvoid local_array_argmax(const float* __restrict__ vals, int chunk, float& out_val, int& out_pos)\n{\n    float v = vals[0];\n    int p = 0;\n    #pragma unroll\n    for (int i = 1; i < 32; ++i) {\n        if (i >= chunk) break;\n        float vi = vals[i];\n        if (vi > v) { v = vi; p = i; }\n    }\n    out_val = v;\n    out_pos = p;\n}\n\n// Warp-wide argmax reduction over (value, lane, pos). Returns in lane 0, but broadcasts to all lanes for convenience.\nstatic __device__ __forceinline__\nvoid warp_argmax(float local_val, int local_pos, int& out_lane, int& out_pos, float& out_val)\n{\n    unsigned mask = full_mask();\n    int l = lane_id();\n    float best_val = local_val;\n    int best_lane = l;\n    int best_pos  = local_pos;\n\n    // Tree reduction using shuffle-down\n    #pragma unroll\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        float other_val = __shfl_down_sync(mask, best_val, offset);\n        int other_lane  = __shfl_down_sync(mask, best_lane, offset);\n        int other_pos   = __shfl_down_sync(mask, best_pos, offset);\n        if (other_val > best_val) {\n            best_val = other_val;\n            best_lane = other_lane;\n            best_pos = other_pos;\n        }\n    }\n    // Broadcast the result from lane 0 to all lanes\n    out_val = __shfl_sync(mask, best_val, 0);\n    out_lane = __shfl_sync(mask, best_lane, 0);\n    out_pos = __shfl_sync(mask, best_pos, 0);\n}\n\n// Warp-wide maximum reduction. Returns the maximum broadcast to all lanes.\nstatic __device__ __forceinline__\nfloat warp_maxf(float v)\n{\n    unsigned mask = full_mask();\n    #pragma unroll\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        v = fmaxf(v, __shfl_down_sync(mask, v, offset));\n    }\n    return __shfl_sync(mask, v, 0);\n}\n\n// Sort k elements in shared memory (dist/idx) cooperatively by a single warp using bitonic sort (ascending by distance).\nstatic __device__ __forceinline__\nvoid warp_bitonic_sort(float* __restrict__ dist, int* __restrict__ idx, int k)\n{\n    unsigned mask = full_mask();\n    int lane = lane_id();\n\n    // Standard bitonic sort network over shared array indices [0..k)\n    for (int size = 2; size <= k; size <<= 1) {\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            for (int i = lane; i < k; i += 32) {\n                int j = i ^ stride;\n                if (j > i) {\n                    bool up = ((i & size) == 0);\n                    float ai = dist[i];\n                    float aj = dist[j];\n                    int   bi = idx[i];\n                    int   bj = idx[j];\n                    // Swap if out of order for this step\n                    if ((ai > aj) == up) {\n                        dist[i] = aj; dist[j] = ai;\n                        idx[i] = bj;  idx[j] = bi;\n                    }\n                }\n            }\n            __syncwarp(mask);\n        }\n    }\n}\n\n// Merge the candidate buffer of a warp into its intermediate top-k, cooperatively using all 32 lanes.\n// - top_d/top_i are the per-lane register arrays of size 'chunk' that together represent the current k best.\n// - candDist/candIdx/candCount are per-warp shared-memory buffers.\n// - maxDistSh is the per-warp shared maximum distance in shared memory; updated here after merging.\n// Note: This routine assumes that the calling warp is uniform (all lanes participate).\nstatic __device__ __forceinline__\nvoid warp_merge_buffer(\n    int k,\n    int chunk,\n    float* __restrict__ candDist_base, // base pointer for all warps\n    int*   __restrict__ candIdx_base,  // base pointer for all warps\n    int*   __restrict__ candCount,     // per-warp counters array\n    float* __restrict__ maxDistSh,     // per-warp max-distance array\n    float* __restrict__ top_d,         // per-lane register array [chunk] of distances\n    int*   __restrict__ top_i          // per-lane register array [chunk] of indices\n)\n{\n    unsigned mask = full_mask();\n    int wid = warp_id();\n    int lane = lane_id();\n\n    float* candDist = candDist_base + wid * k;\n    int*   candIdx  = candIdx_base  + wid * k;\n\n    // Read candidate count uniformly\n    int c = 0;\n    if (lane == 0) c = candCount[wid];\n    c = __shfl_sync(mask, c, 0);\n\n    // Current maximum (k-th best) distance across the top-k\n    float local_max, curMax;\n    int local_pos;\n    local_array_argmax(top_d, chunk, local_max, local_pos);\n    curMax = warp_maxf(local_max);\n\n    // Sequentially insert candidates; each insertion is warp-cooperative via argmax\n    for (int j = 0; j < c; ++j) {\n        float cd = candDist[j];\n        int   ci = candIdx[j];\n\n        if (cd < curMax) {\n            // Find current global argmax across the top-k representation\n            local_array_argmax(top_d, chunk, local_max, local_pos);\n            int max_lane, max_pos;\n            float max_val;\n            warp_argmax(local_max, local_pos, max_lane, max_pos, max_val);\n\n            if (cd < max_val) {\n                if (lane == max_lane) {\n                    top_d[max_pos] = cd;\n                    top_i[max_pos] = ci;\n                }\n                __syncwarp(mask);\n                // Update curMax for next iterations\n                local_array_argmax(top_d, chunk, local_max, local_pos);\n                curMax = warp_maxf(local_max);\n            }\n        }\n    }\n\n    if (lane == 0) candCount[wid] = 0;\n    __syncwarp(mask);\n\n    // Write back the updated maximum to shared memory\n    local_array_argmax(top_d, chunk, local_max, local_pos);\n    curMax = warp_maxf(local_max);\n    if (lane == 0) maxDistSh[wid] = curMax;\n    __syncwarp(mask);\n}\n\n// Kernel implementing k-NN with one warp per query\n__global__ void knn_kernel(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data,  int data_count,\n    std::pair<int, float>* __restrict__ result,\n    int k\n)\n{\n    // Dynamic shared memory layout:\n    // [0]   float2 tile[KNN_TILE_POINTS]\n    // [1]   int    candCount[warpsPerBlock]\n    // [2]   float  maxDist[warpsPerBlock]\n    // [3]   float  candDist[warpsPerBlock * k]\n    // [4]   int    candIdx [warpsPerBlock * k]\n\n    extern __shared__ unsigned char smem_raw[];\n    unsigned char* ptr = smem_raw;\n\n    float2* tile = reinterpret_cast<float2*>(ptr);\n    ptr += sizeof(float2) * KNN_TILE_POINTS;\n\n    // Align to 8 bytes for safety\n    uintptr_t uptr = reinterpret_cast<uintptr_t>(ptr);\n    uptr = (uptr + 7) & ~uintptr_t(7);\n    ptr = reinterpret_cast<unsigned char*>(uptr);\n\n    int wpb = warps_per_block();\n\n    int* candCount = reinterpret_cast<int*>(ptr);\n    ptr += sizeof(int) * wpb;\n\n    float* maxDistSh = reinterpret_cast<float*>(ptr);\n    ptr += sizeof(float) * wpb;\n\n    float* candDist = reinterpret_cast<float*>(ptr);\n    ptr += sizeof(float) * (size_t)wpb * (size_t)k;\n\n    int* candIdx = reinterpret_cast<int*>(ptr);\n    // ptr += sizeof(int) * (size_t)wpb * (size_t)k; // not needed further\n\n    unsigned mask = full_mask();\n    int wid  = warp_id();\n    int lane = lane_id();\n\n    int global_warp = blockIdx.x * wpb + wid;\n    int qid = global_warp; // one query per warp\n    bool valid = (qid < query_count);\n\n    // Each lane holds chunk = k/32 elements of the intermediate top-k\n    const int chunk = max(1, k >> 5); // k is a power of two >= 32\n\n    // Per-lane intermediate top-k storage in registers\n    float top_d[32];\n    int   top_i[32];\n    #pragma unroll\n    for (int i = 0; i < 32; ++i) {\n        if (i < chunk) {\n            top_d[i] = CUDART_INF_F;\n            top_i[i] = -1;\n        }\n    }\n\n    // Initialize per-warp shared variables\n    if (lane == 0) {\n        candCount[wid] = 0;\n        maxDistSh[wid] = CUDART_INF_F;\n    }\n    __syncwarp(mask);\n\n    // Load query point to registers\n    float qx = 0.0f, qy = 0.0f;\n    if (valid) {\n        float2 q = query[qid];\n        qx = q.x;\n        qy = q.y;\n    }\n\n    // Iterate over data in tiles\n    for (int base = 0; base < data_count; base += KNN_TILE_POINTS) {\n        int tileCount = min(KNN_TILE_POINTS, data_count - base);\n\n        // Block-wide load of tile into shared memory\n        for (int t = threadIdx.x; t < tileCount; t += blockDim.x) {\n            tile[t] = data[base + t];\n        }\n        __syncthreads();\n\n        if (valid) {\n            // Local cached copy of max distance for quick checks\n            float curMax = maxDistSh[wid];\n\n            // Process the tile: each lane steps over positions [lane, lane+32, ...]\n            for (int j = lane; j < tileCount; j += 32) {\n                float2 p = tile[j];\n                float dx = p.x - qx;\n                float dy = p.y - qy;\n                float dist = fmaf(dx, dx, dy * dy); // squared L2\n\n                // Check against current threshold\n                int pass = (dist < curMax);\n                unsigned passMask = __ballot_sync(mask, pass);\n                int n = __popc(passMask);\n                if (n) {\n                    // One atomicAdd per warp-reservation\n                    int basePos = 0;\n                    if (lane == 0) basePos = atomicAdd(&candCount[wid], n);\n                    basePos = __shfl_sync(mask, basePos, 0);\n\n                    // Rank within passing lanes\n                    int rank = __popc(passMask & ((1u << lane) - 1));\n\n                    // Number that fit before reaching capacity\n                    int fit = max(0, k - basePos);\n                    int toWrite = min(n, fit);\n\n                    if (pass && (rank < toWrite)) {\n                        int pos = basePos + rank;\n                        candDist[wid * k + pos] = dist;\n                        candIdx [wid * k + pos] = base + j;\n                    }\n\n                    // If buffer is now full or overflown, merge it\n                    bool needFlush = (basePos + n >= k);\n                    if (needFlush) {\n                        __syncwarp(mask);\n                        warp_merge_buffer(k, chunk, candDist, candIdx, candCount, maxDistSh, top_d, top_i);\n                        __syncwarp(mask);\n                        curMax = maxDistSh[wid];\n\n                        // Handle leftovers (those that didn't fit before flush)\n                        int leftover = n - toWrite;\n                        if (leftover > 0) {\n                            int isLeft = (pass && (rank >= toWrite)) ? 1 : 0;\n                            unsigned leftMask = __ballot_sync(mask, isLeft);\n                            int leftRank = __popc(leftMask & ((1u << lane) - 1));\n                            int base2 = 0;\n                            if (lane == 0) base2 = atomicAdd(&candCount[wid], leftover);\n                            base2 = __shfl_sync(mask, base2, 0);\n                            if (isLeft) {\n                                int pos = base2 + leftRank;\n                                candDist[wid * k + pos] = dist;\n                                candIdx [wid * k + pos] = base + j;\n                            }\n                            // If the buffer became exactly full again, flush to keep the requirement\n                            bool flushAgain = (base2 + leftover >= k);\n                            if (flushAgain) {\n                                __syncwarp(mask);\n                                warp_merge_buffer(k, chunk, candDist, candIdx, candCount, maxDistSh, top_d, top_i);\n                                __syncwarp(mask);\n                                curMax = maxDistSh[wid];\n                            }\n                        }\n                    }\n                }\n            }\n\n            // End of tile: flush any remaining candidates\n            int c = 0;\n            if (lane == 0) c = candCount[wid];\n            c = __shfl_sync(mask, c, 0);\n            if (c > 0) {\n                __syncwarp(mask);\n                warp_merge_buffer(k, chunk, candDist, candIdx, candCount, maxDistSh, top_d, top_i);\n                __syncwarp(mask);\n            }\n        }\n\n        __syncthreads(); // ensure tile not reused until all warps finish processing it\n    }\n\n    // After processing all tiles: output results\n    if (valid) {\n        // Reuse candidate buffer as a per-warp temporary array to gather and sort the intermediate top-k\n        float* outDist = candDist + wid * k;\n        int*   outIdx  = candIdx  + wid * k;\n\n        // Gather from per-lane registers to per-warp shared arrays\n        for (int i = 0; i < chunk; ++i) {\n            int pos = lane * chunk + i;\n            if (pos < k) {\n                outDist[pos] = top_d[i];\n                outIdx [pos] = top_i[i];\n            }\n        }\n        __syncwarp(mask);\n\n        // Sort k elements (ascending by distance) cooperatively by the warp\n        warp_bitonic_sort(outDist, outIdx, k);\n\n        // Write to global memory\n        for (int i = lane; i < k; i += 32) {\n            int outPos = qid * k + i;\n            result[outPos].first  = outIdx[i];\n            result[outPos].second = outDist[i];\n        }\n    }\n}\n\n// Host entry point: configures and launches the kernel.\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k)\n{\n    // Kernel configuration\n    const int threads = KNN_THREADS_PER_BLOCK;\n    const int warpsPerBlock = threads / 32;\n    const int blocks = (query_count + warpsPerBlock - 1) / warpsPerBlock;\n\n    // Dynamic shared memory size calculation\n    size_t smem = 0;\n    smem += sizeof(float2) * (size_t)KNN_TILE_POINTS;       // tile\n    smem = (smem + 7) & ~size_t(7);                         // alignment\n    smem += sizeof(int)   * (size_t)warpsPerBlock;          // candCount\n    smem += sizeof(float) * (size_t)warpsPerBlock;          // maxDist\n    smem += sizeof(float) * (size_t)warpsPerBlock * (size_t)k; // candDist\n    smem += sizeof(int)   * (size_t)warpsPerBlock * (size_t)k; // candIdx\n\n    knn_kernel<<<blocks, threads, smem>>>(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 879, "completion_tokens": 20695, "total_tokens": 21574, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 16768, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db86da88190b17ae7253e168963", "custom_id": "knn06-03", "response": {"status_code": 200, "request_id": "ee1073a07a254512411c932e7f886995", "body": {"id": "chatcmpl-C4smOZUYzI7UTUjMEs0Wu4DB9RUbf", "object": "chat.completion", "created": 1755279920, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#include <utility>\n\n// This implementation uses one warp (32 threads) per query to compute k-NN over 2D points.\n// It processes the dataset in tiles cached in shared memory. Each warp maintains:\n// - a private, distributed intermediate top-k list in registers (k/32 entries per thread)\n// - a per-warp candidate buffer in shared memory with capacity k (indices and distances) and an atomic counter\n// When the candidate buffer fills, the warp merges the buffer with its intermediate result\n// using a cooperative, warp-parallel bitonic sort on 2k elements in shared memory, then updates max_distance.\n// After all tiles are processed, any remaining candidates are merged, and final sorted k-NN are written out.\n//\n// Assumptions:\n// - k is a power of two in [32, 1024]\n// - data_count >= k\n// - Arrays 'query', 'data', and 'result' are allocated with cudaMalloc\n// - H100/A100-class GPU, latest CUDA toolkit\n//\n// Hyperparameters:\n// - Warps per block: 4 (128 threads)\n// - Tile size: 2048 points cached in shared memory (16KB)\n// - Dynamic shared memory is used for tile cache and per-warp buffers/workspaces\n\n#ifndef FULL_WARP_MASK\n#define FULL_WARP_MASK 0xFFFFFFFFu\n#endif\n\n// Tunables\nstatic constexpr int WARPS_PER_BLOCK = 4;\nstatic constexpr int THREADS_PER_BLOCK = WARPS_PER_BLOCK * 32;\nstatic constexpr int TILE_POINTS = 2048;\nstatic constexpr int MAX_K = 1024;\nstatic constexpr int MAX_L_PER_THREAD = MAX_K / 32;\n\n// Compute squared Euclidean distance between 2D points\n__device__ __forceinline__ float squared_l2(const float2 a, const float2 b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    // Fused multiply-add for accuracy and throughput\n    return fmaf(dx, dx, dy * dy);\n}\n\n// Warp-wide reduction: maximum\n__device__ __forceinline__ float warpReduceMax(float v) {\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        v = fmaxf(v, __shfl_down_sync(FULL_WARP_MASK, v, offset));\n    }\n    return v;\n}\n\n// In-warp parallel bitonic sort on shared arrays of length 'N' (must be power-of-two).\n// Sorts ascending by distance; keeps indices aligned to distances.\n// Each warp cooperatively sorts its own window. Threads process elements in strides of 32.\n__device__ __forceinline__ void warp_bitonic_sort_asc(float* dist, int* idx, int N) {\n    // Standard bitonic network over N elements. We assume exactly one warp participates.\n    for (int size = 2; size <= N; size <<= 1) {\n        // Merge bitonic sequences of length 'size'\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            for (int i = threadIdx.x & 31; i < N; i += 32) {\n                int j = i ^ stride;\n                if (j > i) {\n                    // Determine sort direction for this pair\n                    bool ascending = ((i & size) == 0);\n                    float ai = dist[i];\n                    float aj = dist[j];\n                    int ii = idx[i];\n                    int ij = idx[j];\n                    // Compare-swap\n                    bool swap = (ai > aj) == ascending;\n                    if (swap) {\n                        dist[i] = aj; dist[j] = ai;\n                        idx[i]  = ij; idx[j]  = ii;\n                    }\n                }\n            }\n            __syncwarp();\n        }\n    }\n}\n\n// Merge per-warp candidate buffer (cand_dist/cand_idx, count) with the warp's current\n// intermediate top-k held in registers (top_dist/top_idx, L entries per thread).\n// Uses a bitonic sort on 2k elements in shared memory, then takes the k smallest.\n// Updates the warp's max_distance and clears the candidate count.\n__device__ void warp_merge_buffer_with_topk(\n    const int lane,\n    const int warp_local_id,\n    const int k,\n    const int L, // k/32\n    // Per-thread distributed top-k\n    float top_dist[MAX_L_PER_THREAD],\n    int   top_idx [MAX_L_PER_THREAD],\n    // Shared memory workspace for this warp\n    float* sort_dist, // length 2k\n    int*   sort_idx,  // length 2k\n    // Candidate buffer for this warp in shared memory\n    volatile float* cand_dist, // length k\n    volatile int*   cand_idx,  // length k\n    // Candidate count and max distance arrays in shared memory\n    volatile int* s_cand_count,\n    volatile float* s_max_dist\n) {\n    // Read current candidate count (safe via atomicAdd with 0)\n    int cand_count = 0;\n    if (lane == 0) {\n        cand_count = atomicAdd((int*)&s_cand_count[warp_local_id], 0);\n    }\n    cand_count = __shfl_sync(FULL_WARP_MASK, cand_count, 0);\n\n    // Prepare combined array of size 2k: first k = current top-k, next cand_count = candidates, rest = +INF\n    // Fill top-k part directly from per-thread registers\n    for (int i = 0; i < L; ++i) {\n        int pos = i * 32 + lane;\n        // pos in [0, k-1]\n        sort_dist[pos] = top_dist[i];\n        sort_idx[pos]  = top_idx[i];\n    }\n    __syncwarp();\n\n    // Fill candidates into positions [k .. k + cand_count - 1]\n    for (int i = lane; i < cand_count; i += 32) {\n        sort_dist[k + i] = cand_dist[i];\n        sort_idx [k + i] = cand_idx [i];\n    }\n    __syncwarp();\n\n    // Fill remaining positions [k + cand_count .. 2k-1] with +INF (ignored after sort)\n    const int totalN = 2 * k;\n    for (int i = k + cand_count + lane; i < totalN; i += 32) {\n        sort_dist[i] = CUDART_INF_F;\n        sort_idx [i] = -1;\n    }\n    __syncwarp();\n\n    // Bitonic sort ascending\n    warp_bitonic_sort_asc(sort_dist, sort_idx, totalN);\n\n    // Take first k elements as new top-k and distribute back to registers (sorted ascending)\n    for (int i = 0; i < L; ++i) {\n        int pos = i * 32 + lane; // 0..k-1\n        top_dist[i] = sort_dist[pos];\n        top_idx [i] = sort_idx [pos];\n    }\n\n    // Update max_distance to the (k-1)-th element (last of top-k)\n    float new_max = 0.0f;\n    if (lane == 0) {\n        new_max = sort_dist[k - 1];\n        s_max_dist[warp_local_id] = new_max;\n        // Reset candidate count to zero\n        s_cand_count[warp_local_id] = 0;\n    }\n    __syncwarp();\n}\n\n// Main kernel\n__global__ void knn_kernel(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data,  int data_count,\n    std::pair<int, float>* __restrict__ result,\n    int k\n) {\n    // One warp per query\n    const int lane = threadIdx.x & 31;\n    const int warp_id_in_block = threadIdx.x >> 5;\n    const int global_warp_id = blockIdx.x * WARPS_PER_BLOCK + warp_id_in_block;\n    if (global_warp_id >= query_count) return;\n\n    // Number of (index, distance) pairs per thread in the warp's distributed top-k\n    const int L = k >> 5; // k / 32; since k is a power of two >= 32\n\n    // Shared memory layout:\n    // [0 .. TILE_POINTS-1]: float2 tile cache\n    // Then per-warp regions of size perWarpBytes = 24*k bytes:\n    //   cand_idx[k], cand_dist[k], sort_idx[2k], sort_dist[2k]\n    extern __shared__ unsigned char smem[];\n    float2* s_points = reinterpret_cast<float2*>(smem);\n    unsigned char* s_after_points = reinterpret_cast<unsigned char*>(s_points + TILE_POINTS);\n\n    const size_t perWarpBytes =\n        (size_t)k * sizeof(int) + (size_t)k * sizeof(float) + // candidate buffer\n        (size_t)(2 * k) * sizeof(int) + (size_t)(2 * k) * sizeof(float); // sort workspace\n\n    unsigned char* myWarpBase = s_after_points + (size_t)warp_id_in_block * perWarpBytes;\n\n    // Candidate buffer for this warp\n    int*   s_cand_idx  = reinterpret_cast<int*>(myWarpBase);\n    float* s_cand_dist = reinterpret_cast<float*>(s_cand_idx + k);\n\n    // Sort workspace for this warp\n    int*   s_sort_idx  = reinterpret_cast<int*>(s_cand_dist + k);\n    float* s_sort_dist = reinterpret_cast<float*>(s_sort_idx + 2 * k);\n\n    // Per-warp candidate counts and max distance in static shared memory\n    __shared__ int   s_candidate_count[WARPS_PER_BLOCK];\n    __shared__ float s_max_distance   [WARPS_PER_BLOCK];\n\n    // Initialize per-warp counters and thresholds\n    if (lane == 0) {\n        s_candidate_count[warp_id_in_block] = 0;\n        s_max_distance   [warp_id_in_block] = CUDART_INF_F;\n    }\n    __syncwarp();\n\n    // Load this warp's query point\n    float2 q = query[global_warp_id];\n\n    // Initialize per-thread portion of the intermediate top-k (private copy per warp, distributed across threads)\n    float top_dist[MAX_L_PER_THREAD];\n    int   top_idx [MAX_L_PER_THREAD];\n    for (int i = 0; i < L; ++i) {\n        top_dist[i] = CUDART_INF_F;\n        top_idx [i] = -1;\n    }\n\n    // Process dataset in tiles cached in shared memory\n    for (int tile_start = 0; tile_start < data_count; tile_start += TILE_POINTS) {\n        const int tile_len = min(TILE_POINTS, data_count - tile_start);\n\n        // Load tile into shared memory cooperatively by the whole block\n        for (int i = threadIdx.x; i < tile_len; i += blockDim.x) {\n            s_points[i] = data[tile_start + i];\n        }\n        __syncthreads();\n\n        // Each warp processes all points in the tile in chunks of 32 (one per lane)\n        for (int t = 0; t < tile_len; t += 32) {\n            const int local_idx = t + lane;\n            const bool valid = (local_idx < tile_len);\n\n            // Snapshot of current threshold for this warp to filter candidates\n            float max_threshold = s_max_distance[warp_id_in_block];\n\n            // Compute distance to candidate point\n            float d = CUDART_INF_F;\n            int   gidx = -1;\n            if (valid) {\n                float2 p = s_points[local_idx];\n                d = squared_l2(p, q);\n                gidx = tile_start + local_idx;\n            }\n\n            // Filter using current threshold\n            const bool qualifies = valid && (d < max_threshold);\n\n            // Warp ballot to compact qualifying candidates in this chunk\n            unsigned mask = __ballot_sync(FULL_WARP_MASK, qualifies);\n            int qcount = __popc(mask);\n            if (qcount > 0) {\n                // Position among qualifiers in this chunk\n                unsigned lane_mask = (lane == 0) ? 0u : ((1u << lane) - 1u);\n                int my_offset = __popc(mask & lane_mask);\n\n                // Reserve space in candidate buffer for all qualifiers in this chunk\n                int base = 0;\n                if (lane == 0) {\n                    base = atomicAdd(&s_candidate_count[warp_id_in_block], qcount);\n                }\n                base = __shfl_sync(FULL_WARP_MASK, base, 0);\n\n                // How many can we insert before reaching capacity k?\n                int allowed = 0;\n                if (base < k) {\n                    int remaining = k - base;\n                    allowed = remaining < qcount ? remaining : qcount;\n                }\n\n                // Insert the first 'allowed' qualifiers\n                if (qualifies && my_offset < allowed) {\n                    int pos = base + my_offset;\n                    s_cand_dist[pos] = d;\n                    s_cand_idx [pos] = gidx;\n                }\n                __syncwarp();\n\n                // If overflow occurred, merge buffer with intermediate result and insert leftovers\n                if (qcount > allowed) {\n                    // Merge now (buffer is full)\n                    warp_merge_buffer_with_topk(\n                        lane, warp_id_in_block, k, L,\n                        top_dist, top_idx,\n                        s_sort_dist, s_sort_idx,\n                        s_cand_dist, s_cand_idx,\n                        s_candidate_count, s_max_distance\n                    );\n                    __syncwarp();\n\n                    // Prepare leftover mask by removing the first 'allowed' set bits from mask\n                    unsigned leftover_mask = mask;\n                    if (lane == 0) {\n                        int to_skip = allowed;\n                        while (to_skip-- > 0) {\n                            // Clear least significant set bit\n                            leftover_mask &= (leftover_mask - 1u);\n                        }\n                    }\n                    leftover_mask = __shfl_sync(FULL_WARP_MASK, leftover_mask, 0);\n                    int leftover_count = qcount - allowed;\n\n                    // Reserve space for leftovers (fresh buffer, count=0 after merge)\n                    int base2 = 0;\n                    if (lane == 0) {\n                        base2 = atomicAdd(&s_candidate_count[warp_id_in_block], leftover_count);\n                    }\n                    base2 = __shfl_sync(FULL_WARP_MASK, base2, 0);\n\n                    // Insert leftovers contiguously\n                    bool is_leftover = ((leftover_mask >> lane) & 1u) != 0u;\n                    if (is_leftover) {\n                        int my_off2 = __popc(leftover_mask & lane_mask);\n                        int pos2 = base2 + my_off2;\n                        s_cand_dist[pos2] = d;\n                        s_cand_idx [pos2] = gidx;\n                    }\n                    __syncwarp();\n\n                    // If the buffer is exactly full after inserting leftovers, merge immediately\n                    int cnt_after = 0;\n                    if (lane == 0) {\n                        cnt_after = atomicAdd(&s_candidate_count[warp_id_in_block], 0);\n                    }\n                    cnt_after = __shfl_sync(FULL_WARP_MASK, cnt_after, 0);\n                    if (cnt_after >= k) {\n                        warp_merge_buffer_with_topk(\n                            lane, warp_id_in_block, k, L,\n                            top_dist, top_idx,\n                            s_sort_dist, s_sort_idx,\n                            s_cand_dist, s_cand_idx,\n                            s_candidate_count, s_max_distance\n                        );\n                    }\n                }\n            }\n\n            __syncwarp();\n        }\n\n        __syncthreads(); // ensure all warps finished reading this tile before loading the next\n    }\n\n    // After all tiles, if there are remaining candidates, merge them\n    int final_count = 0;\n    if (lane == 0) {\n        final_count = atomicAdd(&s_candidate_count[warp_id_in_block], 0);\n    }\n    final_count = __shfl_sync(FULL_WARP_MASK, final_count, 0);\n    if (final_count > 0) {\n        warp_merge_buffer_with_topk(\n            lane, warp_id_in_block, k, L,\n            top_dist, top_idx,\n            s_sort_dist, s_sort_idx,\n            s_cand_dist, s_cand_idx,\n            s_candidate_count, s_max_distance\n        );\n    }\n\n    // Write final results for this query in ascending order of distance\n    // Each thread writes its L entries at positions [i*32 + lane]\n    int out_base = global_warp_id * k;\n    for (int i = 0; i < L; ++i) {\n        int pos = i * 32 + lane;\n        int out_idx = out_base + pos;\n        result[out_idx].first  = top_idx[i];\n        result[out_idx].second = top_dist[i];\n    }\n}\n\n// Host interface\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    // Launch parameters\n    dim3 block(THREADS_PER_BLOCK);\n    dim3 grid((query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK);\n\n    // Dynamic shared memory size: tile + per-warp buffers/workspaces\n    size_t tile_bytes = (size_t)TILE_POINTS * sizeof(float2);\n    size_t per_warp_bytes =\n        (size_t)k * sizeof(int) + (size_t)k * sizeof(float) +   // candidate buffer\n        (size_t)(2 * k) * sizeof(int) + (size_t)(2 * k) * sizeof(float); // sort workspace\n    size_t shared_bytes = tile_bytes + WARPS_PER_BLOCK * per_warp_bytes;\n\n    // Opt-in to larger dynamic shared memory if necessary\n    cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)shared_bytes);\n\n    // Launch\n    knn_kernel<<<grid, block, shared_bytes>>>(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 879, "completion_tokens": 20454, "total_tokens": 21333, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 16576, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db886d081908ce4e534619cd75c", "custom_id": "knn06-04", "response": {"status_code": 200, "request_id": "54db278b2b279617a80ebdc903bdf9b0", "body": {"id": "chatcmpl-C4tDRzP2N7qLeZuPuINld7a6w3UEG", "object": "chat.completion", "created": 1755281597, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#include <cuda.h>\n#include <stdint.h>\n\n// This implementation computes k-NN for 2D points where each query is handled by a single warp (32 threads).\n// It follows the requirements:\n// - Batches (tiles) of data points are cooperatively loaded into shared memory by the full block.\n// - Each warp processes one query and caches candidate neighbors in a per-warp shared memory buffer.\n// - Candidates closer than the current max_distance (distance of the k-th best) are added using atomicAdd\n//   (warp-aggregated) to a per-warp candidate buffer. Whenever the buffer is full, it is merged with the\n//   intermediate result using warp-cooperative routines (reductions via warp shuffles).\n// - After all batches are processed, if the candidate buffer is not empty, a final merge is performed.\n// - The intermediate result (top-k) is kept unsorted during processing. At the end, results are written\n//   to global memory in ascending distance order using repeated warp-cooperative arg-min selection.\n//\n// Performance notes:\n// - Per-warp intermediate result and candidate buffer are placed in shared memory to avoid global memory round-trips.\n// - Memory footprint per warp is ~16*k bytes (best + candidates), with k in [32, 1024]. With 8 warps per block and\n//   tile size 4096 points, the total dynamic shared memory used is below the 164KB per-block limit on A100, and also\n//   well within H100 limits.\n// - Warp-cooperative reductions are used to find argmax/argmin efficiently.\n// - Warp-aggregated atomicAdd is used to reduce contention when appending candidates.\n\n// Simple POD equivalent of std::pair<int,float> for device-side writes.\n// The host run_knn casts the std::pair<int,float>* to this type. Layout is identical on typical ABIs.\nstruct PairIF {\n    int   first;\n    float second;\n};\n\n// Warp size is assumed 32 on NVIDIA GPUs.\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Utility: get lane id in a warp\nstatic __device__ __forceinline__ int lane_id() {\n    return threadIdx.x & (WARP_SIZE - 1);\n}\n\n// Utility: get warp id within the block\nstatic __device__ __forceinline__ int warp_id_in_block() {\n    return threadIdx.x >> 5;\n}\n\n// Utility: full warp mask\nstatic __device__ __forceinline__ unsigned full_mask() {\n    return 0xFFFFFFFFu;\n}\n\n// Warp-cooperative argmax over an array \"arr\" of length k stored in shared memory.\n// Each lane scans a strided subset and keeps its local max, then we reduce across the warp.\n// Returns (maxVal, maxIdx) via references. Only lane 0 holds meaningful outputs;\n// other lanes receive broadcasted values at the end for convenience.\nstatic __device__ __forceinline__ void warp_argmax_shared(const float* arr, int k, float& outVal, int& outIdx) {\n    const int lane = lane_id();\n    float localMax = -CUDART_INF_F;\n    int localIdx = -1;\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        float v = arr[i];\n        if (v > localMax) {\n            localMax = v;\n            localIdx = i;\n        }\n    }\n    // Warp reduction: argmax\n    unsigned mask = full_mask();\n    for (int offset = WARP_SIZE >> 1; offset > 0; offset >>= 1) {\n        float othV = __shfl_down_sync(mask, localMax, offset);\n        int   othI = __shfl_down_sync(mask, localIdx, offset);\n        if (othV > localMax) {\n            localMax = othV;\n            localIdx = othI;\n        }\n    }\n    // Broadcast lane 0 result to all lanes so every lane can read the current max if needed.\n    outVal = __shfl_sync(mask, localMax, 0);\n    outIdx = __shfl_sync(mask, localIdx, 0);\n}\n\n// Warp-cooperative argmin over an array \"arr\" of length k stored in shared memory.\n// Each lane scans a strided subset and keeps its local min, then reduce across the warp.\nstatic __device__ __forceinline__ void warp_argmin_shared(const float* arr, int k, float& outVal, int& outIdx) {\n    const int lane = lane_id();\n    float localMin = CUDART_INF_F;\n    int localIdx = -1;\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        float v = arr[i];\n        if (v < localMin) {\n            localMin = v;\n            localIdx = i;\n        }\n    }\n    unsigned mask = full_mask();\n    for (int offset = WARP_SIZE >> 1; offset > 0; offset >>= 1) {\n        float othV = __shfl_down_sync(mask, localMin, offset);\n        int   othI = __shfl_down_sync(mask, localIdx, offset);\n        if (othV < localMin) {\n            localMin = othV;\n            localIdx = othI;\n        }\n    }\n    outVal = __shfl_sync(mask, localMin, 0);\n    outIdx = __shfl_sync(mask, localIdx, 0);\n}\n\n// Merge per-warp candidate buffer into per-warp intermediate top-k result.\n// - bestIdx, bestDist: arrays of length k (shared), storing current top-k (unsorted).\n// - candIdx, candDist: arrays of length >= candCount (shared), storing candidates.\n// - candCount: number of candidates in the buffer (shared).\n// - maxDistRef: in/out, distance of current k-th nearest (the maximum in bestDist).\n// Implementation details:\n// - We maintain bestDist/Idx as an unsorted set of k best. We repeatedly select candidates one-by-one,\n//   and if a candidate distance is less than current maxDist, we replace the worst element.\n// - The \"worst\" (max) is found via warp-cooperative argmax reduction.\n// - Complexity per merge is O(k * candCount / WARP_SIZE) comparisons, acceptable for k<=1024.\nstatic __device__ __forceinline__ void merge_candidates_warp(\n    int* bestIdx, float* bestDist, int k,\n    const int* candIdx, const float* candDist, int& candCount,\n    float& maxDistRef)\n{\n    // Compute current worst (max) among bestDist\n    float curMax;\n    int curMaxIdx;\n    warp_argmax_shared(bestDist, k, curMax, curMaxIdx);\n\n    // Process all candidates sequentially; lane 0 performs stores, warp helps to find max when replaced.\n    // We read cand arrays directly; they belong exclusively to this warp's region in shared memory.\n    const int lane = lane_id();\n    for (int t = 0; t < candCount; ++t) {\n        // Only lane 0 performs the comparison and potential replacement using the current curMax.\n        float cd;\n        int ci;\n        if (lane == 0) {\n            cd = candDist[t];\n            ci = candIdx[t];\n            // Insert if closer than current k-th nearest\n            if (cd < curMax) {\n                bestDist[curMaxIdx] = cd;\n                bestIdx[curMaxIdx]  = ci;\n            }\n        }\n        // Broadcast lane0's decision to all lanes: we need to know if replacement happened to refresh curMax.\n        unsigned mask = full_mask();\n        float cd_b = __shfl_sync(mask, cd, 0);\n        int   ci_b = __shfl_sync(mask, ci, 0);\n        bool replaced = __shfl_sync(mask, (int)(cd_b < curMax), 0) != 0;\n\n        // If replaced, recompute the current worst among bestDist; else keep previous curMax.\n        if (replaced) {\n            // Ensure the replacement is visible to all lanes before reading bestDist\n            __syncwarp();\n            warp_argmax_shared(bestDist, k, curMax, curMaxIdx);\n        }\n        // Otherwise, no change to curMax/curMaxIdx\n    }\n\n    // Update shared max distance reference and reset candidate count.\n    if (lane == 0) {\n        maxDistRef = curMax;\n        candCount = 0;\n    }\n    __syncwarp();\n}\n\n// KNN kernel: each warp processes one query point.\n// Shared memory layout (dynamic):\n// [0]                          : float2 tilePoints[TILE_POINTS]\n// [tilePoints * sizeof(float2)]: per-warp arrays:\n//   bestDist  [warpsPerBlock * k] (float)\n//   bestIdx   [warpsPerBlock * k] (int)\n//   candDist  [warpsPerBlock * k] (float)\n//   candIdx   [warpsPerBlock * k] (int)\n//   candCount [warpsPerBlock]     (int)\n//   maxDist   [warpsPerBlock]     (float)\n__global__ void knn_kernel_2d(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data,  int data_count,\n    PairIF* __restrict__ result, int k,\n    int tile_points) // number of points per tile loaded into shared memory\n{\n    extern __shared__ unsigned char smem_raw[];\n    unsigned char* smem_ptr = smem_raw;\n\n    const int warpsPerBlock = blockDim.x / WARP_SIZE;\n    const int warpInBlock   = warp_id_in_block();\n    const int lane          = lane_id();\n\n    // Per-block shared tiles for data points\n    float2* tile = reinterpret_cast<float2*>(smem_ptr);\n    size_t tile_bytes = (size_t)tile_points * sizeof(float2);\n    smem_ptr += tile_bytes;\n\n    // Helper lambda to align pointer to alignment bytes\n    auto align_ptr = [&smem_ptr](size_t alignment) {\n        uintptr_t addr = reinterpret_cast<uintptr_t>(smem_ptr);\n        size_t mis = addr % alignment;\n        if (mis != 0) smem_ptr += (alignment - mis);\n    };\n\n    // Align to 16 bytes for safety\n    align_ptr(16);\n\n    // Per-warp arrays in shared memory\n    float* bestDist_all = reinterpret_cast<float*>(smem_ptr);\n    smem_ptr += (size_t)warpsPerBlock * k * sizeof(float);\n    int*   bestIdx_all  = reinterpret_cast<int*>(smem_ptr);\n    smem_ptr += (size_t)warpsPerBlock * k * sizeof(int);\n    float* candDist_all = reinterpret_cast<float*>(smem_ptr);\n    smem_ptr += (size_t)warpsPerBlock * k * sizeof(float);\n    int*   candIdx_all  = reinterpret_cast<int*>(smem_ptr);\n    smem_ptr += (size_t)warpsPerBlock * k * sizeof(int);\n\n    // Per-warp counters and max distance\n    int*   candCount_all = reinterpret_cast<int*>(smem_ptr);\n    smem_ptr += (size_t)warpsPerBlock * sizeof(int);\n    float* maxDist_all   = reinterpret_cast<float*>(smem_ptr);\n    smem_ptr += (size_t)warpsPerBlock * sizeof(float);\n\n    // Compute global warp id and query index handled by this warp\n    int warpGlobal = (blockIdx.x * warpsPerBlock) + warpInBlock;\n    if (warpGlobal >= query_count) return; // nothing to do\n\n    // Pointers to this warp's slices\n    float* bestDist = bestDist_all + (size_t)warpInBlock * k;\n    int*   bestIdx  = bestIdx_all  + (size_t)warpInBlock * k;\n    float* candDist = candDist_all + (size_t)warpInBlock * k;\n    int*   candIdx  = candIdx_all  + (size_t)warpInBlock * k;\n    int&   candCountRef = candCount_all[warpInBlock];\n    float& maxDistRef   = maxDist_all[warpInBlock];\n\n    // Initialize per-warp intermediate result to +inf distances and -1 indices\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        bestDist[i] = CUDART_INF_F;\n        bestIdx[i]  = -1;\n    }\n    if (lane == 0) {\n        candCountRef = 0;\n        maxDistRef = CUDART_INF_F; // current \"k-th nearest\" distance\n    }\n    __syncwarp();\n\n    // Load this warp's query point\n    float2 q = query[warpGlobal];\n    float qx = q.x, qy = q.y;\n\n    // Process data points in tiles\n    for (int tileBase = 0; tileBase < data_count; tileBase += tile_points) {\n        int count = data_count - tileBase;\n        if (count > tile_points) count = tile_points;\n\n        // Block-cooperative load of the tile into shared memory\n        for (int idx = threadIdx.x; idx < count; idx += blockDim.x) {\n            tile[idx] = data[tileBase + idx];\n        }\n        __syncthreads(); // ensure the tile is fully loaded\n\n        // Each warp processes all points in the tile\n        for (int i = lane; i < count; i += WARP_SIZE) {\n            float2 p = tile[i];\n            float dx = p.x - qx;\n            float dy = p.y - qy;\n            float dist = fmaf(dx, dx, dy * dy); // squared L2 distance\n            int   gidx = tileBase + i;\n\n            // Determine if this distance is a candidate (strictly closer than current maxDist)\n            float curMax = maxDistRef; // read current shared maxDist for this warp\n            int want = (dist < curMax) ? 1 : 0;\n\n            // Build a mask of lanes that want to insert for this \"round\"\n            unsigned mask = __ballot_sync(full_mask(), want != 0);\n            int nToInsert = __popc(mask);\n\n            if (nToInsert > 0) {\n                // Ensure there is enough space in the candidate buffer. If not, merge first.\n                if (lane == 0) {\n                    int cc = candCountRef;\n                    if (cc + nToInsert > k) {\n                        // Merge before we insert these candidates to free up space and update maxDist\n                        // Note: This merge uses only warp-local structures; no need for block sync.\n                        merge_candidates_warp(bestIdx, bestDist, k, candIdx, candDist, candCountRef, maxDistRef);\n                    }\n                }\n                __syncwarp();\n                // After merge, maxDist may have changed; recompute predicate and mask\n                curMax = maxDistRef;\n                want = (dist < curMax) ? 1 : 0;\n                mask = __ballot_sync(full_mask(), want != 0);\n                nToInsert = __popc(mask);\n\n                if (nToInsert > 0) {\n                    // Warp-aggregated atomicAdd to reserve a contiguous slice in candidate buffer\n                    int base = 0;\n                    if (lane == 0) {\n                        base = atomicAdd(&candCountRef, nToInsert);\n                    }\n                    base = __shfl_sync(full_mask(), base, 0);\n\n                    // Compute each lane's offset among the inserting lanes\n                    int rank = __popc(mask & ((1u << lane) - 1));\n                    if (want) {\n                        int pos = base + rank;\n                        if (pos < k) {\n                            candDist[pos] = dist;\n                            candIdx[pos]  = gidx;\n                        }\n                    }\n                    __syncwarp();\n\n                    // If we just filled the buffer exactly to capacity, trigger a merge now.\n                    if (lane == 0) {\n                        if (candCountRef >= k) {\n                            merge_candidates_warp(bestIdx, bestDist, k, candIdx, candDist, candCountRef, maxDistRef);\n                        }\n                    }\n                    __syncwarp();\n                }\n            }\n        } // end tile inner loop\n\n        __syncthreads(); // ensure all warps are done reading the tile before loading the next\n    } // end tiles loop\n\n    // Final merge if any candidates remain\n    if (lane == 0) {\n        if (candCountRef > 0) {\n            merge_candidates_warp(bestIdx, bestDist, k, candIdx, candDist, candCountRef, maxDistRef);\n        }\n    }\n    __syncwarp();\n\n    // At this point, bestDist/bestIdx hold the k nearest neighbors (unsorted).\n    // We must output them in ascending distance order. We perform k repeated warp-cooperative argmin selections.\n    // After selecting a min at position selIdx, we mark bestDist[selIdx] = +inf to exclude it from further selections.\n    // Lane 0 writes the outputs to global memory.\n    int outBase = warpGlobal * k;\n    for (int j = 0; j < k; ++j) {\n        float mnVal;\n        int mnIdx;\n        warp_argmin_shared(bestDist, k, mnVal, mnIdx);\n        // Write result j (j-th nearest neighbor)\n        if (lane == 0) {\n            PairIF out;\n            out.first  = bestIdx[mnIdx];\n            out.second = mnVal;\n            result[outBase + j] = out;\n            // Mark as used\n            bestDist[mnIdx] = CUDART_INF_F;\n        }\n        __syncwarp();\n    }\n}\n\n// Host interface\nextern \"C\" void run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    // Choose block and grid configuration.\n    // We use 256 threads per block (8 warps). Each warp processes one query.\n    const int threadsPerBlock = 256;\n    const int warpsPerBlock = threadsPerBlock / WARP_SIZE;\n\n    // Compute number of blocks to cover all queries (one warp per query).\n    int totalWarps = query_count;\n    int blocks = (totalWarps + warpsPerBlock - 1) / warpsPerBlock;\n\n    // Prefer tile size of 4096 points for good bandwidth, adjust down if shared memory is tight.\n    int tilePoints = 4096;\n\n    // Compute dynamic shared memory requirement:\n    // tilePoints * sizeof(float2) + warpsPerBlock * (2*k*(sizeof(float)+sizeof(int)) + sizeof(int) + sizeof(float))\n    size_t perWarpMem = (size_t)2 * k * (sizeof(float) + sizeof(int)) + sizeof(int) + sizeof(float);\n    size_t smemBytes = (size_t)tilePoints * sizeof(float2) + (size_t)warpsPerBlock * perWarpMem;\n\n    // Query device shared memory limits to ensure our configuration fits.\n    int device = 0;\n    cudaGetDevice(&device);\n    int maxOptin = 0;\n    cudaDeviceGetAttribute(&maxOptin, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    if (maxOptin <= 0) {\n        // Fallback to legacy limit if opt-in attribute is not exposed\n        cudaDeviceProp prop;\n        cudaGetDeviceProperties(&prop, device);\n        maxOptin = (int)prop.sharedMemPerBlockOptin;\n        if (maxOptin <= 0) maxOptin = (int)prop.sharedMemPerBlock;\n    }\n\n    // Adjust tilePoints down if needed to fit within the maximum dynamic shared memory\n    // Reserve a small alignment cushion (e.g., 256 bytes).\n    const size_t cushion = 256;\n    if (smemBytes + cushion > (size_t)maxOptin) {\n        size_t maxForTile = (size_t)maxOptin - (size_t)warpsPerBlock * perWarpMem;\n        if ((long long)maxForTile < 0) maxForTile = 0;\n        tilePoints = (int)(maxForTile / sizeof(float2));\n        // Make tilePoints a positive multiple of threadsPerBlock for balanced loads\n        if (tilePoints < threadsPerBlock) tilePoints = threadsPerBlock;\n        tilePoints = (tilePoints / threadsPerBlock) * threadsPerBlock;\n        if (tilePoints <= 0) tilePoints = threadsPerBlock; // ensure positivity\n        smemBytes = (size_t)tilePoints * sizeof(float2) + (size_t)warpsPerBlock * perWarpMem;\n    }\n\n    // Opt-in to the required dynamic shared memory size.\n    cudaFuncSetAttribute(knn_kernel_2d, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)smemBytes);\n    // Prefer shared memory carveout (100% if possible) to maximize shared memory availability.\n    cudaFuncSetAttribute(knn_kernel_2d, cudaFuncAttributePreferredSharedMemoryCarveout, 100);\n\n    // Launch the kernel\n    PairIF* result_alias = reinterpret_cast<PairIF*>(result);\n    knn_kernel_2d<<<blocks, threadsPerBlock, smemBytes>>>(query, query_count, data, data_count, result_alias, k, tilePoints);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 879, "completion_tokens": 17832, "total_tokens": 18711, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 13248, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db8beb48190b695df19b588625e", "custom_id": "knn06-05", "response": {"status_code": 200, "request_id": "a9f07022363722b45a71bf8dc1792b14", "body": {"id": "chatcmpl-C4smTCQQcmhVqlHaVkvCveM631Srq", "object": "chat.completion", "created": 1755279925, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cmath>\n#include <cstdint>\n#include <utility>\n\n// Optimized CUDA KNN for 2D points (squared Euclidean distance).\n// Design notes:\n// - One warp (32 threads) processes one query point.\n// - The block loads data points in tiles into shared memory.\n// - Each warp maintains its own \"intermediate\" top-k list (private per query) in shared memory.\n// - Each warp also owns a shared candidate buffer of size k and a shared integer count updated with atomicAdd.\n// - When a candidate buffer becomes full, the warp merges it with its intermediate result using a warp-cooperative bitonic sort over 2k elements.\n// - After the final tile, any remaining candidates are merged.\n// - Merging and shared-memory usage are carefully designed to satisfy the constraints and maximize performance on modern NVIDIA GPUs.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Choose number of warps per block. 4 warps => 128 threads per block.\n// This balances shared memory footprint and occupancy for k up to 1024.\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 4\n#endif\n\n// Output pair type with the same layout as std::pair<int, float>.\nstruct PairIF {\n    int   first;\n    float second;\n};\n\n// Align up helper (host/device)\nstatic inline __host__ __device__ size_t align_up(size_t x, size_t a) {\n    return (x + (a - 1)) & ~(a - 1);\n}\n\n// Warp-cooperative bitonic sort over pairs (distance asc; tie-breaker by index asc for determinism).\n// The arrays d[0..n-1], idx[0..n-1] reside in shared memory and are private to the calling warp.\n// n must be a power of two (we use n = 2*k; k is guaranteed power of two).\n__device__ inline void warp_bitonic_sort_pairs(float* d, int* idx, int n, unsigned warp_mask) {\n    // Standard bitonic network: O(n log^2 n).\n    for (int size = 2; size <= n; size <<= 1) {\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            int lane = threadIdx.x & (WARP_SIZE - 1);\n            for (int i = lane; i < n; i += WARP_SIZE) {\n                int j = i ^ stride;\n                if (j > i) {\n                    bool up = ((i & size) == 0); // true = ascending region\n                    float di = d[i];\n                    float dj = d[j];\n                    int   ii = idx[i];\n                    int   ij = idx[j];\n                    // Ascending: keep smaller at low index; Descending: keep larger at low index.\n                    bool swap = (di > dj);\n                    if (di == dj) { // tie-breaker (optional)\n                        swap = (ii > ij);\n                    }\n                    if (swap == up) {\n                        d[i] = dj; d[j] = di;\n                        idx[i] = ij; idx[j] = ii;\n                    }\n                }\n            }\n            __syncwarp(warp_mask);\n        }\n    }\n}\n\n// Merge a warp's candidate buffer into its intermediate top-k using a warp-cooperative bitonic sort.\n// Shared memory layout is passed via pointers; all arrays are disjoint per warp.\n// Steps:\n//  1. Copy current top-k (k) and current candidates (<=k) into a work buffer of size 2k,\n//     padding with +inf for missing entries.\n//  2. Sort the 2k pairs (ascending by distance).\n//  3. Write back the first k pairs as the new top-k; update max_distance and topk_size.\n//  4. Reset candidate count to 0.\n__device__ inline void warp_merge_candidates(\n    int warp_id,\n    int k,\n    float* topk_dist, int* topk_idx,\n    float* cand_dist, int* cand_idx,\n    volatile int* cand_count,  // updated with atomics\n    float* work_dist, int* work_idx,\n    float* max_dist, int* topk_size,\n    unsigned warp_mask)\n{\n    const int topk_base = warp_id * k;\n    const int cand_base = warp_id * k;\n    const int work_base = warp_id * (2 * k);\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n\n    // Clamp candidate count to k for copying; extra increments beyond k are ignored here.\n    int ccount = cand_count[warp_id];\n    if (ccount < 0) ccount = 0;\n    if (ccount > k) ccount = k;\n\n    // Copy current top-k into work[0..k-1]\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        work_dist[work_base + i] = topk_dist[topk_base + i];\n        work_idx[work_base + i]  = topk_idx[topk_base + i];\n    }\n    // Copy candidates into work[k..2k-1], padding with +inf\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        if (i < ccount) {\n            work_dist[work_base + k + i] = cand_dist[cand_base + i];\n            work_idx[work_base + k + i]  = cand_idx[cand_base + i];\n        } else {\n            work_dist[work_base + k + i] = CUDART_INF_F;\n            work_idx[work_base + k + i]  = -1;\n        }\n    }\n    __syncwarp(warp_mask);\n\n    // Sort 2k elements (ascending).\n    warp_bitonic_sort_pairs(work_dist + work_base, work_idx + work_base, 2 * k, warp_mask);\n\n    // Write back first k as new top-k.\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        topk_dist[topk_base + i] = work_dist[work_base + i];\n        topk_idx[topk_base + i]  = work_idx[work_base + i];\n    }\n\n    // Update metadata.\n    if (lane == 0) {\n        int new_size = topk_size[warp_id] + ccount;\n        if (new_size > k) new_size = k;\n        topk_size[warp_id] = new_size;\n        max_dist[warp_id] = topk_dist[topk_base + (k - 1)];\n        // Reset candidate count to 0 after merge.\n        cand_count[warp_id] = 0;\n    }\n    __syncwarp(warp_mask);\n}\n\n// Kernel to compute k-NN for 2D points. One warp per query.\n// Parameters:\n//  - query: device pointer to float2 query points\n//  - query_count: number of queries\n//  - data: device pointer to float2 data points\n//  - data_count: number of data points\n//  - out: device pointer to output PairIF (index, distance)\n//  - k: number of neighbors (power of two, between 32 and 1024 inclusive)\n//  - tile_points: number of data points per shared-memory tile (chosen at launch)\n__global__ void knn2d_warp_kernel(\n    const float2* __restrict__ query,\n    int query_count,\n    const float2* __restrict__ data,\n    int data_count,\n    PairIF* __restrict__ out,\n    int k,\n    int tile_points)\n{\n    extern __shared__ unsigned char shmem[];\n    size_t off = 0;\n\n    // Shared memory layout:\n    // 1) Shared tile of data points\n    off = align_up(off, 8);\n    float2* s_points = reinterpret_cast<float2*>(shmem + off);\n    off += static_cast<size_t>(tile_points) * sizeof(float2);\n\n    // 2) Per-warp metadata arrays\n    off = align_up(off, 4);\n    volatile int* s_cand_count = reinterpret_cast<volatile int*>(shmem + off);\n    off += WARPS_PER_BLOCK * sizeof(int);\n\n    off = align_up(off, 4);\n    int* s_topk_size = reinterpret_cast<int*>(shmem + off);\n    off += WARPS_PER_BLOCK * sizeof(int);\n\n    off = align_up(off, 4);\n    float* s_max_dist = reinterpret_cast<float*>(shmem + off);\n    off += WARPS_PER_BLOCK * sizeof(float);\n\n    // 3) Per-warp arrays: topk (k), candidates (k), and merge work buffer (2k)\n    off = align_up(off, 4);\n    int* s_topk_idx = reinterpret_cast<int*>(shmem + off);\n    off += static_cast<size_t>(WARPS_PER_BLOCK) * k * sizeof(int);\n\n    off = align_up(off, 4);\n    float* s_topk_dist = reinterpret_cast<float*>(shmem + off);\n    off += static_cast<size_t>(WARPS_PER_BLOCK) * k * sizeof(float);\n\n    off = align_up(off, 4);\n    int* s_cand_idx = reinterpret_cast<int*>(shmem + off);\n    off += static_cast<size_t>(WARPS_PER_BLOCK) * k * sizeof(int);\n\n    off = align_up(off, 4);\n    float* s_cand_dist = reinterpret_cast<float*>(shmem + off);\n    off += static_cast<size_t>(WARPS_PER_BLOCK) * k * sizeof(float);\n\n    off = align_up(off, 4);\n    int* s_work_idx = reinterpret_cast<int*>(shmem + off);\n    off += static_cast<size_t>(WARPS_PER_BLOCK) * 2 * k * sizeof(int);\n\n    off = align_up(off, 4);\n    float* s_work_dist = reinterpret_cast<float*>(shmem + off);\n    off += static_cast<size_t>(WARPS_PER_BLOCK) * 2 * k * sizeof(float);\n\n    // Warp and query mapping\n    const int lane     = threadIdx.x & (WARP_SIZE - 1);\n    const int warp_id  = threadIdx.x / WARP_SIZE;\n    const int global_warp = blockIdx.x * WARPS_PER_BLOCK + warp_id;\n    const bool active = (global_warp < query_count);\n    const unsigned warp_mask = __activemask();\n\n    // Initialize per-warp metadata and arrays.\n    if (lane == 0) {\n        s_cand_count[warp_id] = 0;\n        s_topk_size[warp_id] = 0;\n        s_max_dist[warp_id] = CUDART_INF_F;\n    }\n    // Set intermediate top-k to +inf distances and invalid indices.\n    const int topk_base = warp_id * k;\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        s_topk_dist[topk_base + i] = CUDART_INF_F;\n        s_topk_idx[topk_base + i]  = -1;\n    }\n    __syncwarp(warp_mask);\n\n    // Load query point and broadcast within warp.\n    float qx = 0.0f, qy = 0.0f;\n    if (active) {\n        if (lane == 0) {\n            float2 q = query[global_warp];\n            qx = q.x; qy = q.y;\n        }\n        qx = __shfl_sync(warp_mask, qx, 0);\n        qy = __shfl_sync(warp_mask, qy, 0);\n    }\n\n    // Process data in tiles: load by the block, consume by each active warp.\n    for (int base = 0; base < data_count; base += tile_points) {\n        const int count = min(tile_points, data_count - base);\n\n        // Block-cooperative load into shared memory\n        for (int i = threadIdx.x; i < count; i += blockDim.x) {\n            s_points[i] = data[base + i];\n        }\n        __syncthreads();\n\n        if (active) {\n            // Each warp iterates over the tile, one element per lane per iteration.\n            for (int t = lane; t < count; t += WARP_SIZE) {\n                float2 p = s_points[t];\n                float dx = p.x - qx;\n                float dy = p.y - qy;\n                // Squared Euclidean distance\n                float dist = fmaf(dx, dx, dy * dy);\n\n                // Threshold: accept if we haven't filled k yet or dist < max_distance.\n                // Read shared metadata.\n                int   tk_sz  = s_topk_size[warp_id];\n                float thr    = (tk_sz < k) ? CUDART_INF_F : s_max_dist[warp_id];\n                bool  accept = (tk_sz < k) || (dist < thr);\n\n                // Try to append to candidate buffer using atomicAdd to get a position.\n                bool pending = false;\n                int  pend_idx = 0;\n                float pend_dist = 0.0f;\n\n                if (accept) {\n                    int pos = atomicAdd((int*)&s_cand_count[warp_id], 1);\n                    if (pos < k) {\n                        s_cand_idx[topk_base + pos]  = base + t;\n                        s_cand_dist[topk_base + pos] = dist;\n                    } else {\n                        // Buffer overflowed; keep this candidate pending to be re-attempted after a merge.\n                        pending = true;\n                        pend_idx = base + t;\n                        pend_dist = dist;\n                    }\n                }\n\n                // If any lane overflowed or the buffer reached capacity, merge and retry pending candidates.\n                while (__any_sync(warp_mask, pending) || (s_cand_count[warp_id] >= k)) {\n                    // Merge current candidates into top-k.\n                    warp_merge_candidates(\n                        warp_id, k,\n                        s_topk_dist, s_topk_idx,\n                        s_cand_dist, s_cand_idx,\n                        s_cand_count,\n                        s_work_dist, s_work_idx,\n                        s_max_dist, s_topk_size,\n                        warp_mask);\n\n                    // Try to (re-)insert pending candidate if it still qualifies against the updated threshold.\n                    if (pending) {\n                        int   tk_sz2 = s_topk_size[warp_id];\n                        float thr2   = (tk_sz2 < k) ? CUDART_INF_F : s_max_dist[warp_id];\n                        if ((tk_sz2 < k) || (pend_dist < thr2)) {\n                            int pos2 = atomicAdd((int*)&s_cand_count[warp_id], 1);\n                            if (pos2 < k) {\n                                s_cand_idx[topk_base + pos2]  = pend_idx;\n                                s_cand_dist[topk_base + pos2] = pend_dist;\n                                pending = false;\n                            } else {\n                                // Still full; will merge again and retry.\n                                pending = true;\n                            }\n                        } else {\n                            // No longer qualifies under the new (tighter) threshold.\n                            pending = false;\n                        }\n                    }\n                    __syncwarp(warp_mask);\n                }\n            }\n            __syncwarp(warp_mask);\n\n            // At tile end, if there are leftover candidates, merge them as well.\n            if (s_cand_count[warp_id] > 0) {\n                warp_merge_candidates(\n                    warp_id, k,\n                    s_topk_dist, s_topk_idx,\n                    s_cand_dist, s_cand_idx,\n                    s_cand_count,\n                    s_work_dist, s_work_idx,\n                    s_max_dist, s_topk_size,\n                    warp_mask);\n            }\n        }\n\n        __syncthreads(); // Ensure all warps are done with this tile before loading the next.\n    }\n\n    // After all tiles processed, ensure any residual candidates are merged (safety).\n    if (active && s_cand_count[warp_id] > 0) {\n        warp_merge_candidates(\n            warp_id, k,\n            s_topk_dist, s_topk_idx,\n            s_cand_dist, s_cand_idx,\n            s_cand_count,\n            s_work_dist, s_work_idx,\n            s_max_dist, s_topk_size,\n            warp_mask);\n    }\n\n    // Write results for this query (ascending by distance).\n    if (active) {\n        PairIF* out_q = out + static_cast<size_t>(global_warp) * k;\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            PairIF p;\n            p.first  = s_topk_idx[topk_base + i];\n            p.second = s_topk_dist[topk_base + i];\n            out_q[i] = p;\n        }\n    }\n}\n\n// Helper to compute dynamic shared memory size required for given k and tile_points.\nstatic inline size_t knn_shmem_bytes(int k, int tile_points) {\n    size_t off = 0;\n    // tile of float2\n    off = align_up(off, 8);\n    off += static_cast<size_t>(tile_points) * sizeof(float2);\n\n    // per-warp metadata\n    off = align_up(off, 4);\n    off += WARPS_PER_BLOCK * sizeof(int);   // s_cand_count\n\n    off = align_up(off, 4);\n    off += WARPS_PER_BLOCK * sizeof(int);   // s_topk_size\n\n    off = align_up(off, 4);\n    off += WARPS_PER_BLOCK * sizeof(float); // s_max_dist\n\n    // per-warp arrays\n    off = align_up(off, 4);\n    off += static_cast<size_t>(WARPS_PER_BLOCK) * k * sizeof(int);   // s_topk_idx\n\n    off = align_up(off, 4);\n    off += static_cast<size_t>(WARPS_PER_BLOCK) * k * sizeof(float); // s_topk_dist\n\n    off = align_up(off, 4);\n    off += static_cast<size_t>(WARPS_PER_BLOCK) * k * sizeof(int);   // s_cand_idx\n\n    off = align_up(off, 4);\n    off += static_cast<size_t>(WARPS_PER_BLOCK) * k * sizeof(float); // s_cand_dist\n\n    off = align_up(off, 4);\n    off += static_cast<size_t>(WARPS_PER_BLOCK) * 2 * k * sizeof(int);   // s_work_idx\n\n    off = align_up(off, 4);\n    off += static_cast<size_t>(WARPS_PER_BLOCK) * 2 * k * sizeof(float); // s_work_dist\n\n    return off;\n}\n\n// Host wrapper that configures and launches the kernel.\n// The result array is std::pair<int, float> but we reinterpret it to PairIF for device writes.\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Choose threads/block and number of blocks.\n    const int threads_per_block = WARPS_PER_BLOCK * WARP_SIZE; // 128\n    const int num_warps_total = (query_count + 0); // one warp per query\n    const int blocks = (query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK;\n\n    // Determine maximum dynamic shared memory supported and choose tile size accordingly.\n    cudaDeviceProp prop;\n    int device = 0;\n    cudaGetDevice(&device);\n    cudaGetDeviceProperties(&prop, device);\n\n    // On A100/H100, opt-in shared memory per block can reach 163 KB or more. Query and set it.\n    int max_optin = 0;\n    cudaDeviceGetAttribute(&max_optin, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    if (max_optin <= 0) {\n        max_optin = prop.sharedMemPerBlock; // Fallback to default\n    }\n\n    // Choose the largest tile_points that fits within the shared memory limit.\n    // Start with a preferred tile size and reduce if necessary.\n    int tile_points = 2048; // initial guess\n    size_t shmem_needed = knn_shmem_bytes(k, tile_points);\n    while (shmem_needed > static_cast<size_t>(max_optin) && tile_points > 0) {\n        tile_points >>= 1; // reduce by half\n        if (tile_points < WARP_SIZE) tile_points = WARP_SIZE;\n        shmem_needed = knn_shmem_bytes(k, tile_points);\n        if (tile_points == WARP_SIZE) break;\n    }\n\n    // Opt-in to larger dynamic shared memory if needed.\n    cudaFuncSetAttribute(knn2d_warp_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, static_cast<int>(shmem_needed));\n\n    // Launch kernel.\n    PairIF* out_pairs = reinterpret_cast<PairIF*>(result);\n    knn2d_warp_kernel<<<blocks, threads_per_block, shmem_needed>>>(\n        query, query_count, data, data_count, out_pairs, k, tile_points\n    );\n\n    // Synchronize to ensure completion (optional depending on caller expectations).\n    cudaDeviceSynchronize();\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 879, "completion_tokens": 22376, "total_tokens": 23255, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 17856, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db8abd481908744bed608989459", "custom_id": "knn06-06", "response": {"status_code": 200, "request_id": "de5a8063843f819fbafed60c5128ee28", "body": {"id": "chatcmpl-C4syiZpDaG2ocQIlT8VbSdEkoQoRH", "object": "chat.completion", "created": 1755280684, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// Optimized CUDA kernel for k-NN (2D, squared Euclidean) with one warp (32 threads) per query.\n// - Data is processed in tiles loaded into shared memory by the entire block.\n// - Each warp maintains a private intermediate top-k (distributed across lanes, in registers).\n// - Each warp has a shared-memory candidate buffer of size k (indices and distances) with an atomic counter.\n// - Distances below the current max_distance (k-th best) are added to the candidate buffer.\n// - When the buffer fills up (detected via atomicAdd overflow), the warp merges candidates into the intermediate result.\n// - After the final tile, any remaining candidates in the buffer are merged.\n// - Final results for each query are written in sorted ascending order of distance.\n// Assumptions:\n// - k is a power of two between 32 and 1024 inclusive.\n// - data_count >= k.\n// - All pointers are device pointers obtained via cudaMalloc.\n// - query_count and data_count are large, typical for GPU workloads.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Tunable kernel parameters chosen for modern NVIDIA datacenter GPUs (A100/H100):\n// - 8 warps per block (256 threads).\n// - Shared tile of 4096 float2 points (32KB).\n// - Per-warp candidate buffer sized to MAX_K (1024).\nstatic constexpr int WARPS_PER_BLOCK = 8;\nstatic constexpr int THREADS_PER_BLOCK = WARPS_PER_BLOCK * WARP_SIZE;\nstatic constexpr int TILE_POINTS = 4096;\nstatic constexpr int MAX_K = 1024;\nstatic constexpr int MAX_PER_LANE = MAX_K / WARP_SIZE;\n\nstruct PairDevice {\n    int   first;\n    float second;\n};\n\n__device__ __forceinline__ int lane_id() {\n    return threadIdx.x & (WARP_SIZE - 1);\n}\n\n__device__ __forceinline__ int warp_id_in_block() {\n    return threadIdx.x / WARP_SIZE;\n}\n\n// Shared memory:\n// - dataTile: cached data points for current tile\n// - cand_idx/cand_dist: per-warp candidate buffers of size MAX_K (we will only use first k slots).\n// - cand_count: per-warp candidate counters.\n__shared__ float2 s_dataTile[TILE_POINTS];\n__shared__ int    s_cand_idx[WARPS_PER_BLOCK][MAX_K];\n__shared__ float  s_cand_dist[WARPS_PER_BLOCK][MAX_K];\n__shared__ int    s_cand_count[WARPS_PER_BLOCK];\n\n// Find the current worst (maximum distance) entry across the warp's private top-k.\n// Inputs:\n// - best_d: per-lane array of distances (length chunk_len).\n// - chunk_len: number of valid entries per lane (k / 32).\n// Outputs (via references):\n// - worstVal: the maximum distance among all top-k entries.\n// - worstLane: the lane ID (0..31) that holds the worst entry.\n// - worstPos: the index within worstLane's local array (0..chunk_len-1).\n__device__ __forceinline__ void warpFindWorst(const float best_d[MAX_PER_LANE],\n                                              int chunk_len,\n                                              float &worstVal,\n                                              int &worstLane,\n                                              int &worstPos) {\n    const unsigned mask = __activemask();\n    const int lane = lane_id();\n\n    // Find local maximum and its local position\n    float localMax = -CUDART_INF_F;\n    int localPos = 0;\n    #pragma unroll\n    for (int i = 0; i < MAX_PER_LANE; ++i) {\n        if (i < chunk_len) {\n            float v = best_d[i];\n            if (v > localMax) {\n                localMax = v;\n                localPos = i;\n            }\n        }\n    }\n\n    // Warp-wide reduction to find global maximum and its lane+pos\n    float v = localMax;\n    int l = lane;\n    int p = localPos;\n\n    // Reduce in log2(32)=5 steps using shfl_down\n    // Note: If tie, keep the earlier lane by leaving condition as (ov > v).\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        float ov = __shfl_down_sync(mask, v, offset);\n        int   ol = __shfl_down_sync(mask, l, offset);\n        int   op = __shfl_down_sync(mask, p, offset);\n        if (ov > v) {\n            v = ov; l = ol; p = op;\n        }\n    }\n\n    // Broadcast results from lane 0\n    worstVal  = __shfl_sync(mask, v, 0);\n    worstLane = __shfl_sync(mask, l, 0);\n    worstPos  = __shfl_sync(mask, p, 0);\n}\n\n// Recompute and broadcast the current worst (k-th) distance across the warp's private top-k.\n// Returns the new worst distance value in maxDist (in all lanes).\n__device__ __forceinline__ void warpRecomputeMaxDist(const float best_d[MAX_PER_LANE],\n                                                     int chunk_len,\n                                                     float &maxDist) {\n    const unsigned mask = __activemask();\n\n    float localMax = -CUDART_INF_F;\n    #pragma unroll\n    for (int i = 0; i < MAX_PER_LANE; ++i) {\n        if (i < chunk_len) {\n            float v = best_d[i];\n            if (v > localMax) localMax = v;\n        }\n    }\n    // Warp max reduce\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        float ov = __shfl_down_sync(mask, localMax, offset);\n        if (ov > localMax) localMax = ov;\n    }\n    maxDist = __shfl_sync(mask, localMax, 0);\n}\n\n// Attempt to insert a candidate (idx, dist) into the warp's private top-k if it improves the result.\n// This function uses the warp to cooperatively locate the current worst and update it.\n// Updates maxDist whenever the k-th neighbor changes.\n__device__ __forceinline__ void tryInsertCandidate(int idx, float dist,\n                                                   float best_d[MAX_PER_LANE],\n                                                   int   best_i[MAX_PER_LANE],\n                                                   int chunk_len,\n                                                   float &maxDist) {\n    float worstVal;\n    int worstLane, worstPos;\n    warpFindWorst(best_d, chunk_len, worstVal, worstLane, worstPos);\n\n    const int lane = lane_id();\n    if (dist < worstVal) {\n        // Replace the current worst element with the new candidate\n        if (lane == worstLane) {\n            best_d[worstPos] = dist;\n            best_i[worstPos] = idx;\n        }\n        // Ensure the replacement is visible within the warp before recomputing maxDist\n        __syncwarp();\n\n        // Recompute the k-th distance after the update\n        warpRecomputeMaxDist(best_d, chunk_len, maxDist);\n    }\n    // If not better than worstVal, nothing to do\n}\n\n// Merge the candidate buffer (per warp) into the private top-k using the warp.\n// Only the first min(count, k) entries are valid in the buffer.\n// After merge, the candidate count is reset to 0.\n// maxDist is updated to reflect the k-th neighbor after merge.\n__device__ __forceinline__ void flush_and_merge_candidates(int warpSlot,\n                                                           int k,\n                                                           int chunk_len,\n                                                           float best_d[MAX_PER_LANE],\n                                                           int   best_i[MAX_PER_LANE],\n                                                           float &maxDist) {\n    const unsigned mask = __activemask();\n    // Synchronize warp before reading shared memory buffer\n    __syncwarp(mask);\n\n    int count = s_cand_count[warpSlot];\n    int valid = (count < k) ? count : k;\n\n    // Process candidates one by one; all lanes cooperate on each insertion.\n    for (int j = 0; j < valid; ++j) {\n        int c_idx = 0;\n        float c_dist = 0.0f;\n        if (lane_id() == 0) {\n            c_idx = s_cand_idx[warpSlot][j];\n            c_dist = s_cand_dist[warpSlot][j];\n        }\n        c_idx  = __shfl_sync(mask, c_idx, 0);\n        c_dist = __shfl_sync(mask, c_dist, 0);\n\n        // Only attempt insert if strictly better than current maxDist\n        if (c_dist < maxDist) {\n            tryInsertCandidate(c_idx, c_dist, best_d, best_i, chunk_len, maxDist);\n        }\n        __syncwarp(mask);\n    }\n\n    // Reset candidate count to zero\n    if (lane_id() == 0) {\n        s_cand_count[warpSlot] = 0;\n    }\n    __syncwarp(mask);\n}\n\n// Per-lane insertion sort (ascending) on the local chunk to prepare for k-way merge.\n// length = chunk_len <= MAX_PER_LANE.\n__device__ __forceinline__ void lane_insertion_sort(float d[MAX_PER_LANE], int idx[MAX_PER_LANE], int length) {\n    #pragma unroll\n    for (int i = 1; i < MAX_PER_LANE; ++i) {\n        if (i >= length) break;\n        float keyd = d[i];\n        int keyi = idx[i];\n        int j = i - 1;\n        for (; j >= 0; --j) {\n            float v = d[j];\n            if (v <= keyd) break;\n            d[j + 1] = v;\n            idx[j + 1] = idx[j];\n        }\n        d[j + 1] = keyd;\n        idx[j + 1] = keyi;\n    }\n}\n\n// Warp-wise argmin: find lane with minimal 'val'. Returns (minVal, minLane).\n__device__ __forceinline__ void warp_argmin(float val, int &minLane, float &minVal) {\n    const unsigned mask = __activemask();\n    int lane = lane_id();\n    float v = val;\n    int l = lane;\n    // Reduce to find minimum value with its lane\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        float ov = __shfl_down_sync(mask, v, offset);\n        int   ol = __shfl_down_sync(mask, l, offset);\n        if (ov < v) { v = ov; l = ol; }\n    }\n    minVal = __shfl_sync(mask, v, 0);\n    minLane = __shfl_sync(mask, l, 0);\n}\n\n__global__ void knn_kernel(const float2 *__restrict__ query,\n                           int query_count,\n                           const float2 *__restrict__ data,\n                           int data_count,\n                           PairDevice *__restrict__ result,\n                           int k) {\n    const int warpInBlock = warp_id_in_block();\n    const int lane = lane_id();\n    const int warpGlobal = blockIdx.x * WARPS_PER_BLOCK + warpInBlock;\n    if (warpGlobal >= query_count) return;\n\n    // Per-warp candidate counter reset\n    if (lane == 0) s_cand_count[warpInBlock] = 0;\n    __syncwarp();\n\n    // Load the warp's query point and broadcast to all lanes\n    float2 q;\n    if (lane == 0) {\n        q = query[warpGlobal];\n    }\n    q.x = __shfl_sync(0xffffffffu, q.x, 0);\n    q.y = __shfl_sync(0xffffffffu, q.y, 0);\n\n    // Per-warp parameters\n    const int chunk_len = k / WARP_SIZE; // since k is power of two and >= 32\n    float best_d[MAX_PER_LANE];\n    int   best_i[MAX_PER_LANE];\n\n    // Initialize intermediate top-k to +inf distances and invalid indices\n    #pragma unroll\n    for (int i = 0; i < MAX_PER_LANE; ++i) {\n        if (i < chunk_len) {\n            best_d[i] = CUDART_INF_F;\n            best_i[i] = -1;\n        }\n    }\n    float maxDist = CUDART_INF_F; // k-th neighbor distance (threshold)\n\n    // Process data in tiles cached in shared memory\n    for (int base = 0; base < data_count; base += TILE_POINTS) {\n        int tileCount = data_count - base;\n        if (tileCount > TILE_POINTS) tileCount = TILE_POINTS;\n\n        // Load tile into shared memory cooperatively by entire block\n        for (int t = threadIdx.x; t < tileCount; t += blockDim.x) {\n            s_dataTile[t] = data[base + t];\n        }\n        __syncthreads();\n\n        // Each warp processes the tile for its query\n        for (int i = lane; i < tileCount; i += WARP_SIZE) {\n            float2 p = s_dataTile[i];\n            float dx = q.x - p.x;\n            float dy = q.y - p.y;\n            float dist = fmaf(dy, dy, dx * dx); // squared L2\n\n            // Quick filter by current threshold\n            if (dist < maxDist) {\n                // Try to push candidate; if buffer overflows, flush/merge then retry.\n                bool done = false;\n                while (!done) {\n                    int pos = atomicAdd(&s_cand_count[warpInBlock], 1);\n                    bool overflow = (pos >= k);\n                    if (!overflow) {\n                        s_cand_dist[warpInBlock][pos] = dist;\n                        s_cand_idx[warpInBlock][pos]  = base + i;\n                        done = true;\n                    }\n                    // If any lane overflowed, flush and merge now (all lanes participate)\n                    if (__any_sync(0xffffffffu, overflow)) {\n                        flush_and_merge_candidates(warpInBlock, k, chunk_len, best_d, best_i, maxDist);\n                        // After flush, if this lane had overflow, it will retry insertion\n                    }\n                }\n            }\n        }\n\n        // After finishing the tile, flush remaining candidates (if any) to quickly tighten maxDist\n        int needFlush = 0;\n        if (lane == 0) needFlush = (s_cand_count[warpInBlock] > 0);\n        needFlush = __shfl_sync(0xffffffffu, needFlush, 0);\n        if (needFlush) {\n            flush_and_merge_candidates(warpInBlock, k, chunk_len, best_d, best_i, maxDist);\n        }\n\n        __syncthreads();\n    }\n\n    // Ensure no leftover (safety, though we flushed at tile boundaries)\n    int leftover = 0;\n    if (lane == 0) leftover = (s_cand_count[warpInBlock] > 0);\n    leftover = __shfl_sync(0xffffffffu, leftover, 0);\n    if (leftover) {\n        flush_and_merge_candidates(warpInBlock, k, chunk_len, best_d, best_i, maxDist);\n    }\n\n    // Sort each lane's local portion of the top-k to prepare for k-way merge\n    lane_insertion_sort(best_d, best_i, chunk_len);\n    __syncwarp();\n\n    // Perform 32-way merge to output final sorted results\n    const int outBase = warpGlobal * k;\n    int head = 0; // per-lane head within its sorted local array\n    for (int outPos = 0; outPos < k; ++outPos) {\n        float myVal = (head < chunk_len) ? best_d[head] : CUDART_INF_F;\n        int winnerLane;\n        float winnerVal;\n        warp_argmin(myVal, winnerLane, winnerVal);\n\n        // Broadcast winner's index (data point id) and write to output in order\n        int winnerIdx = __shfl_sync(0xffffffffu, (lane == winnerLane && head < chunk_len) ? best_i[head] : 0, winnerLane);\n\n        // Increment head of winner lane\n        if (lane == winnerLane) {\n            head++;\n        }\n        // Write output (single lane to avoid write conflicts; lane 0 does the store)\n        if (lane == 0) {\n            result[outBase + outPos].first  = winnerIdx;\n            result[outBase + outPos].second = winnerVal;\n        }\n        __syncwarp();\n    }\n}\n\n// Host wrapper to launch the kernel.\n// query, data, result must be device pointers allocated via cudaMalloc.\n// k must be a power of two between 32 and 1024 inclusive.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data, int data_count,\n             std::pair<int, float> *result, int k) {\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    dim3 blockDim(THREADS_PER_BLOCK, 1, 1);\n    dim3 gridDim((query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK, 1, 1);\n\n    // Reinterpret std::pair<int,float> as a plain device pair type without relying on std on device.\n    PairDevice *result_dev = reinterpret_cast<PairDevice*>(result);\n\n    // Launch kernel\n    knn_kernel<<<gridDim, blockDim>>>(query, query_count, data, data_count, result_dev, k);\n\n    // No device memory allocation inside kernel. Synchronization/stream handling is left to the caller if needed.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 879, "completion_tokens": 25657, "total_tokens": 26536, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 21888, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db89c508190be5c08b28af6d971", "custom_id": "knn06-07", "response": {"status_code": 200, "request_id": "f7b81b4cabd7d85abf335f6b39f4b883", "body": {"id": "chatcmpl-C4solkRCrKrz49g1rBlkGaWSkdXbs", "object": "chat.completion", "created": 1755280067, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <stdint.h>\n\n// This implementation computes k-nearest neighbors (k-NN) for 2D points using a single warp (32 threads) per query.\n// Key design points:\n// - Each warp processes one query and maintains its own intermediate top-k result in shared memory.\n// - Data points are processed in batches that are cooperatively loaded into shared memory by the whole thread block.\n// - Each warp filters candidates using a warp-private max_distance (distance of the current k-th nearest neighbor).\n// - Candidates passing the filter are appended into a per-warp candidate buffer using atomicAdd on a shared counter.\n// - When the candidate buffer is full (or overflows), the warp merges it into the intermediate result using a warp-cooperative bitonic sort on the union.\n// - After finishing all batches, any remaining candidates are merged.\n// - The final result for each query is written as k pairs (index, distance) sorted by ascending distance.\n//\n// Assumptions and constraints fulfilled:\n// - Target GPUs are modern data center GPUs (A100/H100), compiled with latest CUDA toolkit.\n// - k is a power of two between 32 and 1024 inclusive.\n// - data_count >= k.\n// - query_count and data_count are large enough to benefit from GPU parallelism.\n// - No additional device memory is allocated by the kernel (only uses dynamically allocated shared memory).\n// - Thread block configuration: 4 warps per block (128 threads). This balances shared memory footprint and occupancy.\n// - Data tile size: 2048 points per batch (16KB shared memory for float2 tile), fitting within SM shared memory capacity along with per-warp structures.\n\n// Helper pair type matching the layout of std::pair<int, float>.\n// We use this on device side to avoid depending on libstdc++ in device code.\nstruct PairIF {\n    int   first;\n    float second;\n};\n\n// Constants for block configuration and tiling\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Number of warps per block (blockDim.x = WARPS_PER_BLOCK * WARP_SIZE)\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 4\n#endif\n\n#ifndef THREADS_PER_BLOCK\n#define THREADS_PER_BLOCK (WARPS_PER_BLOCK * WARP_SIZE)\n#endif\n\n// Number of data points per tile loaded into shared memory by a block.\n// 2048 points = 16KB shared memory for float2 tile.\n#ifndef TILE_POINTS\n#define TILE_POINTS 2048\n#endif\n\n// Utility: round up x to the next multiple of a (alignment-friendly)\n__host__ __device__ __forceinline__ size_t align_up(size_t x, size_t a) {\n    return (x + (a - 1)) & ~(a - 1);\n}\n\n// Warp-cooperative in-shared-memory bitonic sort for (distance, index) pairs.\n// - Sorts 'n' elements (n must be a power of two) in ascending order by distance.\n// - Operates on shared memory arrays d[] and idx[] (warp-private regions).\n// - Uses warp-level striding; threads synchronize with __syncwarp(mask).\n__device__ __forceinline__ void warp_bitonic_sort_pairs(float* d, int* idx, int n, unsigned mask) {\n    // Classic bitonic sort network: O(n log^2 n)\n    for (int k = 2; k <= n; k <<= 1) {\n        for (int j = k >> 1; j > 0; j >>= 1) {\n            __syncwarp(mask);\n            for (int i = int(threadIdx.x & (WARP_SIZE - 1)); i < n; i += WARP_SIZE) {\n                int ixj = i ^ j;\n                if (ixj > i) {\n                    bool up = ((i & k) == 0);\n                    float di = d[i];\n                    float dj = d[ixj];\n                    int   ii = idx[i];\n                    int   ij = idx[ixj];\n                    // If ordering is wrong for the current sequence direction, swap\n                    if ((di > dj) == up) {\n                        d[i]    = dj;\n                        d[ixj]  = di;\n                        idx[i]  = ij;\n                        idx[ixj]= ii;\n                    }\n                }\n            }\n        }\n    }\n    __syncwarp(mask);\n}\n\n// Warp-cooperative merge of candidate buffer into the intermediate top-k result.\n// - resd/resi: current result arrays (size k) for the warp\n// - p_res_count: pointer to current valid result count (0..k)\n// - candd/candi: candidate arrays (size k) for the warp\n// - p_cand_count: pointer to current candidate count (0..k or more; we only use up to k)\n// - ud/ui: union arrays (size 2k) for temporary merging and sorting\n// - p_maxd: pointer to the warp's max_distance (distance of the current k-th)\n// - k: desired number of neighbors\n// - mask: warp active mask\n__device__ __forceinline__ void warp_merge_candidates_into_result(\n    float* resd, int* resi, int* p_res_count,\n    float* candd, int* candi, int* p_cand_count,\n    float* ud,    int* ui,\n    float* p_maxd,\n    int k,\n    unsigned mask)\n{\n    const int lane = int(threadIdx.x & (WARP_SIZE - 1));\n\n    // Read counts (only lane 0 touches shared variables; broadcast via shfl)\n    int rc = 0, cc = 0;\n    if (lane == 0) {\n        rc = *p_res_count;\n        cc = *p_cand_count;\n        if (cc > k) cc = k; // Only the first k candidates are validly stored in the buffer\n    }\n    rc = __shfl_sync(mask, rc, 0);\n    cc = __shfl_sync(mask, cc, 0);\n\n    // Union size and sort span size (next power-of-two)\n    int union_n = rc + cc;\n    if (union_n == 0) {\n        // Nothing to merge\n        if (lane == 0) {\n            // Reset candidate count; max_distance unchanged\n            *p_cand_count = 0;\n        }\n        __syncwarp(mask);\n        return;\n    }\n\n    int sort_n = 1;\n    while (sort_n < union_n) sort_n <<= 1; // sort_n <= 2k (k is power of two)\n\n    // Copy existing results into union arrays\n    for (int i = lane; i < rc; i += WARP_SIZE) {\n        ud[i] = resd[i];\n        ui[i] = resi[i];\n    }\n    // Copy candidates into union arrays (placed after existing results)\n    for (int i = lane; i < cc; i += WARP_SIZE) {\n        ud[rc + i] = candd[i];\n        ui[rc + i] = candi[i];\n    }\n    // Fill the padding with sentinel values (INF distance, invalid index)\n    for (int i = lane + union_n; i < sort_n; i += WARP_SIZE) {\n        ud[i] = CUDART_INF_F;\n        ui[i] = -1;\n    }\n    __syncwarp(mask);\n\n    // Sort the union in ascending order by distance\n    warp_bitonic_sort_pairs(ud, ui, sort_n, mask);\n\n    // Keep the first min(union_n, k) elements as the new result\n    int new_rc = union_n < k ? union_n : k;\n    for (int i = lane; i < new_rc; i += WARP_SIZE) {\n        resd[i] = ud[i];\n        resi[i] = ui[i];\n    }\n    // If fewer than k elements, fill the rest with INF/-1\n    for (int i = lane + new_rc; i < k; i += WARP_SIZE) {\n        resd[i] = CUDART_INF_F;\n        resi[i] = -1;\n    }\n    __syncwarp(mask);\n\n    if (lane == 0) {\n        *p_res_count  = new_rc;\n        *p_cand_count = 0; // Reset candidate buffer after merging\n        // Update max_distance: distance of the k-th neighbor if we have k; else INF\n        *p_maxd = (new_rc == k) ? resd[k - 1] : CUDART_INF_F;\n    }\n    __syncwarp(mask);\n}\n\n// Kernel computing k-NN for 2D points.\n// Each warp processes one query point. The block cooperatively loads tiles of data points into shared memory.\n// Per-warp shared memory regions hold:\n// - Intermediate result arrays (k pairs)\n// - Candidate buffer arrays (k pairs) + candidate count\n// - Union arrays for merging (2k pairs)\n// - Result count and max_distance\n__global__ void knn2d_kernel(\n    const float2* __restrict__ query,\n    int query_count,\n    const float2* __restrict__ data,\n    int data_count,\n    PairIF* __restrict__ out,\n    int k)\n{\n    // Compute warp and lane identifiers\n    const int lane             = int(threadIdx.x & (WARP_SIZE - 1));\n    const int warp_id_in_block = int(threadIdx.x) / WARP_SIZE;\n    const int warps_per_block  = WARPS_PER_BLOCK;\n    const int global_warp_id   = blockIdx.x * warps_per_block + warp_id_in_block;\n    const bool warp_active     = (global_warp_id < query_count);\n    const unsigned warp_mask   = __activemask();\n\n    // Dynamic shared memory layout\n    extern __shared__ unsigned char smem_bytes[];\n    unsigned char* smem = smem_bytes;\n    size_t offset = 0;\n\n    // Shared tile of data points for the entire block\n    offset = align_up(offset, alignof(float2));\n    float2* smem_tile = reinterpret_cast<float2*>(smem + offset);\n    offset += TILE_POINTS * sizeof(float2);\n\n    // Per-warp control scalars\n    offset = align_up(offset, alignof(int));\n    int* smem_cand_counts = reinterpret_cast<int*>(smem + offset);\n    offset += warps_per_block * sizeof(int);\n\n    offset = align_up(offset, alignof(int));\n    int* smem_res_counts = reinterpret_cast<int*>(smem + offset);\n    offset += warps_per_block * sizeof(int);\n\n    offset = align_up(offset, alignof(float));\n    float* smem_max_dists = reinterpret_cast<float*>(smem + offset);\n    offset += warps_per_block * sizeof(float);\n\n    // Per-warp arrays: result (k), candidate (k), union (2k)\n    // Result distances\n    offset = align_up(offset, alignof(float));\n    float* smem_res_dists = reinterpret_cast<float*>(smem + offset);\n    offset += size_t(warps_per_block) * size_t(k) * sizeof(float);\n\n    // Result indices\n    offset = align_up(offset, alignof(int));\n    int* smem_res_indices = reinterpret_cast<int*>(smem + offset);\n    offset += size_t(warps_per_block) * size_t(k) * sizeof(int);\n\n    // Candidate distances\n    offset = align_up(offset, alignof(float));\n    float* smem_cand_dists = reinterpret_cast<float*>(smem + offset);\n    offset += size_t(warps_per_block) * size_t(k) * sizeof(float);\n\n    // Candidate indices\n    offset = align_up(offset, alignof(int));\n    int* smem_cand_indices = reinterpret_cast<int*>(smem + offset);\n    offset += size_t(warps_per_block) * size_t(k) * sizeof(int);\n\n    // Union distances (2k)\n    offset = align_up(offset, alignof(float));\n    float* smem_union_dists = reinterpret_cast<float*>(smem + offset);\n    offset += size_t(warps_per_block) * size_t(2 * k) * sizeof(float);\n\n    // Union indices (2k)\n    offset = align_up(offset, alignof(int));\n    int* smem_union_indices = reinterpret_cast<int*>(smem + offset);\n    offset += size_t(warps_per_block) * size_t(2 * k) * sizeof(int);\n\n    // Warp-private base pointers for per-warp arrays\n    float* res_dists  = smem_res_dists   + warp_id_in_block * k;\n    int*   res_idx    = smem_res_indices + warp_id_in_block * k;\n    float* cand_dists = smem_cand_dists  + warp_id_in_block * k;\n    int*   cand_idx   = smem_cand_indices+ warp_id_in_block * k;\n    float* un_dists   = smem_union_dists + warp_id_in_block * (2 * k);\n    int*   un_idx     = smem_union_indices+warp_id_in_block * (2 * k);\n    int*   cand_count = smem_cand_counts + warp_id_in_block;\n    int*   res_count  = smem_res_counts  + warp_id_in_block;\n    float* max_dist   = smem_max_dists   + warp_id_in_block;\n\n    // Initialize per-warp state\n    if (warp_active) {\n        if (lane == 0) {\n            *cand_count = 0;\n            *res_count  = 0;\n            *max_dist   = CUDART_INF_F;\n        }\n        // Initialize result arrays to INF / -1 (optional but helps determinism before first merge)\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            res_dists[i] = CUDART_INF_F;\n            res_idx[i]   = -1;\n        }\n    }\n    __syncwarp(warp_mask);\n\n    // Load the query point into registers and broadcast within the warp\n    float qx = 0.0f, qy = 0.0f;\n    if (warp_active) {\n        float2 q;\n        if (lane == 0) {\n            q = query[global_warp_id];\n        }\n        qx = __shfl_sync(warp_mask, q.x, 0);\n        qy = __shfl_sync(warp_mask, q.y, 0);\n    }\n\n    // Process data points in tiles\n    for (int base = 0; base < data_count; base += TILE_POINTS) {\n        // Block loads a tile of data points into shared memory\n        int tile_count = data_count - base;\n        if (tile_count > TILE_POINTS) tile_count = TILE_POINTS;\n\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            smem_tile[i] = data[base + i];\n        }\n        __syncthreads(); // Ensure tile is fully loaded before any warp reads it\n\n        // Warp computes distances to its query and filters by current max_distance\n        if (warp_active) {\n            // Pending candidate for overflow case\n            int   pending_idx = -1;\n            float pending_dist= 0.0f;\n            int   pending_flag= 0;\n\n            for (int i = lane; i < tile_count; i += WARP_SIZE) {\n                float2 p = smem_tile[i];\n                float dx = p.x - qx;\n                float dy = p.y - qy;\n                float dist = fmaf(dx, dx, dy * dy); // squared L2 distance\n\n                float md = *max_dist;\n                // Only consider points closer than current max_distance\n                if (dist < md) {\n                    // Append to candidate buffer; use atomicAdd to get the position\n                    int pos = atomicAdd(cand_count, 1);\n                    if (pos < k) {\n                        cand_dists[pos] = dist;\n                        cand_idx[pos]   = base + i;\n                    } else {\n                        // Buffer overflow: hold this candidate temporarily and trigger a merge\n                        pending_idx  = base + i;\n                        pending_dist = dist;\n                        pending_flag = 1;\n                    }\n                }\n\n                // If any lane overflows the buffer, merge candidates into results now\n                unsigned overflow_mask = __ballot_sync(warp_mask, pending_flag != 0);\n                // Also merge if the buffer just became full (equals k), even without overflow\n                int cc_snapshot = 0;\n                if (lane == 0) cc_snapshot = *cand_count;\n                cc_snapshot = __shfl_sync(warp_mask, cc_snapshot, 0);\n                int just_full = (cc_snapshot >= k);\n\n                if (overflow_mask || just_full) {\n                    __syncwarp(warp_mask);\n                    // Merge current candidate buffer into the intermediate result\n                    warp_merge_candidates_into_result(\n                        res_dists, res_idx, res_count,\n                        cand_dists, cand_idx, cand_count,\n                        un_dists, un_idx,\n                        max_dist,\n                        k,\n                        warp_mask);\n\n                    // After merging, re-try to append the pending candidate if it still passes the updated max_distance.\n                    if (pending_flag) {\n                        float md2 = *max_dist;\n                        if (pending_dist < md2) {\n                            int pos2 = atomicAdd(cand_count, 1);\n                            // Guaranteed pos2 < k because buffer was just reset and k >= WARP_SIZE\n                            if (pos2 < k) {\n                                cand_dists[pos2] = pending_dist;\n                                cand_idx[pos2]   = pending_idx;\n                            }\n                        }\n                        pending_flag = 0;\n                    }\n                    __syncwarp(warp_mask);\n                }\n            }\n\n            // End of tile loop for this warp: if any candidates remain, merge them to update the threshold early\n            int cc_final = 0;\n            if (lane == 0) cc_final = *cand_count;\n            cc_final = __shfl_sync(warp_mask, cc_final, 0);\n            if (cc_final > 0) {\n                __syncwarp(warp_mask);\n                warp_merge_candidates_into_result(\n                    res_dists, res_idx, res_count,\n                    cand_dists, cand_idx, cand_count,\n                    un_dists, un_idx,\n                    max_dist,\n                    k,\n                    warp_mask);\n                __syncwarp(warp_mask);\n            }\n        }\n\n        __syncthreads(); // Ensure all warps are done reading this tile before loading the next one\n    }\n\n    // After processing all tiles, if any candidates remain (shouldn't, but safe), merge them\n    if (warp_active) {\n        int cc_rem = 0;\n        if (lane == 0) cc_rem = *cand_count;\n        cc_rem = __shfl_sync(warp_mask, cc_rem, 0);\n        if (cc_rem > 0) {\n            __syncwarp(warp_mask);\n            warp_merge_candidates_into_result(\n                res_dists, res_idx, res_count,\n                cand_dists, cand_idx, cand_count,\n                un_dists, un_idx,\n                max_dist,\n                k,\n                warp_mask);\n            __syncwarp(warp_mask);\n        }\n\n        // Write final k nearest neighbors for this query to global memory (ascending by distance)\n        int out_base = global_warp_id * k;\n        for (int j = lane; j < k; j += WARP_SIZE) {\n            PairIF pr;\n            pr.first  = res_idx[j];\n            pr.second = res_dists[j];\n            out[out_base + j] = pr;\n        }\n    }\n}\n\n// Host API: launch configuration and shared memory sizing\nextern \"C\" void run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    // Configure grid: one warp per query\n    const int warps_per_block   = WARPS_PER_BLOCK;           // 4\n    const int threads_per_block = THREADS_PER_BLOCK;         // 128\n    const int blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Compute dynamic shared memory size per block\n    // Layout per block:\n    // - TILE_POINTS * sizeof(float2)\n    // - warps_per_block * (sizeof(int) cand_count + sizeof(int) res_count + sizeof(float) max_dist)\n    // - warps_per_block * [ (k * (sizeof(float) + sizeof(int)) for result)\n    //                       + (k * (sizeof(float) + sizeof(int)) for candidates)\n    //                       + (2k * (sizeof(float) + sizeof(int)) for union) ]\n    size_t smem_bytes = 0;\n    smem_bytes = align_up(smem_bytes, alignof(float2));\n    smem_bytes += size_t(TILE_POINTS) * sizeof(float2);\n\n    smem_bytes = align_up(smem_bytes, alignof(int));\n    smem_bytes += size_t(warps_per_block) * sizeof(int);   // cand_counts\n    smem_bytes = align_up(smem_bytes, alignof(int));\n    smem_bytes += size_t(warps_per_block) * sizeof(int);   // res_counts\n    smem_bytes = align_up(smem_bytes, alignof(float));\n    smem_bytes += size_t(warps_per_block) * sizeof(float); // max_dists\n\n    // Per-warp arrays\n    size_t per_warp_pairs_bytes = 0;\n    per_warp_pairs_bytes  = size_t(k) * (sizeof(float) + sizeof(int));   // result\n    per_warp_pairs_bytes += size_t(k) * (sizeof(float) + sizeof(int));   // candidates\n    per_warp_pairs_bytes += size_t(2 * k) * (sizeof(float) + sizeof(int)); // union\n\n    smem_bytes = align_up(smem_bytes, alignof(float));\n    smem_bytes += size_t(warps_per_block) * size_t(k) * sizeof(float); // res dists\n    smem_bytes = align_up(smem_bytes, alignof(int));\n    smem_bytes += size_t(warps_per_block) * size_t(k) * sizeof(int);   // res idx\n    smem_bytes = align_up(smem_bytes, alignof(float));\n    smem_bytes += size_t(warps_per_block) * size_t(k) * sizeof(float); // cand dists\n    smem_bytes = align_up(smem_bytes, alignof(int));\n    smem_bytes += size_t(warps_per_block) * size_t(k) * sizeof(int);   // cand idx\n    smem_bytes = align_up(smem_bytes, alignof(float));\n    smem_bytes += size_t(warps_per_block) * size_t(2 * k) * sizeof(float); // union dists\n    smem_bytes = align_up(smem_bytes, alignof(int));\n    smem_bytes += size_t(warps_per_block) * size_t(2 * k) * sizeof(int);   // union idx\n\n    // Launch kernel\n    knn2d_kernel<<<blocks, threads_per_block, smem_bytes>>>(\n        query,\n        query_count,\n        data,\n        data_count,\n        reinterpret_cast<PairIF*>(result),\n        k\n    );\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 879, "completion_tokens": 20165, "total_tokens": 21044, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 15232, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7dba17108190b6df525e7641b9b2", "custom_id": "knn06-08", "response": {"status_code": 200, "request_id": "c44b54e4fd1f4336ae29c2872aa4de9a", "body": {"id": "chatcmpl-C4t1cEJyyFKy3AQBQoghh9hhWLS8a", "object": "chat.completion", "created": 1755280864, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// Optimized k-NN for 2D points using warp-per-query, shared-memory tiling, and warp-cooperative merges.\n// - Each warp (32 threads) handles one query.\n// - Data points are processed in shared-memory tiles cached by the entire block.\n// - Each warp maintains:\n//   * An intermediate result of size k (indices and distances), stored in shared memory, sorted ascending.\n//   * A candidate buffer of size k (indices and distances), stored in shared memory, unsorted.\n//   * A shared counter for candidates (atomicAdd for increment).\n//   * A shared max_distance = current k-th smallest distance in the intermediate result.\n//   * A shared flush_flag used to coordinate warp-cooperative merges when candidate buffer is full.\n// - When the candidate buffer fills, the warp cooperatively merges it with its intermediate result using an\n//   in-place bitonic sort over 2*k items executed by the warp across the two arrays without allocating extra memory.\n// - After processing all tiles, any remaining candidates are merged as well.\n//\n// Assumptions and choices:\n// - k is a power of two in [32, 1024].\n// - No additional device allocations are performed; only dynamic shared memory is used.\n// - Threads per block = 4 warps = 128 threads.\n// - Tile size = 2048 data points per block (fits comfortably into shared memory on A100/H100).\n// - Max dynamic shared memory is requested via cudaFuncSetAttribute, but the kernel can run within default limits.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Tunables (chosen to balance shared memory usage and occupancy on A100/H100).\nstatic constexpr int WARPS_PER_BLOCK = 4;\nstatic constexpr int THREADS_PER_BLOCK = WARPS_PER_BLOCK * WARP_SIZE;\nstatic constexpr int TILE_POINTS = 2048; // Shared-memory tile size (float2) per block\n\n// Utility: lane and warp indices within a block\nstatic __device__ __forceinline__ int lane_id() { return threadIdx.x & (WARP_SIZE - 1); }\nstatic __device__ __forceinline__ int warp_id_in_block() { return threadIdx.x >> 5; }\n\n// Accessor that represents two contiguous \"virtual\" segments of total length 2*k:\n// positions [0, k-1] map to the \"intermediate\" arrays; positions [k, 2k-1] map to the \"candidate\" arrays.\n// We implement compare-exchanges across these two physical arrays without extra memory.\nstruct TwoSegAccessor {\n    float* inter_dist;\n    int*   inter_idx;\n    float* cand_dist;\n    int*   cand_idx;\n    int    k;\n\n    __device__ __forceinline__ void load(int pos, float& d, int& i) const {\n        if (pos < k) {\n            d = inter_dist[pos];\n            i = inter_idx[pos];\n        } else {\n            int m = pos - k;\n            d = cand_dist[m];\n            i = cand_idx[m];\n        }\n    }\n    __device__ __forceinline__ void store(int pos, float d, int i) const {\n        if (pos < k) {\n            inter_dist[pos] = d;\n            inter_idx[pos]  = i;\n        } else {\n            int m = pos - k;\n            cand_dist[m] = d;\n            cand_idx[m]  = i;\n        }\n    }\n};\n\n// Warp-cooperative in-place bitonic sort of 2*k items accessed via TwoSegAccessor.\n// After completion, elements at virtual positions [0, 2k-1] are globally sorted ascending.\n// Consequently, the first k items (positions [0, k-1]) reside in the intermediate arrays in ascending order.\nstatic __device__ __forceinline__ void warp_bitonic_sort_2k(const TwoSegAccessor& acc, int k) {\n    const int N = 2 * k;\n    const int lane = lane_id();\n    // Standard bitonic sort network using (size, stride) loops, executed warp-synchronously.\n    for (int size = 2; size <= N; size <<= 1) {\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            for (int i = lane; i < N; i += WARP_SIZE) {\n                int j = i ^ stride;\n                if (i < j) {\n                    bool ascending = ((i & size) == 0);\n                    float di, dj;\n                    int ii, ij;\n                    acc.load(i, di, ii);\n                    acc.load(j, dj, ij);\n                    // Compare-exchange\n                    if (ascending) {\n                        if (di > dj) {\n                            acc.store(i, dj, ij);\n                            acc.store(j, di, ii);\n                        } else {\n                            acc.store(i, di, ii);\n                            acc.store(j, dj, ij);\n                        }\n                    } else {\n                        if (di < dj) {\n                            acc.store(i, dj, ij);\n                            acc.store(j, di, ii);\n                        } else {\n                            acc.store(i, di, ii);\n                            acc.store(j, dj, ij);\n                        }\n                    }\n                }\n            }\n            __syncwarp();\n        }\n    }\n}\n\n// Merge candidate buffer into the intermediate result for a single warp.\n// - Pads candidate buffer with +inf distances up to size k.\n// - Runs a warp-cooperative bitonic sort across the 2*k items, mapping them over the two arrays in place.\n// - Writes new max_distance = inter_dist[k-1], resets candidate count to 0, clears flush flag.\nstatic __device__ __forceinline__ void warp_merge_candidates(\n    float* inter_dist, int* inter_idx,\n    float* cand_dist,  int* cand_idx,\n    int* cand_count,   float* max_distance,\n    volatile int* flush_flag,\n    int k)\n{\n    const int lane = lane_id();\n    // Snapshot current candidate count (<= k due to guarded insertion).\n    int count = *cand_count;\n    // Pad remaining candidate slots with +inf so they never enter top-k.\n    for (int t = lane + count; t < k; t += WARP_SIZE) {\n        cand_dist[t] = CUDART_INF_F;\n        cand_idx[t]  = -1;\n    }\n    __syncwarp();\n\n    TwoSegAccessor acc{ inter_dist, inter_idx, cand_dist, cand_idx, k };\n    warp_bitonic_sort_2k(acc, k);\n\n    __syncwarp();\n    if (lane == 0) {\n        *max_distance = inter_dist[k - 1];\n        *cand_count   = 0;\n        *flush_flag   = 0;\n    }\n    __syncwarp();\n}\n\n// Attempt to insert a candidate (idx, dist) into the warp's shared candidate buffer.\n// - Uses atomicAdd on cand_count to get a position; if full, reverts with atomicSub and triggers a cooperative merge.\n// - Joins any in-progress merges initiated by other threads (via flush_flag).\n// - Returns only once the candidate has been inserted successfully.\nstatic __device__ __forceinline__ void warp_push_candidate_or_merge(\n    int idx, float dist,\n    float* inter_dist, int* inter_idx,\n    float* cand_dist,  int* cand_idx,\n    int* cand_count,   float* max_distance,\n    volatile int* flush_flag,\n    int k)\n{\n    // Busy-wait loop: attempt insertion; if full, trigger and join merge, then retry.\n    while (true) {\n        // If a merge is already requested/ongoing by another lane, join it before inserting.\n        if (*flush_flag) {\n            warp_merge_candidates(inter_dist, inter_idx, cand_dist, cand_idx, cand_count, max_distance, flush_flag, k);\n        }\n        int pos = atomicAdd(cand_count, 1);\n        if (pos < k) {\n            // We got a slot.\n            cand_dist[pos] = dist;\n            cand_idx[pos]  = idx;\n            return;\n        } else {\n            // Buffer was full; revert increment and request a merge.\n            atomicSub(cand_count, 1);\n            atomicExch((int*)flush_flag, 1);\n            // Cooperatively merge and then retry insertion.\n            warp_merge_candidates(inter_dist, inter_idx, cand_dist, cand_idx, cand_count, max_distance, flush_flag, k);\n        }\n    }\n}\n\n// Kernel: Each warp computes k-NN for one query. Data points are processed in shared-memory tiles.\n// Dynamic shared memory layout per block:\n// - First: TILE_POINTS of float2 for the data tile.\n// - Then, for each warp (WARPS_PER_BLOCK of them), the per-warp region containing:\n//     * inter_dist[k], inter_idx[k], cand_dist[k], cand_idx[k], cand_count, max_distance, flush_flag\n__global__ void knn_kernel_2d_warpperquery(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data,  int data_count,\n    std::pair<int, float>* __restrict__ result,\n    int k)\n{\n    extern __shared__ unsigned char shared[];\n    const int lane = lane_id();\n    const int warp_in_block = warp_id_in_block();\n    const int warp_global = blockIdx.x * WARPS_PER_BLOCK + warp_in_block;\n    if (warp_global >= query_count) return;\n\n    // Shared memory partitioning\n    // Block-wide data tile\n    float2* smem_tile = reinterpret_cast<float2*>(shared);\n    size_t tile_bytes = sizeof(float2) * TILE_POINTS;\n    // Align next region to 8 bytes\n    size_t offset = (tile_bytes + 7) & ~size_t(7);\n\n    // Per-warp region sizes\n    const size_t dist_bytes = sizeof(float) * k;\n    const size_t idx_bytes  = sizeof(int)   * k;\n    const size_t warp_region_bytes_unaligned = dist_bytes + idx_bytes + dist_bytes + idx_bytes + sizeof(int) + sizeof(float) + sizeof(int);\n    const size_t warp_region_bytes = (warp_region_bytes_unaligned + 7) & ~size_t(7);\n\n    unsigned char* warp_region_base = shared + offset + warp_in_block * warp_region_bytes;\n\n    // Pointers into per-warp region\n    float* inter_dist = reinterpret_cast<float*>(warp_region_base);\n    int*   inter_idx  = reinterpret_cast<int*>(warp_region_base + dist_bytes);\n    float* cand_dist  = reinterpret_cast<float*>(warp_region_base + dist_bytes + idx_bytes);\n    int*   cand_idx   = reinterpret_cast<int*>(warp_region_base + dist_bytes + idx_bytes + dist_bytes);\n    int*   cand_count = reinterpret_cast<int*>(warp_region_base + dist_bytes + idx_bytes + dist_bytes + idx_bytes);\n    float* max_distance = reinterpret_cast<float*>(reinterpret_cast<unsigned char*>(cand_count) + sizeof(int));\n    volatile int* flush_flag = reinterpret_cast<volatile int*>(reinterpret_cast<unsigned char*>(max_distance) + sizeof(float));\n\n    // Initialize per-warp structures\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        inter_dist[i] = CUDART_INF_F;\n        inter_idx[i]  = -1;\n    }\n    if (lane == 0) {\n        *cand_count   = 0;\n        *max_distance = CUDART_INF_F;\n        *flush_flag   = 0;\n    }\n    __syncwarp();\n\n    // Load query point into registers\n    const float2 q = query[warp_global];\n\n    // Iterate over data in tiles\n    for (int tile_start = 0; tile_start < data_count; tile_start += TILE_POINTS) {\n        int tile_count = data_count - tile_start;\n        if (tile_count > TILE_POINTS) tile_count = TILE_POINTS;\n\n        // Cooperative load of tile by entire block\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            smem_tile[i] = data[tile_start + i];\n        }\n        __syncthreads();\n\n        // Each warp processes this tile\n        for (int i = lane; i < tile_count; i += WARP_SIZE) {\n            // Join any pending merge requested by other lanes in this warp.\n            if (*flush_flag) {\n                warp_merge_candidates(inter_dist, inter_idx, cand_dist, cand_idx, cand_count, max_distance, flush_flag, k);\n            }\n\n            float2 p = smem_tile[i];\n            float dx = p.x - q.x;\n            float dy = p.y - q.y;\n            float dist = dx * dx + dy * dy;\n\n            // Read current max_distance; filtering distances no larger than k-th best so far.\n            float maxd = *max_distance;\n            if (dist < maxd) {\n                // Push candidate; may trigger merge if buffer is full.\n                warp_push_candidate_or_merge(tile_start + i, dist,\n                                             inter_dist, inter_idx,\n                                             cand_dist,  cand_idx,\n                                             cand_count, max_distance,\n                                             flush_flag, k);\n            }\n        }\n        __syncwarp();   // Ensure warp finished using this tile before block advances tile\n        __syncthreads();\n    }\n\n    // After all tiles, if there are remaining candidates, merge them.\n    if (lane == 0 && *cand_count > 0) {\n        atomicExch((int*)flush_flag, 1);\n    }\n    if (*flush_flag) {\n        warp_merge_candidates(inter_dist, inter_idx, cand_dist, cand_idx, cand_count, max_distance, flush_flag, k);\n    }\n\n    // Write final results for this query to global memory; inter_dist/inter_idx contain top-k sorted ascending.\n    // Each lane writes a strided subset to maximize coalescing across queries.\n    const int out_base = warp_global * k;\n    for (int j = lane; j < k; j += WARP_SIZE) {\n        result[out_base + j].first  = inter_idx[j];\n        result[out_base + j].second = inter_dist[j];\n    }\n}\n\n// Host entry point. Chooses launch configuration and dynamic shared memory size, then launches the kernel.\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Grid/block configuration\n    dim3 block(THREADS_PER_BLOCK);\n    dim3 grid((query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK);\n\n    // Compute dynamic shared memory size required:\n    // - TILE_POINTS * sizeof(float2) for the block-wide tile\n    // - Per warp: inter_dist[k], inter_idx[k], cand_dist[k], cand_idx[k], cand_count, max_distance, flush_flag\n    size_t tile_bytes = sizeof(float2) * TILE_POINTS;\n    size_t dist_bytes = sizeof(float) * k;\n    size_t idx_bytes  = sizeof(int)   * k;\n    size_t warp_region_bytes_unaligned = dist_bytes + idx_bytes + dist_bytes + idx_bytes + sizeof(int) + sizeof(float) + sizeof(int);\n    size_t warp_region_bytes = (warp_region_bytes_unaligned + 7) & ~size_t(7);\n    size_t dynamic_smem_bytes = ((tile_bytes + 7) & ~size_t(7)) + WARPS_PER_BLOCK * warp_region_bytes;\n\n    // Opt-in to larger dynamic shared memory if available (A100/H100 allow up to ~164KB per block).\n    cudaFuncSetAttribute(knn_kernel_2d_warpperquery, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)dynamic_smem_bytes);\n    cudaFuncSetAttribute(knn_kernel_2d_warpperquery, cudaFuncAttributePreferredSharedMemoryCarveout, 100);\n\n    // Launch kernel\n    knn_kernel_2d_warpperquery<<<grid, block, dynamic_smem_bytes>>>(query, query_count, data, data_count, result, k);\n\n    // Optionally: synchronize or check for launch errors (omitted by design per instruction constraints).\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 879, "completion_tokens": 22246, "total_tokens": 23125, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 18816, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db931b881909858e9bc7889ccd2", "custom_id": "knn06-09", "response": {"status_code": 200, "request_id": "d1596feeffaf56a04d37674d0c3f4514", "body": {"id": "chatcmpl-C4sotGthEtoUc4nT9y8bAYBuXbBxJ", "object": "chat.completion", "created": 1755280075, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cuda.h>\n#include <utility>\n\n// This implementation computes k-NN for 2D points using a single warp (32 threads) per query.\n// It tiles the data points into shared memory, computes squared L2 distances, and maintains\n// per-warp shared-memory buffers:\n//   - An intermediate result of the current top-k neighbors (indices and distances), kept sorted.\n//   - A candidate buffer (indices and distances) with capacity k, and a shared counter updated via atomicAdd.\n// Whenever the candidate buffer becomes full, the warp sorts the candidates and merges them with\n// the intermediate result (taking the k smallest). After processing all tiles, any remaining\n// candidates are merged once more. The final top-k pairs are written to the output.\n//\n// Design notes:\n// - Each warp handles exactly one query.\n// - The candidate buffer and the intermediate top-k arrays are private to each warp and reside in shared memory.\n// - A per-warp shared integer tracks the current number of candidates in the buffer; atomicAdd is used\n//   to allocate slots in the buffer for new candidates.\n// - When the buffer is full, all 32 threads in the warp cooperatively sort and merge.\n// - Distances are squared Euclidean distances; no sqrt is performed.\n// - k is a power of two between 32 and 1024 (inclusive).\n// - No additional device memory is allocated beyond dynamic shared memory.\n//\n// The host function run_knn sets up the kernel launch, including choosing the number of warps per block,\n// computing the dynamic shared memory size, opting into large shared memory if supported, and choosing\n// a suitable tile size.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Define a plain-old-data pair type that matches std::pair<int, float> layout for device writes.\nstruct PairIF {\n    int   first;\n    float second;\n};\n\n// Round up 'x' to the nearest multiple of 'align' (align must be power-of-two).\nstatic inline size_t round_up(size_t x, size_t align) {\n    return (x + (align - 1)) & ~(align - 1);\n}\n\n// Return the next power-of-two >= x. Assumes x > 0 and x <= 1024 in this context.\n__device__ __forceinline__ int next_pow2(int x) {\n    x--;\n    x |= x >> 1;\n    x |= x >> 2;\n    x |= x >> 4;\n    x |= x >> 8;\n    x |= x >> 16;\n    x++;\n    return x;\n}\n\n// In-warp bitonic sort on shared arrays (dist[], idx[]) of given length 'len' (power-of-two).\n// The arrays are accessed cooperatively by the warp's 32 threads. Sorting is ascending by distance.\n// Each pair (i, j=i^stride) is processed exactly once by the condition (j > i) to avoid races.\n__device__ __forceinline__ void warp_bitonic_sort(float* dist, int* idx, int len, unsigned mask) {\n    for (int size = 2; size <= len; size <<= 1) {\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            for (int i = threadIdx.x % WARP_SIZE; i < len; i += WARP_SIZE) {\n                int j = i ^ stride;\n                if (j > i) {\n                    bool up = ((i & size) == 0);\n                    float di = dist[i];\n                    float dj = dist[j];\n                    int   ii = idx[i];\n                    int   ij = idx[j];\n                    bool do_swap = (up && di > dj) || (!up && di < dj);\n                    if (do_swap) {\n                        dist[i] = dj;\n                        dist[j] = di;\n                        idx[i]  = ij;\n                        idx[j]  = ii;\n                    }\n                }\n            }\n            __syncwarp(mask);\n        }\n    }\n}\n\n// Merge-path search to find the partition index 'a_idx' for a given diagonal 'diag' in the merge of A and B.\n// A and B are sorted ascending arrays of lengths a_len and b_len, respectively.\n// Returns a_idx such that a_idx + b_idx = diag with the standard merge invariants.\n__device__ __forceinline__ int merge_path_search(const float* A, int a_len, const float* B, int b_len, int diag) {\n    int a_min = max(0, diag - b_len);\n    int a_max = min(diag, a_len);\n    while (a_min < a_max) {\n        int a_idx = (a_min + a_max) >> 1;\n        int b_idx = diag - a_idx;\n        float A_val = (a_idx < a_len) ? A[a_idx] : CUDART_INF_F;\n        float B_prev = (b_idx > 0) ? B[b_idx - 1] : -CUDART_INF_F;\n        if (A_val < B_prev) {\n            a_min = a_idx + 1;\n        } else {\n            a_max = a_idx;\n        }\n    }\n    return a_min;\n}\n\n// Merge the first 'k_out' smallest elements from two sorted arrays:\n//   A (a_len), B (b_len) into out (out_dist/out_idx).\n// Work is partitioned across the warp using merge-path. The arrays must be ascending.\n// This function does not assume a_len == b_len; it supports merging any lengths.\n__device__ __forceinline__ void warp_merge_k_smallest(\n    const float* A_dist, const int* A_idx, int a_len,\n    const float* B_dist, const int* B_idx, int b_len,\n    float* out_dist, int* out_idx, int k_out, unsigned mask)\n{\n    // Partition output range [0, k_out) into per-lane segments.\n    int lane = threadIdx.x % WARP_SIZE;\n    int seg  = (k_out + WARP_SIZE - 1) / WARP_SIZE; // ceil\n    int start = lane * seg;\n    if (start >= k_out) {\n        __syncwarp(mask);\n        return;\n    }\n    int end = min(start + seg, k_out);\n\n    // Find starting positions in A and B for 'start' and 'end'\n    int a_start = merge_path_search(A_dist, a_len, B_dist, b_len, start);\n    int b_start = start - a_start;\n\n    int a_end = merge_path_search(A_dist, a_len, B_dist, b_len, end);\n    int b_end = end - a_end;\n\n    int ai = a_start;\n    int bi = b_start;\n    for (int out = start; out < end; ++out) {\n        bool takeA = (bi >= b_end) || (ai < a_end && A_dist[ai] <= B_dist[bi]);\n        if (takeA) {\n            out_dist[out] = A_dist[ai];\n            out_idx[out]  = A_idx[ai];\n            ++ai;\n        } else {\n            out_dist[out] = B_dist[bi];\n            out_idx[out]  = B_idx[bi];\n            ++bi;\n        }\n    }\n    __syncwarp(mask);\n}\n\n// Flush and merge the candidate buffer with the intermediate top-k for a single warp.\n// - top_dist/top_idx: the warp's current top-k (sorted ascending).\n// - cand_dist/cand_idx: the warp's candidate buffer (first n_cand entries are valid).\n// - n_cand: number of valid candidates (0 < n_cand <= k).\n// Returns the updated max_distance (k-th smallest), broadcast to all lanes.\n__device__ __forceinline__ float warp_flush_and_merge(\n    float* top_dist, int* top_idx, int k,\n    float* cand_dist, int* cand_idx, int n_cand, unsigned mask)\n{\n    if (n_cand <= 0) {\n        float md = 0.0f;\n        if ((threadIdx.x % WARP_SIZE) == 0) md = top_dist[k - 1];\n        md = __shfl_sync(mask, md, 0);\n        return md;\n    }\n\n    // Pad candidates to next power of two with +INF so bitonic sort can be applied.\n    int len_pow2 = next_pow2(n_cand);\n    for (int i = threadIdx.x % WARP_SIZE; i < (len_pow2 - n_cand); i += WARP_SIZE) {\n        int pos = n_cand + i;\n        if (pos < len_pow2) {\n            cand_dist[pos] = CUDART_INF_F;\n            cand_idx[pos]  = -1;\n        }\n    }\n    __syncwarp(mask);\n\n    // Sort the first len_pow2 entries (real + padded) ascending by distance.\n    warp_bitonic_sort(cand_dist, cand_idx, len_pow2, mask);\n\n    // Merge top-k (A) with n_cand (B) to keep k smallest; write into cand_* as temporary output.\n    warp_merge_k_smallest(top_dist, top_idx, k,\n                          cand_dist, cand_idx, n_cand,\n                          cand_dist, cand_idx, k, mask);\n\n    // Copy merged result back into top_*.\n    for (int i = threadIdx.x % WARP_SIZE; i < k; i += WARP_SIZE) {\n        top_dist[i] = cand_dist[i];\n        top_idx[i]  = cand_idx[i];\n    }\n    __syncwarp(mask);\n\n    // Update and broadcast new max_distance (k-th smallest).\n    float new_max = 0.0f;\n    if ((threadIdx.x % WARP_SIZE) == 0) new_max = top_dist[k - 1];\n    new_max = __shfl_sync(mask, new_max, 0);\n    return new_max;\n}\n\n// Kernel templated on the number of warps per block.\ntemplate<int WARPS_PER_BLOCK>\n__global__ void knn_kernel_2d(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data,  int data_count,\n    PairIF* __restrict__ result,\n    int k, int tile_elems)\n{\n    // Declare dynamic shared memory and compute layout:\n    // [0 .. tile_elems*sizeof(float2))                     -> data tile\n    // [aligned .. aligned + WARPS_PER_BLOCK*sizeof(int))   -> candidate counters (per warp)\n    // [.. + WPB*k*sizeof(float)]                           -> top_dist for all warps\n    // [.. + WPB*k*sizeof(int)]                             -> top_idx for all warps\n    // [.. + WPB*k*sizeof(float)]                           -> cand_dist for all warps\n    // [.. + WPB*k*sizeof(int)]                             -> cand_idx for all warps\n    extern __shared__ unsigned char smem[];\n    unsigned char* base = smem;\n\n    float2* tile = reinterpret_cast<float2*>(base);\n    size_t off = tile_elems * sizeof(float2);\n    off = round_up(off, 16);\n\n    int* cand_counts = reinterpret_cast<int*>(base + off);\n    off += WARPS_PER_BLOCK * sizeof(int);\n    off = round_up(off, 16);\n\n    float* top_dist_all = reinterpret_cast<float*>(base + off);\n    off += size_t(WARPS_PER_BLOCK) * size_t(k) * sizeof(float);\n\n    int* top_idx_all = reinterpret_cast<int*>(base + off);\n    off += size_t(WARPS_PER_BLOCK) * size_t(k) * sizeof(int);\n\n    float* cand_dist_all = reinterpret_cast<float*>(base + off);\n    off += size_t(WARPS_PER_BLOCK) * size_t(k) * sizeof(float);\n\n    int* cand_idx_all = reinterpret_cast<int*>(base + off);\n    // off += size_t(WARPS_PER_BLOCK) * size_t(k) * sizeof(int); // not needed further\n\n    // Warp info\n    int lane        = threadIdx.x & (WARP_SIZE - 1);\n    int warp_in_blk = threadIdx.x >> 5;  // threadIdx.x / WARP_SIZE\n    int global_warp = blockIdx.x * WARPS_PER_BLOCK + warp_in_blk;\n\n    bool warp_active = (global_warp < query_count);\n    unsigned full_mask = 0xFFFFFFFFu;\n\n    // Pointers to this warp's private shared-memory regions.\n    float* top_dist = top_dist_all + size_t(warp_in_blk) * size_t(k);\n    int*   top_idx  = top_idx_all  + size_t(warp_in_blk) * size_t(k);\n    float* cand_dist= cand_dist_all+ size_t(warp_in_blk) * size_t(k);\n    int*   cand_idx = cand_idx_all + size_t(warp_in_blk) * size_t(k);\n    int*   cand_cnt_ptr = cand_counts + warp_in_blk;\n\n    // Initialize per-warp state.\n    if (lane == 0) {\n        *cand_cnt_ptr = 0;\n    }\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        top_dist[i] = CUDART_INF_F;\n        top_idx[i]  = -1;\n    }\n    __syncwarp(full_mask);\n\n    // Local max_distance (k-th smallest) starts as +Inf. Updated after each merge.\n    float max_d = CUDART_INF_F;\n\n    // Main loop: process data in tiles.\n    for (int tile_base = 0; tile_base < data_count; tile_base += tile_elems) {\n        int tile_size = data_count - tile_base;\n        if (tile_size > tile_elems) tile_size = tile_elems;\n\n        // Load this tile into shared memory cooperatively by the whole block.\n        for (int i = threadIdx.x; i < tile_size; i += blockDim.x) {\n            tile[i] = data[tile_base + i];\n        }\n        __syncthreads();\n\n        // Broadcast this warp's query point.\n        float2 q = make_float2(0.f, 0.f);\n        if (warp_active) {\n            if (lane == 0) q = query[global_warp];\n            q.x = __shfl_sync(full_mask, q.x, 0);\n            q.y = __shfl_sync(full_mask, q.y, 0);\n        }\n\n        // Each warp processes the tile.\n        if (warp_active) {\n            for (int t = lane; t < tile_size; t += WARP_SIZE) {\n                float2 p = tile[t];\n                float dx = p.x - q.x;\n                float dy = p.y - q.y;\n                float d2 = dx * dx + dy * dy; // squared Euclidean distance\n\n                if (d2 < max_d) {\n                    bool pending = true;\n                    int  gidx    = tile_base + t;\n\n                    // Attempt to push this candidate; if buffer fills, flush; if overflow, flush and retry.\n                    while (pending) {\n                        int old = atomicAdd(cand_cnt_ptr, 1);\n                        bool accept   = (old < k);\n                        bool overflow = (old >= k);\n\n                        if (accept) {\n                            cand_dist[old] = d2;\n                            cand_idx[old]  = gidx;\n                        }\n\n                        unsigned over_mask = __ballot_sync(full_mask, pending && overflow);\n                        if (over_mask) {\n                            // Overshoot: some lanes overflowed; bring count back to k and flush.\n                            int overshoot = __popc(over_mask);\n                            int leader    = __ffs(over_mask) - 1;\n                            if (lane == leader) {\n                                atomicSub(cand_cnt_ptr, overshoot);\n                            }\n                            __syncwarp(full_mask);\n\n                            // Flush buffer (exactly k candidates present after subtraction).\n                            float new_max = warp_flush_and_merge(top_dist, top_idx, k,\n                                                                 cand_dist, cand_idx, k, full_mask);\n                            max_d = new_max;\n\n                            if (lane == 0) *cand_cnt_ptr = 0;\n                            __syncwarp(full_mask);\n\n                            // Re-check with tightened max_d whether this candidate should still be inserted.\n                            pending = (d2 < max_d);\n                            continue;\n                        }\n\n                        // Check if we just filled the last slot exactly (no overflow).\n                        unsigned fill_mask = __ballot_sync(full_mask, pending && accept && (old == (k - 1)));\n                        if (fill_mask) {\n                            __syncwarp(full_mask);\n\n                            // Flush buffer (k candidates) and reset.\n                            float new_max = warp_flush_and_merge(top_dist, top_idx, k,\n                                                                 cand_dist, cand_idx, k, full_mask);\n                            max_d = new_max;\n\n                            if (lane == 0) *cand_cnt_ptr = 0;\n                            __syncwarp(full_mask);\n\n                            // This candidate already accepted; nothing to retry.\n                            pending = false;\n                            continue;\n                        }\n\n                        // Accepted and buffer not full: done.\n                        if (accept) {\n                            pending = false;\n                        } else {\n                            // Should not reach here; overflow handled above.\n                            pending = false;\n                        }\n                    } // while pending\n                } // if (d2 < max_d)\n            } // for (t)\n        } // if warp_active\n\n        __syncthreads(); // Ensure all warps done with this tile before reloading shared memory.\n    } // for tiles\n\n    // Final flush of remaining candidates (if any).\n    if (warp_active) {\n        int cc = 0;\n        if (lane == 0) cc = *cand_cnt_ptr;\n        cc = __shfl_sync(full_mask, cc, 0);\n\n        if (cc > 0) {\n            float new_max = warp_flush_and_merge(top_dist, top_idx, k,\n                                                 cand_dist, cand_idx, cc, full_mask);\n            (void)new_max; // max_d updated if needed; not used further.\n            if (lane == 0) *cand_cnt_ptr = 0;\n            __syncwarp(full_mask);\n        }\n\n        // Write final top-k for this query.\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            size_t out_pos = size_t(global_warp) * size_t(k) + size_t(i);\n            result[out_pos].first  = top_idx[i];\n            result[out_pos].second = top_dist[i];\n        }\n    }\n}\n\n// Host-side launcher. Chooses kernel configuration and dynamic shared memory size, and launches the kernel.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data,  int data_count,\n             std::pair<int, float> *result, int k)\n{\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Choose warps per block based on k to keep shared memory usage within limits.\n    // For larger k, use fewer warps per block to accommodate per-warp buffers.\n    int warps_per_block = (k >= 512) ? 4 : 8;\n    int threads_per_block = warps_per_block * WARP_SIZE;\n\n    // Determine the maximum shared memory per block (opt-in if available).\n    int device = 0;\n    cudaGetDevice(&device);\n\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&prop, device);\n\n    int smem_optin = 0;\n    cudaDeviceGetAttribute(&smem_optin, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    int smem_limit = (smem_optin > 0) ? smem_optin : prop.sharedMemPerBlock;\n\n    // Shared memory usage per block:\n    // - Candidate counters: warps_per_block * sizeof(int)\n    // - Per-warp arrays: 2 * k * (sizeof(float) + sizeof(int)) bytes per warp = 16 * k bytes per warp\n    size_t per_warp_bytes = size_t(16) * size_t(k);\n    size_t counters_bytes = size_t(warps_per_block) * sizeof(int);\n\n    // Decide tile size (number of float2 points per tile) to fit into shared memory.\n    // Total dynamic shared memory = tile_bytes + counters + (per_warp_bytes * warps_per_block).\n    size_t fixed_bytes = round_up(counters_bytes, 16) + per_warp_bytes * size_t(warps_per_block);\n    // Leave a small safety margin.\n    size_t safety_margin = 0;\n    if (smem_limit > 0) {\n        safety_margin = 0; // We rely on exact computation with alignment.\n    }\n\n    size_t max_tile_bytes = (smem_limit > fixed_bytes + safety_margin)\n                          ? (smem_limit - fixed_bytes - safety_margin)\n                          : size_t(0);\n    // Ensure tile has at least WARP_SIZE elements; otherwise, reduce warps per block.\n    int tile_elems = 0;\n    if (max_tile_bytes >= sizeof(float2)) {\n        tile_elems = int(max_tile_bytes / sizeof(float2));\n    }\n    if (tile_elems < WARP_SIZE) {\n        // Fallback: reduce warps_per_block to 4 if possible to get more shared memory headroom.\n        if (warps_per_block == 8) {\n            warps_per_block = 4;\n            threads_per_block = warps_per_block * WARP_SIZE;\n            per_warp_bytes = size_t(16) * size_t(k);\n            counters_bytes = size_t(warps_per_block) * sizeof(int);\n            fixed_bytes = round_up(counters_bytes, 16) + per_warp_bytes * size_t(warps_per_block);\n            max_tile_bytes = (smem_limit > fixed_bytes + safety_margin)\n                           ? (smem_limit - fixed_bytes - safety_margin)\n                           : size_t(0);\n            tile_elems = (max_tile_bytes >= sizeof(float2)) ? int(max_tile_bytes / sizeof(float2)) : 0;\n        }\n        if (tile_elems < WARP_SIZE) {\n            // As a last resort, set tile_elems to WARP_SIZE (will fit on A100/H100 with opt-in).\n            tile_elems = WARP_SIZE;\n        }\n    }\n\n    // Use as large a tile as possible but not larger than data_count to avoid wasting load iterations.\n    if (tile_elems > data_count) tile_elems = data_count;\n    if (tile_elems < 1) tile_elems = 1;\n\n    // Compute dynamic shared memory size (must match kernel's layout).\n    size_t tile_bytes = size_t(tile_elems) * sizeof(float2);\n    size_t dyn_smem_bytes = round_up(tile_bytes, 16)\n                          + round_up(counters_bytes, 16)\n                          + size_t(warps_per_block) * size_t(k) * sizeof(float)  // top_dist\n                          + size_t(warps_per_block) * size_t(k) * sizeof(int)    // top_idx\n                          + size_t(warps_per_block) * size_t(k) * sizeof(float)  // cand_dist\n                          + size_t(warps_per_block) * size_t(k) * sizeof(int);   // cand_idx\n\n    // Opt-in to large shared memory if supported.\n    if (smem_optin > 0) {\n        // We need to set the attribute per kernel instantiation.\n        if (warps_per_block == 8) {\n            cudaFuncSetAttribute(knn_kernel_2d<8>, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)dyn_smem_bytes);\n        } else {\n            cudaFuncSetAttribute(knn_kernel_2d<4>, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)dyn_smem_bytes);\n        }\n    }\n\n    // Grid configuration: one warp per query.\n    int total_warps = (query_count + 0); // equals query_count\n    int blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    PairIF* out_pairs = reinterpret_cast<PairIF*>(const_cast<std::pair<int, float>*>(result));\n\n    if (warps_per_block == 8) {\n        knn_kernel_2d<8><<<blocks, threads_per_block, dyn_smem_bytes>>>(\n            query, query_count, data, data_count, out_pairs, k, tile_elems);\n    } else {\n        knn_kernel_2d<4><<<blocks, threads_per_block, dyn_smem_bytes>>>(\n            query, query_count, data, data_count, out_pairs, k, tile_elems);\n    }\n    // Optional: synchronize or check errors if desired by the caller.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 879, "completion_tokens": 26358, "total_tokens": 27237, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 21120, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db91340819087fea2a1d881428e", "custom_id": "knn06-10", "response": {"status_code": 200, "request_id": "fa2822b2f4c8687d392e6b8d4a3540b8", "body": {"id": "chatcmpl-C4sozMLJNRdfIM1w5HHsYyCPFtx4g", "object": "chat.completion", "created": 1755280081, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <stdint.h>\n\n// This implementation assigns exactly one warp (32 threads) per query point.\n// Each warp maintains three per-query arrays in shared memory:\n//  - best_k:    the current top-k nearest neighbors (indices and distances), sorted ascending by distance\n//  - cand_buf:  a buffer of up to k candidate neighbors (indices and distances) collected from recent data batches\n//  - merge_out: temporary output buffer used when merging cand_buf into best_k\n//\n// All warps in the block cooperatively cache the current batch (tile) of data points into shared memory.\n// Each active warp then processes the tile to generate candidate neighbors. The warp uses warp-cooperative\n// reservation of slots in its candidate buffer using atomicAdd on a per-warp shared counter. When the\n// candidate buffer is full (no capacity left), the warp merges cand_buf into best_k. After the final tile,\n// any remaining candidates are also merged.\n//\n// The best_k is always kept sorted ascending; max_distance is the current k-th (worst) distance in best_k.\n// Only distances strictly less than max_distance are inserted into cand_buf.\n//\n// Hyper-parameters chosen for modern data center GPUs (A100/H100):\n//  - Warps per block: 6 (192 threads per block). This provides high warp occupancy while leaving\n//    sufficient shared memory for large k (up to 1024).\n//  - Tile size: 2048 points (16 KB). Fits into shared memory together with the per-warp buffers at k=1024.\n//\n// Notes:\n//  - k is a power of two between 32 and 1024 inclusive.\n//  - data_count >= k.\n//  - query_count and data_count are large; the kernel is designed for throughput.\n//  - The kernel does not allocate device memory; it uses shared memory and per-thread registers only.\n\nstruct PairIF { int first; float second; };\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Tunable parameters\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 6\n#endif\n#ifndef TILE_POINTS\n#define TILE_POINTS 2048\n#endif\n\n// Utility: squared L2 distance for 2D points\nstatic __device__ __forceinline__ float sqr_dist2(const float2 a, const float2 b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    // FMA for better throughput\n    return fmaf(dx, dx, dy * dy);\n}\n\n// Reserve up to 'request' slots in [0, capacity) using atomicAdd on shared counter.\n// Returns base index (via ref) and the number of slots actually reserved (via ref).\n// If no space available, alloc=0. This uses atomicAdd (and atomicSub for overflow correction)\n// to meet the requirement of using atomicAdd to update the number of stored candidates.\nstatic __device__ __forceinline__ void reserve_slots_atomicAdd(int* count_ptr, int capacity, int request, int &base, int &alloc) {\n    // One caller at a time per warp uses this function; others wait for broadcast results.\n    int pos = atomicAdd(count_ptr, request);\n    int avail = capacity - pos;\n    if (avail <= 0) {\n        // No space at all; revert the addition to keep the counter unchanged.\n        atomicSub(count_ptr, request);\n        base = 0;\n        alloc = 0;\n        return;\n    }\n    // Some space available\n    alloc = (avail < request) ? avail : request;\n    base  = pos;\n    if (alloc < request) {\n        // Only a portion was reserved; revert the unallocated part\n        atomicSub(count_ptr, request - alloc);\n    }\n}\n\n// Bitonic sort (ascending) for key-value pairs stored in shared memory.\n// Sorts the first n elements. n must be <= capacity; the algorithm sorts up to n_p2,\n// the next power-of-two >= n, and assumes that keys[i] = +INF for i in [n, n_p2).\nstatic __device__ __forceinline__ void warp_bitonic_sort_pairs(float* keys, int* vals, int n, int capacity, unsigned mask) {\n    if (n <= 1) return;\n\n    // Compute next power of two >= n, but not exceeding capacity\n    int n_p2 = 1;\n    while (n_p2 < n) n_p2 <<= 1;\n    if (n_p2 > capacity) n_p2 = capacity;\n\n    // Pad the tail [n, n_p2) with +INF and a dummy index, so sort network behaves correctly.\n    const float INF_F = CUDART_INF_F;\n    for (int i = threadIdx.x & (WARP_SIZE - 1); i < n_p2; i += WARP_SIZE) {\n        if (i >= n) {\n            keys[i] = INF_F;\n            vals[i] = -1;\n        }\n    }\n    __syncwarp(mask);\n\n    // Standard bitonic sort network\n    for (int k = 2; k <= n_p2; k <<= 1) {\n        // Merge bitonic sequences of length k\n        for (int j = k >> 1; j > 0; j >>= 1) {\n            for (int i = threadIdx.x & (WARP_SIZE - 1); i < n_p2; i += WARP_SIZE) {\n                int ixj = i ^ j;\n                if (ixj > i) {\n                    bool ascending = ((i & k) == 0); // Determine sort direction for this subsequence\n                    float key_i = keys[i];\n                    float key_j = keys[ixj];\n                    int val_i = vals[i];\n                    int val_j = vals[ixj];\n                    bool comp = (key_i > key_j);\n                    if (!ascending) comp = !comp; // reverse condition for descending\n                    if (comp) {\n                        // Swap\n                        keys[i] = key_j;\n                        keys[ixj] = key_i;\n                        vals[i] = val_j;\n                        vals[ixj] = val_i;\n                    }\n                }\n            }\n            __syncwarp(mask);\n        }\n    }\n    // After sorting, the first n elements are in ascending order; padded elements are +INF at the end.\n}\n\n// Merge (k and c elements, both ascending) into top-k smallest (ascending).\n// - best_k_dist/best_k_idx: input sorted ascending, length k\n// - cand_dist/cand_idx: input sorted ascending, length c\n// - out_dist/out_idx: output buffer of length k (ascending)\n// This uses a warp-cooperative merge path partition; each lane produces out elements at stride WARP_SIZE.\nstatic __device__ __forceinline__ void warp_merge_topk_sorted(\n    const float* __restrict__ best_k_dist,\n    const int*   __restrict__ best_k_idx,\n    int k,\n    const float* __restrict__ cand_dist,\n    const int*   __restrict__ cand_idx,\n    int c,\n    float* __restrict__ out_dist,\n    int*   __restrict__ out_idx,\n    unsigned mask)\n{\n    // Handle trivial cases\n    if (c <= 0) {\n        for (int t = threadIdx.x & (WARP_SIZE - 1); t < k; t += WARP_SIZE) {\n            out_dist[t] = best_k_dist[t];\n            out_idx[t]  = best_k_idx[t];\n        }\n        __syncwarp(mask);\n        return;\n    }\n    // Merge-path per output position t in [0, k)\n    const float INF_F = CUDART_INF_F;\n    for (int t = threadIdx.x & (WARP_SIZE - 1); t < k; t += WARP_SIZE) {\n        int low  = max(0, t - c);\n        int high = min(t, k);\n        // Binary search on diagonal t: find i such that best[i-1] <= cand[t-i] and cand[t-i-1] < best[i]\n        while (low < high) {\n            int mid = (low + high) >> 1;\n            int j = t - mid;\n            float left_best = (mid > 0) ? best_k_dist[mid - 1] : -CUDART_INF_F;\n            float left_cand = (j   > 0) ? cand_dist[j - 1]     : -CUDART_INF_F;\n            if (left_best <= left_cand) {\n                low = mid + 1;\n            } else {\n                high = mid;\n            }\n        }\n        int i = low;\n        int j = t - i;\n        float next_best = (i < k) ? best_k_dist[i] : INF_F;\n        float next_cand = (j < c) ? cand_dist[j]   : INF_F;\n        if (next_best <= next_cand) {\n            out_dist[t] = next_best;\n            out_idx[t]  = best_k_idx[i];\n        } else {\n            out_dist[t] = next_cand;\n            out_idx[t]  = cand_idx[j];\n        }\n    }\n    __syncwarp(mask);\n}\n\n// Merge cand_buf into best_k (both in shared memory), update max_distance and reset cand_count to 0.\n// - best_k_* are always kept sorted ascending.\n// - cand_buf_* are sorted (ascending) here before merging.\n// - merge_out_* are temporary buffers to hold the merged top-k before copying back to best_k_*.\nstatic __device__ __forceinline__ void warp_flush_and_merge(\n    float* best_k_dist, int* best_k_idx,\n    float* cand_dist,   int* cand_idx,\n    float* merge_out_dist, int* merge_out_idx,\n    int k,\n    int* cand_count_ptr,\n    float* max_dist_ptr,\n    unsigned mask)\n{\n    // All threads in this warp must call this function together\n    __syncwarp(mask);\n\n    int c;\n    if ((threadIdx.x & (WARP_SIZE - 1)) == 0) {\n        c = *cand_count_ptr;\n    }\n    c = __shfl_sync(mask, c, 0);\n    if (c <= 0) {\n        return;\n    }\n\n    // Sort candidate buffer (ascending). Only need first c entries.\n    warp_bitonic_sort_pairs(cand_dist, cand_idx, c, k, mask);\n\n    // Merge best_k (k ascending) with cand (c ascending) into top-k (ascending)\n    warp_merge_topk_sorted(best_k_dist, best_k_idx, k, cand_dist, cand_idx, c, merge_out_dist, merge_out_idx, mask);\n\n    // Copy merged result back into best_k\n    for (int i = threadIdx.x & (WARP_SIZE - 1); i < k; i += WARP_SIZE) {\n        best_k_dist[i] = merge_out_dist[i];\n        best_k_idx[i]  = merge_out_idx[i];\n    }\n    __syncwarp(mask);\n\n    // Update max_distance = k-th (worst) distance\n    if ((threadIdx.x & (WARP_SIZE - 1)) == 0) {\n        *max_dist_ptr = best_k_dist[k - 1];\n        // Reset candidate count\n        *cand_count_ptr = 0;\n    }\n    __syncwarp(mask);\n}\n\nstatic __global__ void knn_kernel(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data,  int data_count,\n    PairIF* __restrict__ result,\n    int k)\n{\n    // Warp and lane identifiers\n    const int lane       = threadIdx.x & (WARP_SIZE - 1);\n    const int warp_in_blk= threadIdx.x >> 5; // 0..WARPS_PER_BLOCK-1\n    const int warps_per_block = WARPS_PER_BLOCK;\n    const unsigned FULL_MASK = 0xFFFFFFFFu;\n\n    // Global warp id maps to query id\n    const int global_warp_id = blockIdx.x * warps_per_block + warp_in_blk;\n    const bool warp_active = (global_warp_id < query_count);\n\n    // Dynamic shared memory layout:\n    extern __shared__ unsigned char smem[];\n    unsigned char* ptr = smem;\n\n    // Shared tile for data points (cached by the block)\n    float2* tile = reinterpret_cast<float2*>(ptr);\n    ptr += sizeof(float2) * TILE_POINTS;\n\n    // Per-warp arrays stored contiguously across the block\n    // Layout for all warps in block: [best_dist][best_idx][cand_dist][cand_idx][merge_out_dist][merge_out_idx][cand_counts][max_dists]\n    float* best_all_dist = reinterpret_cast<float*>(ptr);\n    ptr += sizeof(float) * warps_per_block * k;\n\n    int*   best_all_idx  = reinterpret_cast<int*>(ptr);\n    ptr += sizeof(int)   * warps_per_block * k;\n\n    float* cand_all_dist = reinterpret_cast<float*>(ptr);\n    ptr += sizeof(float) * warps_per_block * k;\n\n    int*   cand_all_idx  = reinterpret_cast<int*>(ptr);\n    ptr += sizeof(int)   * warps_per_block * k;\n\n    float* merge_all_dist= reinterpret_cast<float*>(ptr);\n    ptr += sizeof(float) * warps_per_block * k;\n\n    int*   merge_all_idx = reinterpret_cast<int*>(ptr);\n    ptr += sizeof(int)   * warps_per_block * k;\n\n    int*   cand_counts   = reinterpret_cast<int*>(ptr);\n    ptr += sizeof(int)   * warps_per_block;\n\n    float* max_dists     = reinterpret_cast<float*>(ptr);\n    // ptr += sizeof(float) * warps_per_block; // not needed further\n\n    // Pointers to this warp's per-query buffers in shared memory\n    float* best_k_dist   = best_all_dist   + warp_in_blk * k;\n    int*   best_k_idx    = best_all_idx    + warp_in_blk * k;\n    float* cand_dist     = cand_all_dist   + warp_in_blk * k;\n    int*   cand_idx      = cand_all_idx    + warp_in_blk * k;\n    float* merge_out_dist= merge_all_dist  + warp_in_blk * k;\n    int*   merge_out_idx = merge_all_idx   + warp_in_blk * k;\n    int*   cand_count_ptr= cand_counts     + warp_in_blk;\n    float* max_dist_ptr  = max_dists       + warp_in_blk;\n\n    // Initialize per-warp structures\n    if (warp_active) {\n        // best_k initialized to +INF distances and invalid indices; kept sorted ascending (trivially)\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            best_k_dist[i] = CUDART_INF_F;\n            best_k_idx[i]  = -1;\n        }\n        if (lane == 0) {\n            *cand_count_ptr = 0;\n            *max_dist_ptr   = CUDART_INF_F;\n        }\n    } else {\n        // Still initialize shared structures to avoid undefined behavior\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            best_k_dist[i] = CUDART_INF_F;\n            best_k_idx[i]  = -1;\n        }\n        if (lane == 0) {\n            *cand_count_ptr = 0;\n            *max_dist_ptr   = CUDART_INF_F;\n        }\n    }\n    __syncwarp(FULL_MASK);\n\n    // Load the query point for this warp and broadcast\n    float2 q = make_float2(0.f, 0.f);\n    if (lane == 0 && warp_active) {\n        q = query[global_warp_id];\n    }\n    q.x = __shfl_sync(FULL_MASK, q.x, 0);\n    q.y = __shfl_sync(FULL_MASK, q.y, 0);\n\n    // Iterate over data points in tiles\n    for (int base = 0; base < data_count; base += TILE_POINTS) {\n        int tile_count = data_count - base;\n        if (tile_count > TILE_POINTS) tile_count = TILE_POINTS;\n\n        // Block-wide cooperative load of data into shared memory tile\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            tile[i] = data[base + i];\n        }\n        __syncthreads(); // ensure tile is ready\n\n        // Process tile: each warp computes distances for its query and collects candidates\n        if (warp_active) {\n            float current_maxd = *max_dist_ptr;\n\n            for (int ti = lane; ti < tile_count; ti += WARP_SIZE) {\n                // Compute squared distance for this data point\n                float2 p = tile[ti];\n                float d  = sqr_dist2(p, q);\n                int   idx = base + ti;\n\n                // Consider as candidate only if strictly better than current k-th best\n                bool is_pending = (d < current_maxd);\n\n                // Attempt to insert this element into cand_buf; if buffer full, flush and retry if still qualifies\n                while (true) {\n                    unsigned pending_mask = __ballot_sync(FULL_MASK, is_pending);\n                    int need = __popc(pending_mask);\n                    if (need == 0) break;\n\n                    int base_pos = 0, alloc = 0;\n                    if (lane == 0) {\n                        reserve_slots_atomicAdd(cand_count_ptr, k, need, base_pos, alloc);\n                    }\n                    base_pos = __shfl_sync(FULL_MASK, base_pos, 0);\n                    alloc    = __shfl_sync(FULL_MASK, alloc,    0);\n\n                    if (alloc > 0) {\n                        // Compute position among pending lanes\n                        int my_rank = __popc(pending_mask & ((1u << lane) - 1));\n                        if (is_pending && my_rank < alloc) {\n                            int wpos = base_pos + my_rank;\n                            // Insert into candidate buffer\n                            cand_dist[wpos] = d;\n                            cand_idx[wpos]  = idx;\n                            // Mark as inserted\n                            is_pending = false;\n                        }\n                        // If some pending were not inserted due to insufficient space, they will loop again\n                    } else {\n                        // No capacity at all; flush/merge and update current_maxd, then re-evaluate pending\n                        warp_flush_and_merge(best_k_dist, best_k_idx,\n                                             cand_dist, cand_idx,\n                                             merge_out_dist, merge_out_idx,\n                                             k, cand_count_ptr, max_dist_ptr, FULL_MASK);\n                        current_maxd = *max_dist_ptr;\n                        // Re-evaluate if this element still qualifies after the threshold decreased\n                        is_pending = (d < current_maxd);\n                    }\n                }\n            }\n\n            // After finishing the tile, flush remaining candidates (if any)\n            int remaining = 0;\n            if (lane == 0) remaining = *cand_count_ptr;\n            remaining = __shfl_sync(FULL_MASK, remaining, 0);\n            if (remaining > 0) {\n                warp_flush_and_merge(best_k_dist, best_k_idx,\n                                     cand_dist, cand_idx,\n                                     merge_out_dist, merge_out_idx,\n                                     k, cand_count_ptr, max_dist_ptr, FULL_MASK);\n            }\n        }\n\n        __syncthreads(); // ensure all warps are done with this tile before overwriting it\n    }\n\n    // Write out final results for this query\n    if (warp_active) {\n        // best_k_dist/best_k_idx are sorted ascending\n        int out_base = global_warp_id * k;\n        // Each lane writes a strided subset\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            PairIF r; r.first = best_k_idx[i]; r.second = best_k_dist[i];\n            result[out_base + i] = r;\n        }\n    }\n}\n\n// Host API\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    // Reinterpret result pointer as a trivial pair struct to avoid device-side std::pair dependencies\n    PairIF* d_result = reinterpret_cast<PairIF*>(result);\n\n    const int warps_per_block = WARPS_PER_BLOCK;\n    const int threads_per_block = warps_per_block * WARP_SIZE;\n    const int num_warps = (query_count + warps_per_block - 1) / warps_per_block;\n    const dim3 grid(num_warps);\n    const dim3 block(threads_per_block);\n\n    // Compute dynamic shared memory size:\n    // - TILE_POINTS float2:  TILE_POINTS * 8 bytes\n    // - Per-warp arrays (for all warps in the block):\n    //     best_dist:      w * k * 4\n    //     best_idx:       w * k * 4\n    //     cand_dist:      w * k * 4\n    //     cand_idx:       w * k * 4\n    //     merge_out_dist: w * k * 4\n    //     merge_out_idx:  w * k * 4\n    //     cand_counts:    w * 4\n    //     max_dists:      w * 4\n    size_t shmem_size = 0;\n    shmem_size += sizeof(float2) * TILE_POINTS;\n    shmem_size += (size_t)warps_per_block * k * sizeof(float); // best_dist\n    shmem_size += (size_t)warps_per_block * k * sizeof(int);   // best_idx\n    shmem_size += (size_t)warps_per_block * k * sizeof(float); // cand_dist\n    shmem_size += (size_t)warps_per_block * k * sizeof(int);   // cand_idx\n    shmem_size += (size_t)warps_per_block * k * sizeof(float); // merge_out_dist\n    shmem_size += (size_t)warps_per_block * k * sizeof(int);   // merge_out_idx\n    shmem_size += (size_t)warps_per_block * sizeof(int);       // cand_counts\n    shmem_size += (size_t)warps_per_block * sizeof(float);     // max_dists\n\n    // Launch kernel\n    knn_kernel<<<grid, block, shmem_size>>>(query, query_count, data, data_count, d_result, k);\n    // The caller is responsible for checking/handling CUDA errors and synchronization if desired.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 879, "completion_tokens": 26611, "total_tokens": 27490, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 21824, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db8af8c81909c66776649f2d8b4", "custom_id": "knn07-01", "response": {"status_code": 200, "request_id": "6579ca0cce7a30a10941414bb03325e9", "body": {"id": "chatcmpl-C4t4TFYsqwBkELlECeT2TbBUfKwyy", "object": "chat.completion", "created": 1755281041, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#include <utility>\n#include <limits>\n\n// This implementation assigns one warp (32 threads) per query point.\n// The kernel processes the dataset in batches (tiles) cached in shared memory,\n// and uses per-warp shared memory for:\n// - intermediate result (k nearest so far), sorted ascending by distance\n// - candidate buffer (up to k candidates), unsorted between merges\n// - temporary buffer used during merging (k elements)\n// - a per-warp shared counter (atomicAdd) to count candidates currently in the buffer\n//\n// Whenever a buffer fills (candidate count >= k), the warp:\n// 1) Sorts the candidate buffer (bitonic sort, ascending)\n// 2) Merges the candidate buffer with the intermediate result by taking element-wise minima\n//    between buffer[i] and result[k-1-i], forming a bitonic sequence of k elements\n// 3) Sorts the merged sequence (bitonic sort, ascending) to obtain the new intermediate result\n//\n// After the last tile is processed, the warp merges any remaining candidates as above,\n// and then writes out the final k nearest neighbor indices and distances for its query.\n//\n// Notes:\n// - Distance is squared Euclidean in 2D.\n// - k is a power of two within [32, 1024].\n// - Shared memory is dynamically partitioned into per-warp areas and a data tile buffer.\n// - We use warp-synchronous programming with __syncwarp for warp coordination.\n// - AtomicAdd on the per-warp counter is used with a warp-wide reservation using ballot/popc,\n//   which prevents buffer overflow and allows immediate merging when the buffer fills.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Device utility: warp-synchronous bitonic sort for paired arrays (distance + index), ascending by distance.\n// - n must be a power-of-two (here, n == k).\n// - All threads in the warp participate; each thread processes indices i = lane, i += warpSize.\n// - Uses shared memory arrays for dist and idx; operations are in-place.\n__device__ __forceinline__\nvoid warp_bitonic_sort_pairs_ascending(int n, float* dist, int* idx, unsigned mask, int lane) {\n    // Outer loop: size of bitonic sequences\n    for (int k = 2; k <= n; k <<= 1) {\n        // Inner loop: stride within sequences\n        for (int j = k >> 1; j > 0; j >>= 1) {\n            // Each thread processes several indices in [0, n)\n            for (int i = lane; i < n; i += WARP_SIZE) {\n                int l = i ^ j;\n                if (l > i) {\n                    bool ascending = ((i & k) == 0);\n                    float ai = dist[i];\n                    float al = dist[l];\n                    // Perform compare-and-swap depending on direction\n                    bool do_swap = (ascending && ai > al) || (!ascending && ai < al);\n                    if (do_swap) {\n                        dist[i] = al;\n                        dist[l] = ai;\n                        int ti = idx[i];\n                        idx[i] = idx[l];\n                        idx[l] = ti;\n                    }\n                }\n            }\n            __syncwarp(mask);\n        }\n    }\n    __syncwarp(mask);\n}\n\n// Device utility: merge candidate buffer into the intermediate result.\n// Steps:\n// 1) Sort buffer (ascending)\n// 2) Create bitonic merge array tmp[i] = min(buffer[i], result[k-1-i])\n// 3) Sort tmp (ascending) and write back to result\n// 4) Update maxDistance and reset candidate counter and buffer contents to +inf/-1\n__device__ __forceinline__\nvoid warp_merge_buffer_into_result(\n    int k,\n    int* bufIdx, float* bufDist,\n    int* resIdx, float* resDist,\n    int* tmpIdx, float* tmpDist,\n    volatile int* candCountPtr,\n    unsigned mask, int lane,\n    float& maxDistance)\n{\n    // 1) Sort candidate buffer\n    warp_bitonic_sort_pairs_ascending(k, bufDist, bufIdx, mask, lane);\n\n    // 2) Element-wise min between buffer[i] and result[k-1-i] to form a bitonic sequence\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        float bd = bufDist[i]; int bi = bufIdx[i];\n        float rd = resDist[k - 1 - i]; int ri = resIdx[k - 1 - i];\n        if (bd < rd) {\n            tmpDist[i] = bd; tmpIdx[i] = bi;\n        } else {\n            tmpDist[i] = rd; tmpIdx[i] = ri;\n        }\n    }\n    __syncwarp(mask);\n\n    // 3) Sort the merged (bitonic) sequence ascending, then write back to result\n    warp_bitonic_sort_pairs_ascending(k, tmpDist, tmpIdx, mask, lane);\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        resDist[i] = tmpDist[i];\n        resIdx[i] = tmpIdx[i];\n    }\n    __syncwarp(mask);\n\n    // 4) Update maxDistance, reset candidate counter and clear buffer (set INF/-1)\n    if (lane == 0) {\n        maxDistance = resDist[k - 1];\n        *candCountPtr = 0;\n    }\n    maxDistance = __shfl_sync(mask, maxDistance, 0);\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        bufDist[i] = CUDART_INF_F;\n        bufIdx[i] = -1;\n    }\n    __syncwarp(mask);\n}\n\n// Device utility: warp-synchronous buffered insertion with overflow-safe merging.\n// Each lane can optionally provide a candidate (cand==true) with (candIdx, candDist).\n// The function reserves space in the per-warp candidate buffer using a single atomicAdd,\n// then writes successful candidates (within buffer capacity).\n// If the reservation causes the buffer to fill (base + nCand >= k), the warp merges the buffer\n// into the intermediate result immediately and retries unresolved candidates.\n// maxDistance is updated during merges and broadcast to all lanes.\n__device__ __forceinline__\nvoid warp_try_push_candidate(\n    bool cand, int candIdx, float candDist,\n    volatile int* candCountPtr, // shared counter for this warp\n    int* bufIdx, float* bufDist,\n    int* resIdx, float* resDist,\n    int* tmpIdx, float* tmpDist,\n    int k, unsigned mask, int lane,\n    float& maxDistance)\n{\n    while (true) {\n        unsigned active = __ballot_sync(mask, cand);\n        int n = __popc(active);\n        if (n == 0) break; // No pending candidates to insert\n\n        // Compute rank among active lanes\n        unsigned laneMask = (1u << lane) - 1u;\n        int rank = __popc(active & laneMask);\n\n        // Reserve slots in buffer with a single atomicAdd by lane 0\n        int base = 0;\n        if ((threadIdx.x & (WARP_SIZE - 1)) == 0) {\n            base = atomicAdd((int*)candCountPtr, n);\n        }\n        base = __shfl_sync(mask, base, 0);\n\n        int pos = base + rank;\n        bool success = cand && (pos < k);\n\n        // Write successful candidates (within capacity)\n        if (success) {\n            bufIdx[pos] = candIdx;\n            bufDist[pos] = candDist;\n        }\n        __syncwarp(mask);\n\n        // If buffer filled or overflowed, merge now; losers will retry\n        bool needMerge = (base + n) >= k;\n        if (needMerge) {\n            warp_merge_buffer_into_result(k, bufIdx, bufDist, resIdx, resDist,\n                                          tmpIdx, tmpDist, candCountPtr, mask, lane, maxDistance);\n            cand = cand && !success; // losers remain pending\n        } else {\n            // No merge needed; all pending candidates were inserted successfully\n            break;\n        }\n    }\n}\n\n// Kernel: one warp per query. The whole block loads data tiles into shared memory.\n// Dynamic shared memory layout:\n// [ per-warp data (W warps): (resIdx[k], resDist[k], bufIdx[k], bufDist[k], tmpIdx[k], tmpDist[k]) ] +\n// [ per-warp counters (W ints) ] +\n// [ data tile buffer (tilePoints float2) ]\n__global__\nvoid knn_kernel_2d_warp(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data, int data_count,\n    std::pair<int, float>* __restrict__ out,\n    int k, int tilePoints)\n{\n    extern __shared__ unsigned char smem[];\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n    const int warpInBlock = threadIdx.x >> 5;\n    const int warpsPerBlock = blockDim.x >> 5;\n    const int warpGlobal = blockIdx.x * warpsPerBlock + warpInBlock;\n    const unsigned fullMask = 0xFFFFFFFFu;\n\n    // Compute per-warp shared memory region\n    const size_t perWarpStrideBytes =\n        (size_t)k * sizeof(int)   + (size_t)k * sizeof(float) + // resIdx, resDist\n        (size_t)k * sizeof(int)   + (size_t)k * sizeof(float) + // bufIdx, bufDist\n        (size_t)k * sizeof(int)   + (size_t)k * sizeof(float);  // tmpIdx, tmpDist\n\n    unsigned char* base = smem;\n    unsigned char* warpBase = base + perWarpStrideBytes * warpInBlock;\n\n    int*   resIdx = reinterpret_cast<int*>(warpBase);\n    float* resDist = reinterpret_cast<float*>(resIdx + k);\n    int*   bufIdx = reinterpret_cast<int*>(resDist + k);\n    float* bufDist = reinterpret_cast<float*>(bufIdx + k);\n    int*   tmpIdx = reinterpret_cast<int*>(bufDist + k);\n    float* tmpDist = reinterpret_cast<float*>(tmpIdx + k);\n\n    // Counters area for all warps in block\n    unsigned char* ctrBase = base + perWarpStrideBytes * warpsPerBlock;\n    int* counters = reinterpret_cast<int*>(ctrBase);\n    volatile int* candCountPtr = counters + warpInBlock;\n\n    // Data tile area\n    unsigned char* tileBase = reinterpret_cast<unsigned char*>(counters + warpsPerBlock);\n    float2* tile = reinterpret_cast<float2*>(tileBase);\n\n    // All threads in the block will participate in tile loads (__syncthreads barriers).\n    // Only warps mapped to a valid query will compute k-NN.\n    bool warpActive = (warpGlobal < query_count);\n\n    // Initialize per-warp structures\n    float qx = 0.0f, qy = 0.0f;\n    float maxDistance = CUDART_INF_F;\n\n    if (warpActive) {\n        // Load query point and broadcast within warp\n        if (lane == 0) {\n            float2 q = query[warpGlobal];\n            qx = q.x; qy = q.y;\n        }\n        qx = __shfl_sync(fullMask, qx, 0);\n        qy = __shfl_sync(fullMask, qy, 0);\n\n        // Initialize intermediate result with +inf distances and invalid indices\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            resIdx[i] = -1;\n            resDist[i] = CUDART_INF_F;\n            bufIdx[i] = -1;\n            bufDist[i] = CUDART_INF_F;\n            tmpIdx[i] = -1;\n            tmpDist[i] = CUDART_INF_F;\n        }\n        if (lane == 0) {\n            *candCountPtr = 0;\n        }\n        __syncwarp(fullMask);\n        maxDistance = CUDART_INF_F;\n    }\n\n    // Process data in tiles cached in shared memory\n    for (int tileStart = 0; tileStart < data_count; tileStart += tilePoints) {\n        int count = data_count - tileStart;\n        if (count > tilePoints) count = tilePoints;\n\n        // Load tile into shared memory using the entire block\n        for (int t = threadIdx.x; t < count; t += blockDim.x) {\n            tile[t] = data[tileStart + t];\n        }\n        __syncthreads(); // Ensure tile is loaded before any warp computes\n\n        if (warpActive) {\n            // Each lane processes a strided subset of the tile\n            float localMax = maxDistance; // cache of maxDistance (updated only after merges)\n            for (int t = lane; t < count; t += WARP_SIZE) {\n                float2 p = tile[t];\n                float dx = p.x - qx;\n                float dy = p.y - qy;\n                float d2 = dx * dx + dy * dy;\n                bool isCand = d2 < localMax;\n\n                // Try to push candidate; may trigger merge if buffer fills\n                if (isCand) {\n                    int dataIdx = tileStart + t;\n                    warp_try_push_candidate(true, dataIdx, d2,\n                                            candCountPtr, bufIdx, bufDist,\n                                            resIdx, resDist, tmpIdx, tmpDist,\n                                            k, fullMask, lane, localMax);\n                    // Update cached threshold after potential merge\n                    maxDistance = localMax;\n                }\n            }\n        }\n\n        __syncthreads(); // Ensure all warps done with this tile before it is overwritten\n    }\n\n    // After processing all tiles: if buffer has unused candidates, merge them.\n    if (warpActive) {\n        int pending = 0;\n        if (lane == 0) pending = *candCountPtr;\n        pending = __shfl_sync(fullMask, pending, 0);\n        if (pending > 0) {\n            warp_merge_buffer_into_result(k, bufIdx, bufDist, resIdx, resDist,\n                                          tmpIdx, tmpDist, candCountPtr, fullMask, lane, maxDistance);\n        }\n\n        // Write out final k nearest neighbors for this query\n        int outBase = warpGlobal * k;\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            out[outBase + i].first = resIdx[i];\n            out[outBase + i].second = resDist[i];\n        }\n    }\n}\n\n// Host interface: determines a suitable launch configuration (warps per block, tile size, shared memory),\n// launches the kernel, and relies solely on the provided device allocations.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data, int data_count,\n             std::pair<int, float> *result, int k)\n{\n    // Device properties related to shared memory\n    int device = 0;\n    cudaGetDevice(&device);\n\n    int maxSharedOptin = 0;\n    cudaDeviceGetAttribute(&maxSharedOptin, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    int maxSharedDefault = 0;\n    cudaDeviceGetAttribute(&maxSharedDefault, cudaDevAttrMaxSharedMemoryPerBlock, device);\n\n    // Use the largest available shared memory per block\n    int maxSharedPerBlock = (maxSharedOptin > 0) ? maxSharedOptin : maxSharedDefault;\n\n    // Per-warp shared memory footprint (bytes)\n    size_t perWarpStrideBytes =\n        (size_t)k * sizeof(int)   + (size_t)k * sizeof(float) + // resIdx, resDist\n        (size_t)k * sizeof(int)   + (size_t)k * sizeof(float) + // bufIdx, bufDist\n        (size_t)k * sizeof(int)   + (size_t)k * sizeof(float);  // tmpIdx, tmpDist\n\n    // Choose warps per block and tile size to fit in shared memory\n    int bestWarpsPerBlock = 0;\n    int bestTilePoints = 0;\n    size_t bestSmemBytes = 0;\n\n    // Limit warps per block to not exceed 1024 threads per block\n    const int maxWarpsByThreads = 1024 / WARP_SIZE;\n    // Reasonable upper bound on warps per block to balance shared memory use and parallelism\n    const int maxWarpsConsider = (maxWarpsByThreads < 8) ? maxWarpsByThreads : 8;\n\n    // Prefer a reasonably large tile for global memory efficiency\n    const int preferredTilePoints = 4096;\n\n    for (int warps = maxWarpsConsider; warps >= 1; --warps) {\n        size_t perBlockWarpArea = perWarpStrideBytes * (size_t)warps;\n        size_t countersBytes = sizeof(int) * (size_t)warps;\n        // Compute the maximum tile size that fits into shared memory\n        size_t leftover = (maxSharedPerBlock > (perBlockWarpArea + countersBytes))\n                        ? (maxSharedPerBlock - perBlockWarpArea - countersBytes) : 0;\n        int tilePoints = (int)(leftover / sizeof(float2));\n        if (tilePoints <= 0) continue;\n\n        // Cap tile size at preferredTilePoints if possible\n        if (tilePoints > preferredTilePoints) tilePoints = preferredTilePoints;\n\n        // Align tile size to a multiple of blockDim.x for coalesced loads\n        int blockThreads = warps * WARP_SIZE;\n        int alignedTile = (tilePoints / blockThreads) * blockThreads;\n        if (alignedTile == 0) alignedTile = blockThreads; // at least one per-thread load when possible\n        tilePoints = alignedTile;\n\n        size_t totalSmem = perBlockWarpArea + countersBytes + (size_t)tilePoints * sizeof(float2);\n        if (totalSmem <= (size_t)maxSharedPerBlock) {\n            bestWarpsPerBlock = warps;\n            bestTilePoints = tilePoints;\n            bestSmemBytes = totalSmem;\n            break;\n        }\n    }\n\n    // Fallback: ensure we have at least one warp and a minimal tile\n    if (bestWarpsPerBlock == 0) {\n        bestWarpsPerBlock = 1;\n        int blockThreads = bestWarpsPerBlock * WARP_SIZE;\n        size_t perBlockWarpArea = perWarpStrideBytes * (size_t)bestWarpsPerBlock;\n        size_t countersBytes = sizeof(int) * (size_t)bestWarpsPerBlock;\n        size_t leftover = (maxSharedPerBlock > (perBlockWarpArea + countersBytes))\n                        ? (maxSharedPerBlock - perBlockWarpArea - countersBytes) : 0;\n        int tilePoints = (int)(leftover / sizeof(float2));\n        if (tilePoints <= 0) tilePoints = blockThreads; // minimal viable tile\n        // Align tile size\n        int alignedTile = (tilePoints / blockThreads) * blockThreads;\n        if (alignedTile == 0) alignedTile = blockThreads;\n        bestTilePoints = alignedTile;\n        bestSmemBytes = perBlockWarpArea + countersBytes + (size_t)bestTilePoints * sizeof(float2);\n    }\n\n    // Configure launch\n    dim3 block(bestWarpsPerBlock * WARP_SIZE, 1, 1);\n    dim3 grid((query_count + bestWarpsPerBlock - 1) / bestWarpsPerBlock, 1, 1);\n\n    // Opt-in to large dynamic shared memory if available/needed\n    cudaFuncSetAttribute(knn_kernel_2d_warp, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)bestSmemBytes);\n\n    // Launch kernel\n    knn_kernel_2d_warp<<<grid, block, bestSmemBytes>>>(query, query_count, data, data_count, result, k, bestTilePoints);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1203, "completion_tokens": 22742, "total_tokens": 23945, "prompt_tokens_details": {"cached_tokens": 1152, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 18432, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db9500081909248f4d01a5ed4d2", "custom_id": "knn07-02", "response": {"status_code": 200, "request_id": "b2b0c472fd279e777a99f1ede2bd737b", "body": {"id": "chatcmpl-C4sr7XfV5rb1EkfeLE77PWVfnY0hu", "object": "chat.completion", "created": 1755280213, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n#include <cfloat>\n\n// Optimized CUDA implementation of k-NN for 2D points using one warp (32 threads) per query.\n// The algorithm processes the dataset in shared-memory tiles. Each warp maintains:\n// - A private \"intermediate result\" top-k (indices and distances) kept sorted in ascending order.\n// - A per-warp candidate buffer in shared memory with capacity k, and a shared atomic counter.\n// New candidates with distance < max_distance (k-th distance in the intermediate result) are pushed\n// into the buffer using warp-aggregated atomicAdd. When the buffer gets full, it is sorted with\n// Bitonic Sort, merged with the intermediate result using the described min-pairing trick to create\n// a bitonic sequence of length k, then sorted again with Bitonic Sort to update the intermediate result.\n// At the end, any remaining candidates in the buffer are merged as well.\n//\n// Notes:\n// - k is a power of two in [32, 1024], making Bitonic Sort straightforward.\n// - No extra device memory is allocated; all temporary storage is in shared memory.\n// - For performance, we use warp-aggregated atomics (one atomicAdd per warp event) and shared memory tiling.\n// - The result array is of type std::pair<int, float>, but on device we reinterpret it as a POD struct.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Tunable parameters: number of warps per block and shared-memory tile size for data points.\n// WARPS_PER_BLOCK influences shared memory usage per block: each warp needs ~ (4 * k * sizeof(T)) bytes.\n// TILE_SIZE * sizeof(float2) is also allocated in shared memory per block.\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 4\n#endif\n\n#ifndef TILE_SIZE\n#define TILE_SIZE 2048\n#endif\n\n// POD type compatible with std::pair<int, float> memory layout\nstruct PairIF { int first; float second; };\n\nstatic __device__ __forceinline__ float sqdist2(const float2& a, const float2& b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    // Use FMA to improve throughput: dx*dx + dy*dy\n    return fmaf(dy, dy, dx * dx);\n}\n\n// Warp-scope Bitonic Sort for arrays stored in shared memory.\n// - Sorts 'dist[0..n-1]' ascending and applies the same swaps to 'idx'.\n// - n must be a power of two; here n == k in [32, 1024], so it's valid.\n// - Parallelization: each lane processes indices i = lane + t * WARP_SIZE.\n//   We use __syncwarp between stages to ensure memory ordering across the warp.\nstatic __device__ __forceinline__ void warp_bitonic_sort_asc(float* dist, int* idx, int n) {\n    unsigned mask = 0xFFFFFFFFu;\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n\n    for (int k = 2; k <= n; k <<= 1) {\n        for (int j = k >> 1; j > 0; j >>= 1) {\n            for (int i = lane; i < n; i += WARP_SIZE) {\n                int l = i ^ j;\n                if (l > i) {\n                    bool up = ((i & k) == 0);\n                    float di = dist[i];\n                    float dl = dist[l];\n                    int ii = idx[i];\n                    int il = idx[l];\n                    // Bitonic compare-and-swap:\n                    // if (up  && di > dl) swap\n                    // if (!up && di < dl) swap\n                    if ( (up  && (di > dl)) || (!up && (di < dl)) ) {\n                        dist[i] = dl; dist[l] = di;\n                        idx[i]  = il; idx[l]  = ii;\n                    }\n                }\n            }\n            __syncwarp(mask);\n        }\n    }\n}\n\n// Merge the full buffer (size k, sorted ascending) with the current intermediate top-k (sorted ascending):\n// 1. Create a bitonic sequence by taking element-wise min of (res[i], buf[k-1-i]) into buf[i].\n// 2. Sort the resulting bitonic sequence in-place (ascending) using Bitonic Sort.\n// 3. Copy back to res[], update max_distance (res[k-1]).\n// All arrays are per-warp shared-memory slices.\nstatic __device__ __forceinline__ void warp_merge_buffer_with_result(\n    float* bufDist, int* bufIdx,\n    float* resDist, int* resIdx,\n    int k,\n    float* maxDistSharedForWarp // pointer to per-warp shared max_distance\n) {\n    unsigned mask = 0xFFFFFFFFu;\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n\n    // Step 2: pairwise min to form a bitonic sequence (write into buffer)\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        int j = k - 1 - i;\n        float dr = resDist[i];\n        float db = bufDist[j];\n        int ir = resIdx[i];\n        int ib = bufIdx[j];\n        if (dr <= db) {\n            bufDist[i] = dr; bufIdx[i] = ir;\n        } else {\n            bufDist[i] = db; bufIdx[i] = ib;\n        }\n    }\n    __syncwarp(mask);\n\n    // Step 3: sort the merged bitonic sequence ascending\n    warp_bitonic_sort_asc(bufDist, bufIdx, k);\n\n    // Copy back to result arrays and update max_distance\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        resDist[i] = bufDist[i];\n        resIdx[i]  = bufIdx[i];\n    }\n    __syncwarp(mask);\n\n    if (lane == 0) {\n        *maxDistSharedForWarp = resDist[k - 1];\n    }\n    __syncwarp(mask);\n}\n\n// Perform a full merge when the candidate buffer is full (or padded at the end):\n// - Sort the buffer (ascending).\n// - Merge with intermediate top-k.\n// - Reset candidate count to 0.\nstatic __device__ __forceinline__ void warp_flush_and_merge(\n    float* bufDist, int* bufIdx,\n    float* resDist, int* resIdx,\n    int k,\n    int* candCountSharedForWarp,\n    float* maxDistSharedForWarp\n) {\n    // Sort buffer ascending\n    warp_bitonic_sort_asc(bufDist, bufIdx, k);\n    // Merge into result\n    warp_merge_buffer_with_result(bufDist, bufIdx, resDist, resIdx, k, maxDistSharedForWarp);\n    // Reset candidate buffer count\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n    if (lane == 0) {\n        *candCountSharedForWarp = 0;\n    }\n    __syncwarp();\n}\n\n// Kernel implementing warp-per-query k-NN with shared-memory tiling and buffered merging.\n// Shared memory layout:\n// [0 .. TILE_SIZE-1] float2 tile of data points\n// Followed by per-warp slices:\n//   candDist[WARPS_PER_BLOCK][k], candIdx[WARPS_PER_BLOCK][k],\n//   resDist[WARPS_PER_BLOCK][k],  resIdx[WARPS_PER_BLOCK][k],\n//   candCount[WARPS_PER_BLOCK],   maxDist[WARPS_PER_BLOCK]\n__global__ void knn_kernel(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data,  int data_count,\n    PairIF* __restrict__ result,\n    int k\n) {\n    // Identify warp and lane\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n    int warpInBlock = threadIdx.x / WARP_SIZE;\n    const unsigned fullMask = 0xFFFFFFFFu;\n\n    extern __shared__ unsigned char shared_raw[];\n    unsigned char* ptr = shared_raw;\n\n    // Shared tile of data points\n    float2* shData = reinterpret_cast<float2*>(ptr);\n    ptr += TILE_SIZE * sizeof(float2);\n\n    // Per-warp arrays (laid out consecutively to avoid pointer arrays in shared memory)\n    float* candDistBase = reinterpret_cast<float*>(ptr);                   // [WARPS_PER_BLOCK * k]\n    ptr += WARPS_PER_BLOCK * k * sizeof(float);\n    int*   candIdxBase  = reinterpret_cast<int*>(ptr);                     // [WARPS_PER_BLOCK * k]\n    ptr += WARPS_PER_BLOCK * k * sizeof(int);\n    float* resDistBase  = reinterpret_cast<float*>(ptr);                   // [WARPS_PER_BLOCK * k]\n    ptr += WARPS_PER_BLOCK * k * sizeof(float);\n    int*   resIdxBase   = reinterpret_cast<int*>(ptr);                     // [WARPS_PER_BLOCK * k]\n    ptr += WARPS_PER_BLOCK * k * sizeof(int);\n\n    int*   candCount    = reinterpret_cast<int*>(ptr);                     // [WARPS_PER_BLOCK]\n    ptr += WARPS_PER_BLOCK * sizeof(int);\n    float* maxDist      = reinterpret_cast<float*>(ptr);                   // [WARPS_PER_BLOCK]\n    // ptr += WARPS_PER_BLOCK * sizeof(float); // End of shared allocation\n\n    // Grid-stride over queries in groups of WARPS_PER_BLOCK per block to safely use __syncthreads for tile loads.\n    for (int qStart = blockIdx.x * WARPS_PER_BLOCK; qStart < query_count; qStart += gridDim.x * WARPS_PER_BLOCK) {\n        int qIdx = qStart + warpInBlock;\n\n        // Per-warp shared-memory slices\n        float* bufDist = candDistBase + warpInBlock * k;\n        int*   bufIdx  = candIdxBase  + warpInBlock * k;\n        float* resDist = resDistBase  + warpInBlock * k;\n        int*   resIdx  = resIdxBase   + warpInBlock * k;\n        int*   myCandCount = candCount + warpInBlock;\n        float* myMaxDist   = maxDist   + warpInBlock;\n\n        // Initialize per-warp state if this warp has a valid query\n        float2 q = make_float2(0.f, 0.f);\n        if (qIdx < query_count) {\n            q = query[qIdx];\n            // Initialize intermediate result arrays to +inf distances and -1 indices\n            for (int i = lane; i < k; i += WARP_SIZE) {\n                resDist[i] = FLT_MAX;\n                resIdx[i]  = -1;\n            }\n            if (lane == 0) {\n                *myCandCount = 0;\n                *myMaxDist   = FLT_MAX; // No filtering until the first merge\n            }\n        }\n        __syncwarp(fullMask);\n\n        // Process data in shared-memory tiles\n        for (int base = 0; base < data_count; base += TILE_SIZE) {\n            int tileLen = data_count - base;\n            if (tileLen > TILE_SIZE) tileLen = TILE_SIZE;\n\n            // Entire block loads the current tile into shared memory\n            __syncthreads();\n            for (int t = threadIdx.x; t < tileLen; t += blockDim.x) {\n                shData[t] = data[base + t];\n            }\n            __syncthreads();\n\n            if (qIdx < query_count) {\n                // For each point in the tile, compute distance and push candidate if closer than max_distance\n                for (int t = lane; t < tileLen; t += WARP_SIZE) {\n                    float d2 = sqdist2(q, shData[t]);\n\n                    // Load current max_distance for this warp (may be updated after merges)\n                    float currMax = *myMaxDist;\n                    // Determine which threads have candidates\n                    int pred = (d2 < currMax) ? 1 : 0;\n                    unsigned mask = __ballot_sync(fullMask, pred);\n\n                    if (mask) {\n                        int n = __popc(mask);\n                        // Warp-aggregated atomicAdd: lane 0 reserves 'n' slots in the candidate buffer count\n                        int prior = 0;\n                        if (lane == 0) {\n                            prior = atomicAdd(myCandCount, n);\n                        }\n                        prior = __shfl_sync(fullMask, prior, 0);\n\n                        // Compute per-thread rank among the active lanes\n                        int rank = __popc(mask & ((1u << lane) - 1));\n\n                        // Determine how many slots are actually available (to avoid writing past k)\n                        int granted = 0;\n                        if (lane == 0) {\n                            int space = k - prior;\n                            if (space < 0) space = 0;\n                            granted = (n < space) ? n : space;\n                        }\n                        granted = __shfl_sync(fullMask, granted, 0);\n\n                        // Write granted candidates to buffer\n                        if (pred && (rank < granted)) {\n                            int pos = prior + rank;\n                            bufDist[pos] = d2;\n                            bufIdx[pos]  = base + t; // global data index\n                        }\n\n                        // If buffer is (logically) full, perform flush+merge.\n                        // Note: myCandCount may exceed k due to reservation for 'n' even if not all were written.\n                        int needFlush = ((prior + n) >= k) ? 1 : 0;\n                        if (needFlush) {\n                            __syncwarp(fullMask);\n                            // Pad any unwritten entries (if any) with +inf to ensure we have k valid entries to sort/merge.\n                            // Already ensured by 'granted' limiting actual writes; we rely on old contents for remaining slots,\n                            // but to be robust, explicitly fill any remaining slots when 'prior + granted < k'.\n                            for (int i = lane + prior + granted; i < k; i += WARP_SIZE) {\n                                bufDist[i] = FLT_MAX;\n                                bufIdx[i]  = -1;\n                            }\n                            __syncwarp(fullMask);\n\n                            // Sort, merge, reset count, update max_distance\n                            warp_flush_and_merge(bufDist, bufIdx, resDist, resIdx, k, myCandCount, myMaxDist);\n\n                            // Reattempt insertion for leftover threads from this iteration (those not granted)\n                            int leftoverPred = (pred && (rank >= granted)) ? 1 : 0;\n                            unsigned leftoverMask = __ballot_sync(fullMask, leftoverPred);\n                            int leftoverN = __popc(leftoverMask);\n                            if (leftoverN) {\n                                int prior2 = 0;\n                                if (lane == 0) {\n                                    prior2 = atomicAdd(myCandCount, leftoverN);\n                                }\n                                prior2 = __shfl_sync(fullMask, prior2, 0);\n                                int rank2 = __popc(leftoverMask & ((1u << lane) - 1));\n                                int granted2 = 0;\n                                if (lane == 0) {\n                                    int space2 = k - prior2;\n                                    if (space2 < 0) space2 = 0;\n                                    granted2 = (leftoverN < space2) ? leftoverN : space2;\n                                }\n                                granted2 = __shfl_sync(fullMask, granted2, 0);\n\n                                if (leftoverPred && (rank2 < granted2)) {\n                                    int pos2 = prior2 + rank2;\n                                    bufDist[pos2] = d2;\n                                    bufIdx[pos2]  = base + t;\n                                }\n\n                                // If we filled the buffer again, flush and merge once more\n                                int needFlush2 = ((prior2 + leftoverN) >= k) ? 1 : 0;\n                                if (needFlush2) {\n                                    __syncwarp(fullMask);\n                                    for (int i = lane + prior2 + granted2; i < k; i += WARP_SIZE) {\n                                        bufDist[i] = FLT_MAX;\n                                        bufIdx[i]  = -1;\n                                    }\n                                    __syncwarp(fullMask);\n                                    warp_flush_and_merge(bufDist, bufIdx, resDist, resIdx, k, myCandCount, myMaxDist);\n                                }\n                            }\n                        }\n                    }\n                } // end tile points loop\n            } // end if valid query\n\n            __syncthreads();\n        } // end for all tiles\n\n        // After all tiles, if buffer has any remaining candidates, pad and merge.\n        if (qIdx < query_count) {\n            int count = (lane == 0) ? *myCandCount : 0;\n            count = __shfl_sync(fullMask, count, 0);\n            if (count > 0) {\n                // Pad to k with +inf\n                for (int i = lane + count; i < k; i += WARP_SIZE) {\n                    bufDist[i] = FLT_MAX;\n                    bufIdx[i]  = -1;\n                }\n                __syncwarp(fullMask);\n                // Sort and merge the (possibly partially filled) buffer\n                warp_flush_and_merge(bufDist, bufIdx, resDist, resIdx, k, myCandCount, myMaxDist);\n            }\n\n            // Write final sorted results (ascending distances) to global memory\n            PairIF* out = result + static_cast<size_t>(qIdx) * static_cast<size_t>(k);\n            for (int i = lane; i < k; i += WARP_SIZE) {\n                out[i].first  = resIdx[i];\n                out[i].second = resDist[i];\n            }\n        }\n\n        __syncthreads();\n    } // end query groups\n}\n\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    // Kernel launch configuration\n    const int warpsPerBlock = WARPS_PER_BLOCK;\n    const int threadsPerBlock = warpsPerBlock * WARP_SIZE;\n    const int blocks = (query_count + warpsPerBlock - 1) / warpsPerBlock;\n\n    // Dynamic shared memory size calculation:\n    // - TILE_SIZE float2s for data tile\n    // - For each warp:\n    //    candDist[k] + candIdx[k] + resDist[k] + resIdx[k] => (2*k*sizeof(float) + 2*k*sizeof(int))\n    //    candCount[1] + maxDist[1] => (sizeof(int) + sizeof(float))\n    size_t perWarpBytes = 2ull * k * sizeof(float) + 2ull * k * sizeof(int) + sizeof(int) + sizeof(float);\n    size_t sharedBytes = TILE_SIZE * sizeof(float2) + static_cast<size_t>(warpsPerBlock) * perWarpBytes;\n\n    // Launch kernel. Reinterpret std::pair pointer to POD PairIF pointer for device access.\n    PairIF* out = reinterpret_cast<PairIF*>(result);\n    knn_kernel<<<blocks, threadsPerBlock, sharedBytes>>>(query, query_count, data, data_count, out, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1203, "completion_tokens": 21379, "total_tokens": 22582, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 17280, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db8cc1481908e9ee7b25cfd2863", "custom_id": "knn07-03", "response": {"status_code": 200, "request_id": "1aa736923a71fd8b78e8babbb157676d", "body": {"id": "chatcmpl-C4ssHHHFgAkNf4PiaP74JMd3jJ6r9", "object": "chat.completion", "created": 1755280285, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// This implementation assigns one warp (32 threads) per query and processes the data in shared-memory tiles.\n// For each warp/query, we keep two per-warp shared-memory arrays of length k:\n//   - intermediate result (sorted ascending): distances and indices\n//   - candidate buffer (unsorted, up to k elements) with a shared atomic counter\n// We also keep per-warp max_distance (distance of current k-th nearest neighbor).\n// The algorithm iterates over the data in tiles cached in shared memory. Each lane computes distances to its\n// assigned points, filters them by max_distance, and tries to insert them into the warp's candidate buffer using\n// atomicAdd to get an insertion position. When the buffer is full (or at the end of the data), the warp:\n//   1) Sorts the candidate buffer using Bitonic Sort (ascending).\n//   2) Forms a bitonic-merged result of size k by taking pairwise minima of the candidate buffer (ascending)\n//      and the intermediate result reversed (descending), i.e., out[i] = min(cand[i], inter[k-1-i]).\n//   3) Sorts that merged result (a bitonic sequence) again using Bitonic Sort to produce the updated intermediate.\n// After processing all tiles and performing a final flush, results are written back in row-major order.\n//\n// Notes on design:\n// - k is guaranteed to be a power of two between 32 and 1024, inclusive.\n// - We keep per-warp \"private\" buffers in shared memory (distinct regions per warp).\n// - We use warp-synchronous programming: __syncwarp() within warp tasks and __syncthreads() for block-wide tasks.\n// - We use a fixed number of warps per block (WARPS_PER_BLOCK = 4) and a fixed tile size (TILE_POINTS = 4096),\n//   which fits comfortably in the shared memory budgets of A100/H100 even for k=1024.\n// - The bitonic sort is parallelized across 32 threads of a warp by mapping compare-and-swap work over indices.\n//\n// Result writing:\n// - The output array is std::pair<int, float>, allocated with cudaMalloc. On device we reinterpret it as a simple\n//   POD struct with identical layout, to safely write (first=index, second=distance).\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Number of warps per block. 4 warps per block -> 128 threads per block.\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 4\n#endif\n\n// Number of data points loaded per tile into shared memory. 4096 points = 32KB of shared memory for float2.\n#ifndef TILE_POINTS\n#define TILE_POINTS 4096\n#endif\n\n// POD struct to mirror std::pair<int,float> layout on device.\nstruct PairIF {\n    int   first;\n    float second;\n};\n\n// Utility to compute the minimum of two (distance, index) pairs by distance.\n__device__ __forceinline__ void min_pair(float da, int ia, float db, int ib, float &do_, int &io_) {\n    if (da < db) {\n        do_ = da; io_ = ia;\n    } else {\n        do_ = db; io_ = ib;\n    }\n}\n\n// Bitonic sort (ascending) on shared arrays of length n (power of two), executed by one warp.\n// All 32 threads in the warp cooperate. Uses shared memory arrays dist[] and idx[].\n__device__ __forceinline__ void bitonic_sort_warp(float *dist, int *idx, int n) {\n    // Warp-wide sort with compare-and-swap pattern; the \"n\" is power-of-two.\n    // Two nested loops follow the standard bitonic sort network:\n    // - size: current size of subsequences to be merged (2,4,8,...,n)\n    // - stride: distance between compared elements (size/2, size/4, ..., 1)\n    // Threads distribute the i indices (0..n-1) in steps of warpSize.\n    const unsigned mask = 0xFFFFFFFFu;\n    for (int size = 2; size <= n; size <<= 1) {\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            // Each lane processes indices i in [lane, n) with step WARP_SIZE\n            int lane = threadIdx.x & (WARP_SIZE - 1);\n            for (int i = lane; i < n; i += WARP_SIZE) {\n                int l = i ^ stride;\n                if (l > i) {\n                    bool up = ((i & size) == 0); // ascending in this subsequence\n                    float di = dist[i];\n                    float dl = dist[l];\n                    int   ii = idx[i];\n                    int   il = idx[l];\n                    bool cond = (up ? (di > dl) : (di < dl));\n                    if (cond) {\n                        // swap\n                        dist[i] = dl; dist[l] = di;\n                        idx[i]  = il; idx[l]  = ii;\n                    }\n                }\n            }\n            __syncwarp(mask);\n        }\n    }\n}\n\n// Kernel implementing k-NN for 2D points as described above.\n__global__ void knn_kernel(const float2 * __restrict__ query,\n                           int query_count,\n                           const float2 * __restrict__ data,\n                           int data_count,\n                           PairIF * __restrict__ result,\n                           int k)\n{\n    // Block configuration and warp/lane indices.\n    const int warp_id_in_block = threadIdx.x / WARP_SIZE;\n    const int lane_id          = threadIdx.x & (WARP_SIZE - 1);\n    const int warps_per_block  = blockDim.x / WARP_SIZE;\n    const int warp_global_id   = blockIdx.x * warps_per_block + warp_id_in_block;\n    const bool active_warp     = (warp_global_id < query_count);\n\n    // Shared memory layout:\n    // [0]             float2 tile_points[TILE_POINTS]\n    // [tile_bytes]    float inter_d[warps_per_block][k]\n    // [...          ] int   inter_i[warps_per_block][k]\n    // [...          ] float cand_d [warps_per_block][k]\n    // [...          ] int   cand_i [warps_per_block][k]\n    // [...          ] int   cand_count[warps_per_block]\n    // [...          ] float max_d    [warps_per_block]\n    extern __shared__ unsigned char smem[];\n    size_t offset = 0;\n\n    // Tile of data points cached by the entire block.\n    float2 *tile_points = reinterpret_cast<float2*>(smem + offset);\n    const size_t tile_bytes = sizeof(float2) * TILE_POINTS;\n    offset += tile_bytes;\n\n    // Align the offset to 8 bytes for good measure\n    offset = (offset + 7) & ~size_t(7);\n\n    // Per-warp arrays: inter distances\n    float *s_inter_d = reinterpret_cast<float*>(smem + offset);\n    offset += sizeof(float) * size_t(warps_per_block) * size_t(k);\n\n    // Per-warp arrays: inter indices\n    int *s_inter_i = reinterpret_cast<int*>(smem + offset);\n    offset += sizeof(int) * size_t(warps_per_block) * size_t(k);\n\n    // Per-warp arrays: candidate distances\n    float *s_cand_d = reinterpret_cast<float*>(smem + offset);\n    offset += sizeof(float) * size_t(warps_per_block) * size_t(k);\n\n    // Per-warp arrays: candidate indices\n    int *s_cand_i = reinterpret_cast<int*>(smem + offset);\n    offset += sizeof(int) * size_t(warps_per_block) * size_t(k);\n\n    // Per-warp atomic counters: candidate counts\n    int *s_cand_count = reinterpret_cast<int*>(smem + offset);\n    offset += sizeof(int) * size_t(warps_per_block);\n\n    // Per-warp max distances\n    float *s_max_d = reinterpret_cast<float*>(smem + offset);\n    offset += sizeof(float) * size_t(warps_per_block);\n\n    // Warp-local base offsets for its shared arrays\n    const int warp_base = warp_id_in_block * k;\n\n    // Initialize per-warp state: intermediate result distances to +inf, indices to -1, cand_count=0, max_d=+inf.\n    if (active_warp) {\n        for (int pos = lane_id; pos < k; pos += WARP_SIZE) {\n            s_inter_d[warp_base + pos] = CUDART_INF_F;\n            s_inter_i[warp_base + pos] = -1;\n        }\n    }\n    if (lane_id == 0) {\n        s_cand_count[warp_id_in_block] = 0;\n        s_max_d[warp_id_in_block] = CUDART_INF_F;\n    }\n    __syncthreads();\n\n    // Helper lambda to flush the candidate buffer into the intermediate result for this warp.\n    auto flush_candidates = [&]() {\n        __syncwarp();\n\n        // Fill any unused candidate slots with +inf so we can sort exactly k elements.\n        if (active_warp) {\n            int cnt = s_cand_count[warp_id_in_block];\n            // Cap cnt to at most k; entries >= k are ignored.\n            if (cnt > k) cnt = k;\n            // Initialize distances/indices for slots [cnt, k) to +inf/-1\n            for (int pos = lane_id + cnt; pos < k; pos += WARP_SIZE) {\n                s_cand_d[warp_base + pos] = CUDART_INF_F;\n                s_cand_i[warp_base + pos] = -1;\n            }\n        }\n        __syncwarp();\n\n        if (active_warp) {\n            // 1) Sort the candidate buffer (ascending).\n            bitonic_sort_warp(&s_cand_d[warp_base], &s_cand_i[warp_base], k);\n\n            // 2) Merge with the current intermediate result by taking pairwise minima of:\n            //    - s_cand_d[i] (ascending)\n            //    - s_inter_d[k-1-i] (descending)\n            //    Store the minima back into the candidate buffer.\n            for (int i = lane_id; i < k; i += WARP_SIZE) {\n                int j = k - 1 - i;\n                float out_d; int out_i;\n                min_pair(s_cand_d[warp_base + i], s_cand_i[warp_base + i],\n                         s_inter_d[warp_base + j], s_inter_i[warp_base + j],\n                         out_d, out_i);\n                s_cand_d[warp_base + i] = out_d;\n                s_cand_i[warp_base + i] = out_i;\n            }\n            __syncwarp();\n\n            // 3) Sort the merged result (bitonic sequence) ascending to get updated intermediate.\n            bitonic_sort_warp(&s_cand_d[warp_base], &s_cand_i[warp_base], k);\n\n            // Copy merged result into the intermediate arrays and update max_d (k-th neighbor distance).\n            for (int pos = lane_id; pos < k; pos += WARP_SIZE) {\n                s_inter_d[warp_base + pos] = s_cand_d[warp_base + pos];\n                s_inter_i[warp_base + pos] = s_cand_i[warp_base + pos];\n            }\n            __syncwarp();\n            if (lane_id == 0) {\n                s_cand_count[warp_id_in_block] = 0;\n                s_max_d[warp_id_in_block] = s_inter_d[warp_base + (k - 1)];\n            }\n        }\n        __syncwarp();\n    };\n\n    // Main loop: process data points in shared-memory tiles.\n    const unsigned full_mask = 0xFFFFFFFFu;\n    for (int tile_start = 0; tile_start < data_count; tile_start += TILE_POINTS) {\n        int tile_count = data_count - tile_start;\n        if (tile_count > TILE_POINTS) tile_count = TILE_POINTS;\n\n        // Load the tile into shared memory cooperatively by the entire block.\n        for (int t = threadIdx.x; t < tile_count; t += blockDim.x) {\n            tile_points[t] = data[tile_start + t];\n        }\n        __syncthreads();\n\n        // Broadcast the query point to all lanes in the warp.\n        float qx = 0.0f, qy = 0.0f;\n        if (active_warp) {\n            if (lane_id == 0) {\n                float2 q = query[warp_global_id];\n                qx = q.x;\n                qy = q.y;\n            }\n            qx = __shfl_sync(full_mask, qx, 0);\n            qy = __shfl_sync(full_mask, qy, 0);\n        }\n\n        // Process the tile: each lane works on indices t = it*WARP_SIZE + lane_id (warp-synchronous loop).\n        int iters = (tile_count + WARP_SIZE - 1) / WARP_SIZE;\n        for (int it = 0; it < iters; ++it) {\n            bool request_flush = false;\n            if (active_warp) {\n                int t = it * WARP_SIZE + lane_id;\n                if (t < tile_count) {\n                    float2 p = tile_points[t];\n                    float dx = p.x - qx;\n                    float dy = p.y - qy;\n                    float dist = dx * dx + dy * dy;\n\n                    float maxd = s_max_d[warp_id_in_block];\n                    if (dist < maxd) {\n                        int pos = atomicAdd(&s_cand_count[warp_id_in_block], 1);\n                        if (pos < k) {\n                            s_cand_d[warp_base + pos] = dist;\n                            s_cand_i[warp_base + pos] = tile_start + t;\n                            // If we just filled the k-th slot (pos == k-1), request a flush.\n                            if (pos + 1 == k) request_flush = true;\n                        } else {\n                            // Buffer overflow: request flush; this candidate is dropped.\n                            request_flush = true;\n                        }\n                    }\n                }\n            }\n            // If any lane in the warp requested a flush, perform it now.\n            unsigned mask = __ballot_sync(full_mask, request_flush);\n            if (mask) {\n                flush_candidates();\n            }\n        }\n        __syncthreads();\n    }\n\n    // Final flush after all tiles: merge any remaining candidates.\n    if (active_warp) {\n        // All lanes must see the same condition/value for synchronization.\n        int cnt = s_cand_count[warp_id_in_block];\n        unsigned mask = __ballot_sync(full_mask, cnt > 0);\n        if (mask && (lane_id == 0 || true)) {\n            // All lanes enter the flush function to keep warp-synchronous semantics.\n            flush_candidates();\n        }\n    }\n    __syncthreads();\n\n    // Write the final top-k results for this query in ascending order.\n    if (active_warp) {\n        // Store to global memory in row-major: result[qid * k + j] = {index, distance}\n        int out_base = warp_global_id * k;\n        for (int pos = lane_id; pos < k; pos += WARP_SIZE) {\n            PairIF out;\n            out.first  = s_inter_i[warp_base + pos];\n            out.second = s_inter_d[warp_base + pos];\n            result[out_base + pos] = out;\n        }\n    }\n}\n\n// Host API: run_knn\n// This function launches the kernel with an appropriate configuration for the target GPU.\n// - query: pointer to query_count float2 points\n// - data: pointer to data_count float2 points\n// - result: pointer to result array of length query_count * k (std::pair<int,float>)\n// - k: number of neighbors (power of two, 32..1024)\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k)\n{\n    // Kernel launch configuration.\n    const int warps_per_block = WARPS_PER_BLOCK;\n    const int threads_per_block = warps_per_block * WARP_SIZE;\n    const int blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Compute dynamic shared memory size:\n    // - TILE_POINTS * sizeof(float2) for the data tile\n    // - per-warp: 2 arrays (inter) of k*(float + int) + 2 arrays (cand) of k*(float + int)\n    // - per-warp: 1 int (cand_count) + 1 float (max_d)\n    size_t tile_bytes = sizeof(float2) * size_t(TILE_POINTS);\n    size_t per_warp_bytes = (size_t)k * (sizeof(float) + sizeof(int)) * 2  // inter\n                          + (size_t)k * (sizeof(float) + sizeof(int)) * 2  // cand\n                          + sizeof(int) + sizeof(float);                   // counters\n    size_t shared_bytes = tile_bytes + per_warp_bytes * warps_per_block;\n\n    // Launch kernel. Reinterpret result pointer as POD PairIF.\n    PairIF *result_if = reinterpret_cast<PairIF*>(result);\n    knn_kernel<<<blocks, threads_per_block, shared_bytes>>>(query, query_count, data, data_count, result_if, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1203, "completion_tokens": 15100, "total_tokens": 16303, "prompt_tokens_details": {"cached_tokens": 1152, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 11328, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7dbb55088190bebda62b83238e53", "custom_id": "knn07-04", "response": {"status_code": 200, "request_id": "1388e0012e414398d7333f62a7ffd7be", "body": {"id": "chatcmpl-C4ssXau21C2waPjTeYiA2eZrx4RhG", "object": "chat.completion", "created": 1755280301, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// This implementation performs k-NN for 2D points on the GPU.\n// - One warp (32 threads) handles one query point.\n// - Input data points are processed in shared-memory batches (\"tiles\") loaded by the entire block.\n// - Each warp maintains:\n//   * A private intermediate result of k nearest neighbors (sorted ascending by distance) in shared memory.\n//   * A candidate buffer (size k) in shared memory with a shared counter updated via atomicAdd.\n//   * A \"max_distance\" shared variable storing the current k-th (largest) distance in the intermediate result.\n// - When the warp's candidate buffer becomes full, it is sorted (Bitonic sort), merged with the intermediate result\n//   (into a bitonic sequence using pairwise minima), and sorted again (Bitonic sort) to update the intermediate result.\n// - After processing all tiles, any remaining candidates in the buffer are merged in the same way.\n// - Distances are squared Euclidean distances; ties are resolved arbitrarily.\n//\n// Design choices:\n// - Threads per block: 128 (4 warps). This balances shared memory usage (for k up to 1024) and occupancy on A100/H100.\n// - Tile size (data points cached per block in shared memory): 4096 points (32 KB). Combined with per-warp buffers of\n//   3*k pairs (intermediate, buffer, merged), this fits within typical shared memory limits for A100/H100 when k <= 1024.\n// - Bitonic sort is implemented in parallel across the 32 lanes of the warp, operating on shared memory with warp-scope\n//   synchronization via __syncwarp().\n//\n// Notes on correctness and synchronization:\n// - The candidate buffer's count is always updated using atomicAdd as required. Appends are performed warp-cooperatively:\n//   the warp aggregates accepted candidates for a 32-point group, reserves space with a single atomicAdd, writes those\n//   that fit, flushes when needed, and retries any leftovers after the flush. This ensures no candidate is lost.\n// - When flushing, the buffer is sorted (size k), merged with the current intermediate result, and the merged result is\n//   sorted to produce the updated intermediate result. max_distance is updated accordingly.\n\n#ifndef KNN_THREADS_PER_BLOCK\n#define KNN_THREADS_PER_BLOCK 128  // 4 warps per block\n#endif\n\n#ifndef KNN_TILE_POINTS\n#define KNN_TILE_POINTS 4096       // points per shared-memory tile (float2 => 8 bytes each; 4096 -> 32KB)\n#endif\n\n// Simple POD pair (index, distance) used on the device side.\n// This has the same memory footprint as std::pair<int,float> (8 bytes).\nstruct PairIF {\n    int   first;   // index in the data array\n    float second;  // squared distance\n};\n\n// Internal candidate representation for sorts/merges.\nstruct Candidate {\n    float dist;\n    int   idx;\n};\n\nstatic __device__ __forceinline__ Candidate make_cand(float d, int i) {\n    Candidate c; c.dist = d; c.idx = i; return c;\n}\n\n// Warp-parallel Bitonic Sort (ascending) on shared-memory array 'arr' of size 'n' (power of two).\n// - The 32 lanes cooperatively execute the compare-and-swap network.\n// - Each lane handles indices i = lane, lane+32, lane+64, ... looping across the array.\n// - Synchronization between stages is via __syncwarp(warp_mask).\nstatic __device__ __forceinline__ void warp_bitonic_sort_asc(Candidate* arr, int n, unsigned warp_mask) {\n    // Bitonic sort network as per the given pseudocode, parallelized by distributing 'i' across lanes.\n    for (int k = 2; k <= n; k <<= 1) {\n        for (int j = k >> 1; j > 0; j >>= 1) {\n            // Each thread processes multiple i values, striding by 32.\n            int lane = threadIdx.x & 31;\n            for (int i = lane; i < n; i += 32) {\n                int l = i ^ j;\n                if (l > i) {\n                    bool ascending = ((i & k) == 0);\n                    Candidate ai = arr[i];\n                    Candidate al = arr[l];\n                    // Compare by distance; swap according to bitonic ordering rule.\n                    bool cmp = (ascending ? (ai.dist > al.dist) : (ai.dist < al.dist));\n                    if (cmp) {\n                        arr[i] = al;\n                        arr[l] = ai;\n                    }\n                }\n            }\n            __syncwarp(warp_mask);\n        }\n    }\n}\n\n// Merge buffer and intermediate into 'merged' via pairwise minima to form a bitonic sequence, then sort.\n// Precondition:\n// - 'inter' is sorted ascending (invariant).\n// - 'buf' is sorted ascending.\n// - 'merged' is a temporary array (size k).\n// Postcondition:\n// - 'inter' updated to the sorted ascending top-k result from buf \u222a inter.\n// - 'max_dist' updated to inter[k-1].dist.\n// - 'buf_count' reset to 0.\nstatic __device__ __forceinline__ void warp_merge_flush(\n    Candidate* inter, Candidate* buf, Candidate* merged,\n    int k, int lane, unsigned warp_mask,\n    volatile int* buf_count_ptr, volatile float* max_dist_ptr)\n{\n    int buf_count = *buf_count_ptr;\n    if (buf_count <= 0) return;\n\n    // If buffer is not full, pad with +INF so that Bitonic Sort (size k) is valid.\n    if (buf_count < k) {\n        for (int i = lane + buf_count; i < k; i += 32) {\n            buf[i] = make_cand(CUDART_INF_F, -1);\n        }\n        __syncwarp(warp_mask);\n    }\n\n    // 1) Sort the buffer ascending.\n    warp_bitonic_sort_asc(buf, k, warp_mask);\n\n    // 2) Merge buffer and intermediate via pairwise minima to produce a bitonic sequence.\n    for (int i = lane; i < k; i += 32) {\n        Candidate a = buf[i];\n        Candidate b = inter[k - 1 - i];\n        merged[i] = (a.dist <= b.dist) ? a : b;\n    }\n    __syncwarp(warp_mask);\n\n    // 3) Sort the merged result ascending.\n    warp_bitonic_sort_asc(merged, k, warp_mask);\n\n    // Update the intermediate result and max distance.\n    for (int i = lane; i < k; i += 32) {\n        inter[i] = merged[i];\n    }\n    __syncwarp(warp_mask);\n    if (lane == 0) {\n        *max_dist_ptr = inter[k - 1].dist;\n        *buf_count_ptr = 0;\n    }\n    __syncwarp(warp_mask);\n}\n\n// Kernel implementing k-NN using one warp per query.\n__global__ void knn_kernel(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data,  int data_count,\n    PairIF* __restrict__ result,\n    int k)\n{\n    constexpr int WARPS_PER_BLOCK = KNN_THREADS_PER_BLOCK / 32;\n    const int lane        = threadIdx.x & 31;\n    const int warp_local  = threadIdx.x >> 5; // warp index within block\n    const int warp_global = blockIdx.x * WARPS_PER_BLOCK + warp_local;\n\n    // Shared memory layout (dynamic):\n    // [tile_data (KNN_TILE_POINTS float2)] [inter_all (WARPS*k Candidates)] [buf_all (WARPS*k)] [merged_all (WARPS*k)]\n    extern __shared__ unsigned char smem[];\n    unsigned char* smem_ptr = smem;\n    size_t offset = 0;\n\n    // Tile buffer shared by the entire block\n    float2* tile_data = reinterpret_cast<float2*>(smem_ptr + offset);\n    offset += static_cast<size_t>(KNN_TILE_POINTS) * sizeof(float2);\n    // Align to 8-byte boundary\n    offset = (offset + 7) & ~static_cast<size_t>(7);\n\n    // Per-warp arrays\n    Candidate* inter_all  = reinterpret_cast<Candidate*>(smem_ptr + offset);\n    offset += static_cast<size_t>(WARPS_PER_BLOCK) * static_cast<size_t>(k) * sizeof(Candidate);\n    Candidate* buf_all    = reinterpret_cast<Candidate*>(smem_ptr + offset);\n    offset += static_cast<size_t>(WARPS_PER_BLOCK) * static_cast<size_t>(k) * sizeof(Candidate);\n    Candidate* merged_all = reinterpret_cast<Candidate*>(smem_ptr + offset);\n    offset += static_cast<size_t>(WARPS_PER_BLOCK) * static_cast<size_t>(k) * sizeof(Candidate);\n\n    Candidate* inter  = inter_all  + warp_local * k;\n    Candidate* buf    = buf_all    + warp_local * k;\n    Candidate* merged = merged_all + warp_local * k;\n\n    // Shared control variables per warp\n    __shared__ int   s_buf_count[WARPS_PER_BLOCK];\n    __shared__ float s_max_dist[WARPS_PER_BLOCK];\n\n    // Initialize per-warp state\n    if (lane == 0) {\n        s_buf_count[warp_local] = 0;\n        s_max_dist[warp_local]  = CUDART_INF_F;\n    }\n    // Initialize the intermediate result to +INF (sorted by construction)\n    for (int i = lane; i < k; i += 32) {\n        inter[i] = make_cand(CUDART_INF_F, -1);\n    }\n    __syncwarp();\n\n    // Warp mask for synchronization among active lanes in the warp\n    const unsigned warp_mask = 0xffffffffu;\n\n    // Preload the query point for this warp (if it corresponds to a valid query)\n    float qx = 0.0f, qy = 0.0f;\n    if (warp_global < query_count) {\n        float2 q;\n        if (lane == 0) q = query[warp_global];\n        qx = __shfl_sync(warp_mask, q.x, 0);\n        qy = __shfl_sync(warp_mask, q.y, 0);\n    }\n\n    // Process data in tiles loaded into shared memory\n    for (int tile_base = 0; tile_base < data_count; tile_base += KNN_TILE_POINTS) {\n        const int tile_count = min(KNN_TILE_POINTS, data_count - tile_base);\n\n        // Load the tile cooperatively by all threads in the block\n        for (int idx = threadIdx.x; idx < tile_count; idx += blockDim.x) {\n            tile_data[idx] = data[tile_base + idx];\n        }\n        __syncthreads();\n\n        if (warp_global < query_count) {\n            // Iterate over the tile in warp-sized groups\n            for (int t = 0; t < tile_count; t += 32) {\n                // Each lane computes at most one candidate in this group\n                const int local_idx = t + lane;\n                float dist = CUDART_INF_F;\n                int   gidx = -1;\n                if (local_idx < tile_count) {\n                    float2 p = tile_data[local_idx];\n                    float dx = p.x - qx;\n                    float dy = p.y - qy;\n                    dist = dx * dx + dy * dy;\n                    gidx = tile_base + local_idx;\n                }\n\n                // Check against current max distance to filter\n                const float maxd = s_max_dist[warp_local];\n                const bool  valid = (local_idx < tile_count) && (dist < maxd);\n                unsigned    mask  = __ballot_sync(warp_mask, valid);\n                if (mask == 0) continue; // no candidates in this group\n\n                int num_valid = __popc(mask);\n\n                // Reserve space in the candidate buffer with a single atomicAdd\n                int base = 0;\n                if (lane == 0) {\n                    base = atomicAdd(&s_buf_count[warp_local], num_valid);\n                }\n                base = __shfl_sync(warp_mask, base, 0);\n                int space_left = max(0, k - base);\n                // Compute per-lane rank among valid lanes\n                unsigned lt_mask = (lane == 0) ? 0u : (0xffffffffu >> (32 - lane));\n                int rank = __popc(mask & lt_mask);\n\n                // Write the portion that fits\n                bool will_write = valid && (rank < space_left);\n                if (will_write) {\n                    int pos = base + rank;\n                    buf[pos] = make_cand(dist, gidx);\n                }\n                __syncwarp(warp_mask);\n\n                // If overflow occurred, flush buffer and retry leftovers\n                if (num_valid > space_left) {\n                    // Cap buffer count at k before flushing\n                    if (lane == 0) {\n                        s_buf_count[warp_local] = k;\n                    }\n                    __syncwarp(warp_mask);\n\n                    // Flush (sort buffer, merge, sort merged)\n                    warp_merge_flush(inter, buf, merged, k, lane, warp_mask,\n                                     &s_buf_count[warp_local], &s_max_dist[warp_local]);\n\n                    // Retry leftovers (those with rank >= space_left) against updated max distance\n                    const float new_maxd = s_max_dist[warp_local];\n                    bool leftover = valid && (rank >= space_left) && (dist < new_maxd);\n                    unsigned mask2 = __ballot_sync(warp_mask, leftover);\n                    int num2 = __popc(mask2);\n                    if (num2 > 0) {\n                        int base2 = 0;\n                        if (lane == 0) {\n                            base2 = atomicAdd(&s_buf_count[warp_local], num2);\n                        }\n                        base2 = __shfl_sync(warp_mask, base2, 0);\n                        unsigned lt_mask2 = (lane == 0) ? 0u : (0xffffffffu >> (32 - lane));\n                        int rank2 = __popc(mask2 & lt_mask2);\n                        if (leftover) {\n                            int pos2 = base2 + rank2;\n                            // This always fits since num2 <= 32 and k >= 32 and buffer was empty after flush.\n                            buf[pos2] = make_cand(dist, gidx);\n                        }\n                        __syncwarp(warp_mask);\n                    }\n                }\n            }\n        }\n\n        __syncthreads(); // Ensure all warps are done with this tile before loading the next one\n    }\n\n    // Final flush for any remaining candidates\n    if (warp_global < query_count) {\n        int count = s_buf_count[warp_local];\n        if (count > 0) {\n            // Pad and flush merge\n            warp_merge_flush(inter, buf, merged, k, lane, warp_mask,\n                             &s_buf_count[warp_local], &s_max_dist[warp_local]);\n        }\n\n        // Write out the final top-k results for this query\n        for (int i = lane; i < k; i += 32) {\n            Candidate c = inter[i];\n            result[warp_global * k + i] = PairIF{ c.idx, c.dist };\n        }\n    }\n}\n\n// Host-side launcher matching the requested interface.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data,  int data_count,\n             std::pair<int, float> *result, int k)\n{\n    // Configuration\n    constexpr int threads_per_block = KNN_THREADS_PER_BLOCK;\n    constexpr int warps_per_block   = threads_per_block / 32;\n    const int total_warps           = query_count;\n    const int blocks                = (total_warps + warps_per_block - 1) / warps_per_block;\n\n    // Dynamic shared memory size:\n    // tile_data + per-warp (inter + buf + merged), each of size k Candidates (8 bytes each)\n    const size_t smem_tile_bytes    = static_cast<size_t>(KNN_TILE_POINTS) * sizeof(float2);\n    const size_t smem_perwarp_bytes = static_cast<size_t>(k) * sizeof(Candidate) * 3u;\n    const size_t smem_bytes         = smem_tile_bytes + static_cast<size_t>(warps_per_block) * smem_perwarp_bytes;\n\n    // Reinterpret the result pointer to a POD pair type with the same memory layout.\n    PairIF* result_alias = reinterpret_cast<PairIF*>(result);\n\n    // Launch kernel\n    knn_kernel<<<blocks, threads_per_block, smem_bytes>>>(\n        query, query_count, data, data_count, result_alias, k\n    );\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1203, "completion_tokens": 19191, "total_tokens": 20394, "prompt_tokens_details": {"cached_tokens": 1152, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 15616, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db9a1088190a51f490911c7979f", "custom_id": "knn07-05", "response": {"status_code": 200, "request_id": "45a8e6ea1532131fd44927b85baceca3", "body": {"id": "chatcmpl-C4t6xbpxWVfCW316p3sFuUrJ4W3Sl", "object": "chat.completion", "created": 1755281195, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n#include <algorithm>\n\n// This code implements a k-NN (k-nearest neighbors) search for 2D points using CUDA.\n// - Each query is processed by a single warp (32 threads).\n// - The block cooperatively loads tiles of the data points into shared memory.\n// - Each warp maintains a private intermediate result (top-k) and a candidate buffer (size k) in shared memory.\n// - Distances are squared Euclidean distances.\n// - Candidate insertion uses warp-aggregated atomicAdd on a per-warp shared counter.\n// - When the candidate buffer is full, the warp merges it with the intermediate result:\n//   1) Sort candidate buffer with bitonic sort (ascending).\n//   2) Merge (bitonic merge trick): merged[i] = min(intermediate[i], candidate[k-1-i]).\n//   3) Sort merged (ascending) to form the updated intermediate result.\n// - After all tiles, any remaining candidates are merged the same way.\n// - Results (indices and distances) are written to the output array in ascending order of distances.\n\n#ifndef KNN_WARPS_PER_BLOCK\n#define KNN_WARPS_PER_BLOCK 4  // Number of warps per block (blockDim.x = 32 * KNN_WARPS_PER_BLOCK)\n#endif\n\n#ifndef KNN_TILE_CAPACITY\n#define KNN_TILE_CAPACITY 8192 // Number of data points per tile cached in shared memory\n#endif\n\n// Simple POD pair compatible with std::pair<int,float> memory layout for device writes.\n// We reinterpret device output pointer to this layout to avoid relying on libstdc++ in device code.\nstruct PairIF { int first; float second; };\n\n// Return a lane-local mask of lower lanes: bits set for threads with laneId < this lane.\nstatic __device__ __forceinline__ unsigned lane_mask_lt()\n{\n#if (__CUDACC_VER_MAJOR__ >= 9)\n    // Alternative using asm: but simple arithmetic is sufficient\n#endif\n    unsigned lane = threadIdx.x & 31;\n    return (lane == 0) ? 0u : ((1u << lane) - 1u);\n}\n\n// Warp-level bitonic sort on an array of pairs (dist[idx], idx[idx]) stored in shared memory.\n// - n must be a power of two (here n == k, guaranteed by problem statement).\n// - Sorting is ascending by distance.\n// - All 32 threads in the warp cooperate; each thread processes a strided subset of indices.\nstatic __device__ __forceinline__ void warp_bitonic_sort_pairs(float* dist, int* idx, int n, unsigned warp_mask)\n{\n    // Outer loop controls the size of the bitonic sequence (size = 2, 4, 8, ..., n)\n    for (int size = 2; size <= n; size <<= 1)\n    {\n        // Inner loop controls the distance between compared elements (stride halves each iteration)\n        for (int stride = size >> 1; stride > 0; stride >>= 1)\n        {\n            // Each thread processes indices i = lane, lane+32, lane+64, ...\n            for (int i = (threadIdx.x & 31); i < n; i += 32)\n            {\n                int l = i ^ stride;\n                if (l > i)\n                {\n                    bool up = ((i & size) == 0); // ascending segment if true, descending if false\n                    float di = dist[i], dl = dist[l];\n                    int   ii = idx[i],  il = idx[l];\n\n                    // Compare-exchange based on direction 'up'\n                    // For ascending segments, place smaller at i; for descending, place larger at i.\n                    bool cond = (di > dl);\n                    if (up ? cond : !cond)\n                    {\n                        dist[i] = dl; dist[l] = di;\n                        idx[i]  = il; idx[l]  = ii;\n                    }\n                }\n            }\n            __syncwarp(warp_mask);\n        }\n    }\n}\n\n// Merge the per-warp candidate buffer into the intermediate top-k result using the prescribed method.\n// Steps:\n//   1) Pad candidate buffer tail with +INF (if not full) and bitonic-sort ascending.\n//   2) For i in [0..k): merged[i] = min(inter[i], cand[k-1-i]) -> written into cand[i].\n//   3) Bitonic-sort merged (cand) ascending.\n//   4) Copy merged back to inter; update max_distance and reset candidate count.\n// All operations are warp-synchronous and use shared memory arrays.\nstatic __device__ __forceinline__ void warp_flush_merge(\n    float* cand_dist, int* cand_idx,\n    float* inter_dist, int* inter_idx,\n    volatile int* cand_count_ptr, volatile float* max_dist_ptr,\n    int k, unsigned warp_mask)\n{\n    int cand_count = *cand_count_ptr;\n\n    // Pad unused candidate entries with +INF and invalid index.\n    // This ensures bitonic sort behaves correctly for the full k-length array.\n    for (int i = (threadIdx.x & 31); i < k; i += 32)\n    {\n        if (i >= cand_count)\n        {\n            cand_dist[i] = CUDART_INF_F;\n            cand_idx[i]  = -1;\n        }\n    }\n    __syncwarp(warp_mask);\n\n    // Step 1: sort candidate buffer ascending\n    warp_bitonic_sort_pairs(cand_dist, cand_idx, k, warp_mask);\n\n    // Step 2: merge with intermediate: merged[i] = min(inter[i], cand[k-1-i])\n    // Write merged into cand arrays (OK with warp barrier to avoid read-write hazards).\n    for (int i = (threadIdx.x & 31); i < k; i += 32)\n    {\n        float di = inter_dist[i];\n        int   ii = inter_idx[i];\n        float dc = cand_dist[k - 1 - i];\n        int   ic = cand_idx[k - 1 - i];\n\n        bool take_cand = (dc < di);\n        cand_dist[i] = take_cand ? dc : di;\n        cand_idx[i]  = take_cand ? ic : ii;\n    }\n    __syncwarp(warp_mask);\n\n    // Step 3: sort merged result ascending\n    warp_bitonic_sort_pairs(cand_dist, cand_idx, k, warp_mask);\n\n    // Step 4: copy to intermediate result and update max distance, reset candidate count\n    for (int i = (threadIdx.x & 31); i < k; i += 32)\n    {\n        inter_dist[i] = cand_dist[i];\n        inter_idx[i]  = cand_idx[i];\n    }\n    __syncwarp(warp_mask);\n\n    if ((threadIdx.x & 31) == 0) {\n        *max_dist_ptr = inter_dist[k - 1];\n        *cand_count_ptr = 0;\n    }\n    __syncwarp(warp_mask);\n}\n\n// Try to insert a single candidate (index, distance) into the per-warp candidate buffer.\n// - pass: whether this thread produced a candidate (distance < max_distance).\n// - Uses warp-aggregated atomicAdd to reserve positions in the buffer.\n// - If buffer capacity (k) would be exceeded, partially fills remaining slots, flushes (merge), then\n//   re-evaluates remaining candidates against updated max_distance and inserts the remainder.\nstatic __device__ __forceinline__ void warp_try_add_candidate(\n    bool pass, int index, float distance, int k,\n    float* cand_dist, int* cand_idx, volatile int* cand_count_ptr, volatile float* max_dist_ptr,\n    float* inter_dist, int* inter_idx,\n    unsigned warp_mask)\n{\n    unsigned mask = __ballot_sync(warp_mask, pass);\n    int lane = threadIdx.x & 31;\n    if (mask == 0) return;\n\n    int n = __popc(mask);\n    int rank = __popc(mask & lane_mask_lt());\n\n    // Read current count atomically (atomicAdd with 0) to satisfy the requirement to use atomicAdd.\n    int cur = atomicAdd((int*)cand_count_ptr, 0);\n    int space = k - cur;\n\n    if (space >= n)\n    {\n        // Enough space for all in one go\n        int base = atomicAdd((int*)cand_count_ptr, n);\n        if (pass)\n        {\n            int pos = base + rank;\n            cand_dist[pos] = distance;\n            cand_idx[pos]  = index;\n        }\n        return;\n    }\n    else\n    {\n        // Not enough space: fill remaining, flush, then insert leftovers if still passing threshold\n        int to_write = max(0, space);\n        if (to_write > 0)\n        {\n            int base = atomicAdd((int*)cand_count_ptr, to_write);\n            if (pass && (rank < to_write))\n            {\n                int pos = base + rank;\n                cand_dist[pos] = distance;\n                cand_idx[pos]  = index;\n            }\n        }\n        __syncwarp(warp_mask);\n\n        // Flush (merge) full candidate buffer\n        warp_flush_merge(cand_dist, cand_idx, inter_dist, inter_idx, cand_count_ptr, max_dist_ptr, k, warp_mask);\n        __syncwarp(warp_mask);\n\n        // Re-evaluate leftover lanes with updated max_distance\n        float maxd = *max_dist_ptr;\n        bool keep_leftover = pass && (rank >= to_write) && (distance < maxd);\n        unsigned rem_mask = __ballot_sync(warp_mask, keep_leftover);\n        int rem = __popc(rem_mask);\n        if (rem > 0)\n        {\n            int r2 = __popc(rem_mask & lane_mask_lt());\n            int base2 = atomicAdd((int*)cand_count_ptr, rem);\n            if (keep_leftover)\n            {\n                int pos = base2 + r2;\n                cand_dist[pos] = distance;\n                cand_idx[pos]  = index;\n            }\n        }\n    }\n}\n\n// Kernel: Each warp processes one query. The block loads tiles of the data array to shared memory.\n__global__ void knn_kernel_2d(\n    const float2* __restrict__ query,\n    int query_count,\n    const float2* __restrict__ data,\n    int data_count,\n    PairIF* __restrict__ result,\n    int k)\n{\n    // Thread and warp identifiers\n    const int lane  = threadIdx.x & 31;\n    const int warp_in_block = threadIdx.x >> 5; // warp index within the block\n    const int warps_per_block = blockDim.x >> 5;\n    const unsigned warp_mask = 0xFFFFFFFFu;\n\n    // Global warp index maps one-to-one with query index\n    const int warp_global = blockIdx.x * warps_per_block + warp_in_block;\n    const int query_idx = warp_global;\n\n    // Shared memory layout (dynamic):\n    // [0 .. TILE_CAPACITY-1] float2: tile of data points loaded by the whole block\n    extern __shared__ unsigned char shared_bytes[];\n    float2* s_data = reinterpret_cast<float2*>(shared_bytes);\n\n    // Compute base pointer for per-warp storage after the data tile region\n    unsigned char* perwarp_base = shared_bytes + sizeof(float2) * KNN_TILE_CAPACITY;\n\n    // Per-warp storage size (bytes): 4 arrays of k elements (float, int) + int counter + float max_distance\n    const size_t per_warp_bytes = ((size_t)k * (sizeof(float) + sizeof(int)) * 2) + sizeof(int) + sizeof(float);\n\n    // Compute this warp's base pointer into its private shared memory region\n    unsigned char* wptr = perwarp_base + warp_in_block * per_warp_bytes;\n\n    // Carve per-warp regions\n    float* w_cand_dist = reinterpret_cast<float*>(wptr);\n    int*   w_cand_idx  = reinterpret_cast<int*>(w_cand_dist + k);\n    float* w_inter_dist= reinterpret_cast<float*>(w_cand_idx + k);\n    int*   w_inter_idx = reinterpret_cast<int*>(w_inter_dist + k);\n    volatile int*   w_cand_count = reinterpret_cast<volatile int*>(w_inter_idx + k);\n    volatile float* w_max_dist   = reinterpret_cast<volatile float*>(const_cast<int*>(reinterpret_cast<const int*>(w_cand_count)) + 1);\n\n    // Initialize per-warp structures\n    if (query_idx < query_count)\n    {\n        for (int i = lane; i < k; i += 32)\n        {\n            w_inter_dist[i] = CUDART_INF_F;\n            w_inter_idx[i]  = -1;\n        }\n    }\n    __syncwarp(warp_mask);\n    if (lane == 0) {\n        *w_cand_count = 0;\n        *w_max_dist   = CUDART_INF_F;\n    }\n    __syncwarp(warp_mask);\n\n    // Load query point into registers and broadcast within warp\n    float2 q = make_float2(0.f, 0.f);\n    if (lane == 0 && query_idx < query_count)\n    {\n        q = query[query_idx];\n    }\n    q.x = __shfl_sync(warp_mask, q.x, 0);\n    q.y = __shfl_sync(warp_mask, q.y, 0);\n\n    // Process data in tiles\n    for (int tile_start = 0; tile_start < data_count; tile_start += KNN_TILE_CAPACITY)\n    {\n        int tile_size = min(KNN_TILE_CAPACITY, data_count - tile_start);\n\n        // Cooperative load of the tile into shared memory by the entire block\n        for (int i = threadIdx.x; i < tile_size; i += blockDim.x)\n        {\n            s_data[i] = data[tile_start + i];\n        }\n        __syncthreads();\n\n        // Each active warp processes its query over the cached tile\n        if (query_idx < query_count)\n        {\n            float maxd = *w_max_dist; // Local copy of current max distance for this warp\n            for (int t = lane; t < tile_size; t += 32)\n            {\n                float2 p = s_data[t];\n                float dx = p.x - q.x;\n                float dy = p.y - q.y;\n                float d2 = fmaf(dx, dx, dy * dy);\n\n                // Filter by current max_distance; attempt candidate insertion if it's better\n                bool pass = (d2 < maxd);\n                int  idx  = tile_start + t;\n\n                // Insert candidate using warp-aggregated atomic pattern,\n                // flushing (merge) when the candidate buffer fills up.\n                if (pass)\n                {\n                    warp_try_add_candidate(pass, idx, d2, k,\n                        w_cand_dist, w_cand_idx, w_cand_count, w_max_dist,\n                        w_inter_dist, w_inter_idx, warp_mask);\n\n                    // Update local copy of maxd after potential flush/merge\n                    maxd = *w_max_dist;\n                }\n            }\n        }\n        __syncthreads(); // Ensure tile is no longer in use before overwriting in next iteration\n    }\n\n    // After all tiles, flush remaining candidates if any\n    if (query_idx < query_count)\n    {\n        int cc = atomicAdd((int*)w_cand_count, 0);\n        if (cc > 0)\n        {\n            __syncwarp(warp_mask);\n            warp_flush_merge(w_cand_dist, w_cand_idx, w_inter_dist, w_inter_idx, w_cand_count, w_max_dist, k, warp_mask);\n            __syncwarp(warp_mask);\n        }\n\n        // Write final results: intermediate result holds k nearest neighbors in ascending order\n        PairIF* out = result + (size_t)query_idx * (size_t)k;\n        for (int i = lane; i < k; i += 32)\n        {\n            out[i].first  = w_inter_idx[i];\n            out[i].second = w_inter_dist[i];\n        }\n    }\n}\n\n// Host-side launcher with sensible defaults for modern data-center GPUs (A100/H100).\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k)\n{\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Kernel configuration: 4 warps per block (128 threads/block) by default.\n    constexpr int warps_per_block = KNN_WARPS_PER_BLOCK;\n    constexpr int threads_per_block = warps_per_block * 32;\n    dim3 block(threads_per_block, 1, 1);\n    dim3 grid((query_count + warps_per_block - 1) / warps_per_block, 1, 1);\n\n    // Dynamic shared memory size:\n    // - Data tile: KNN_TILE_CAPACITY * sizeof(float2)\n    // - Per-warp private storage: warps_per_block * (4*k elements + counters)\n    size_t tile_bytes = (size_t)KNN_TILE_CAPACITY * sizeof(float2);\n    size_t per_warp_bytes = ((size_t)k * (sizeof(float) + sizeof(int)) * 2) + sizeof(int) + sizeof(float);\n    size_t smem_bytes = tile_bytes + (size_t)warps_per_block * per_warp_bytes;\n\n    // Opt-in to large dynamic shared memory if needed (beyond 48KB default).\n    int device = 0;\n    cudaGetDevice(&device);\n    int max_optin = 0;\n    cudaDeviceGetAttribute(&max_optin, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    int requested = (int)smem_bytes;\n    if (requested > max_optin) {\n        // If requested exceeds the opt-in max, clamp tile size heuristically by reducing tile capacity.\n        // Fallback heuristic: shrink tile to fit within max_optin while keeping per-warp storage intact.\n        int64_t available_for_tile = (int64_t)max_optin - (int64_t)warps_per_block * (int64_t)per_warp_bytes;\n        int new_tile_cap = (available_for_tile > 0) ? (int)(available_for_tile / (int64_t)sizeof(float2)) : 0;\n        // Ensure at least 1024 tile capacity to maintain some caching benefit; if not possible, set to minimal positive.\n        if (new_tile_cap < 1024) new_tile_cap = max(1, new_tile_cap);\n        // Recompute shared memory size with reduced tile capacity\n        tile_bytes = (size_t)new_tile_cap * sizeof(float2);\n        smem_bytes = tile_bytes + (size_t)warps_per_block * per_warp_bytes;\n        requested = (int)smem_bytes;\n    }\n    cudaFuncSetAttribute(knn_kernel_2d, cudaFuncAttributeMaxDynamicSharedMemorySize, requested);\n\n    // Launch kernel\n    knn_kernel_2d<<<grid, block, smem_bytes>>>(\n        query, query_count, data, data_count,\n        reinterpret_cast<PairIF*>(result), k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1203, "completion_tokens": 17977, "total_tokens": 19180, "prompt_tokens_details": {"cached_tokens": 1152, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 13888, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db9002481909b5af95f9fefdeab", "custom_id": "knn07-06", "response": {"status_code": 200, "request_id": "4309c5da12d8fd2e19e1ede536fbb98f", "body": {"id": "chatcmpl-C4stbWmTUtFKd4agouzzf9nHTihPG", "object": "chat.completion", "created": 1755280367, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n#include <algorithm>\n#include <cmath>\n\n// This implementation computes k-NN for 2D points using a single warp (32 threads) per query.\n// The algorithm processes the data in shared-memory tiles. Each warp maintains:\n// - An intermediate result of size k (sorted ascending by distance).\n// - A candidate buffer of size k (unsorted; sorted on flush).\n// - A temporary array of size k for merging/sorting.\n// - A shared counter for the number of candidates currently stored in the buffer.\n// When the candidate buffer becomes full (>= k), the warp flushes it:\n//   1) Sort the buffer with bitonic sort (ascending).\n//   2) Merge the buffer and the intermediate result into a bitonic sequence using pairwise min between\n//      buffer[i] and intermediate[k-1-i].\n//   3) Sort the merged result with bitonic sort (ascending) to get the updated intermediate result.\n// The warp maintains a max_distance value equal to the distance of the current k-th neighbor (last element\n// of the intermediate result). New distances >= max_distance are ignored at insertion time.\n//\n// The kernel uses dynamic shared memory and cooperatively loads tiles of data points per block.\n// The host function selects warps per block and tile size to fit into the device shared memory budget,\n// and opts-in to use the maximum dynamic shared memory when available on modern GPUs (A100/H100).\n\n// Helpers\n#define CUDA_FULL_MASK 0xFFFFFFFFu\nstatic constexpr int WARP_SIZE = 32;\n\nstruct KDPair {\n    float dist;\n    int   idx;\n};\n\n// Align an offset up to the specified alignment (which is a power of two)\n__host__ __device__ static inline size_t align_up(size_t offset, size_t alignment) {\n    return (offset + alignment - 1) & ~(alignment - 1);\n}\n\n// Warp-synchronous bitonic sort on shared-memory array \"arr\" of length n (power of two).\n// Sorts ascending by distance. Threads participate cooperatively with strided indices.\n__device__ __forceinline__ void warp_bitonic_sort(KDPair* arr, int n) {\n    // Bitonic sort with XOR-based partner indexing, using the standard nested-loop structure.\n    for (int kstage = 2; kstage <= n; kstage <<= 1) {\n        for (int j = kstage >> 1; j > 0; j >>= 1) {\n            // Each lane processes multiple indices i in [0, n) with stride WARP_SIZE\n            for (int i = threadIdx.x & (WARP_SIZE - 1); i < n; i += WARP_SIZE) {\n                int l = i ^ j;\n                if (l > i) {\n                    bool up = ((i & kstage) == 0);\n                    KDPair ai = arr[i];\n                    KDPair al = arr[l];\n                    bool swap_cond = up ? (ai.dist > al.dist) : (ai.dist < al.dist);\n                    if (swap_cond) {\n                        arr[i] = al;\n                        arr[l] = ai;\n                    }\n                }\n            }\n            __syncwarp();\n        }\n    }\n}\n\n// Fill arr[n] with value v cooperatively by a warp (strided by lane).\n__device__ __forceinline__ void warp_fill(KDPair* arr, int n, const KDPair& v) {\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n    for (int i = lane; i < n; i += WARP_SIZE) {\n        arr[i] = v;\n    }\n    __syncwarp();\n}\n\n// Copy src[n] to dst[n] cooperatively by a warp (strided by lane).\n__device__ __forceinline__ void warp_copy(KDPair* dst, const KDPair* src, int n) {\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n    for (int i = lane; i < n; i += WARP_SIZE) {\n        dst[i] = src[i];\n    }\n    __syncwarp();\n}\n\n// Merge two sorted arrays 'buf_sorted' (ascending) and 'inter_sorted' (ascending) into 'merged' (size n)\n// via the bitonic \"min\" trick: merged[i] = min(buf_sorted[i], inter_sorted[n-1-i]) by distance.\n// Result is a bitonic sequence which is then sorted with bitonic sort to obtain ascending order.\n__device__ __forceinline__ void warp_bitonic_min_merge(KDPair* merged, const KDPair* buf_sorted, const KDPair* inter_sorted, int n) {\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n    for (int i = lane; i < n; i += WARP_SIZE) {\n        KDPair a = buf_sorted[i];\n        KDPair b = inter_sorted[n - 1 - i];\n        merged[i] = (a.dist < b.dist) ? a : b;\n    }\n    __syncwarp();\n}\n\n// Flush the candidate buffer into the intermediate result.\n// Steps:\n// 1) Pad the buffer (positions [count, k)) with +inf distances.\n// 2) Sort the buffer with warp_bitonic_sort (ascending).\n// 3) Merge buffer and intermediate with warp_bitonic_min_merge into 'merged'.\n// 4) Sort merged with warp_bitonic_sort (ascending).\n// 5) Copy merged to intermediate.\n// 6) Reset candidate count and update max_distance (return it).\n__device__ __forceinline__ float warp_flush_buffer(KDPair* buf, KDPair* inter, KDPair* merged,\n                                                   int k, int* candidate_count_ptr) {\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n\n    // Read current candidate count (the buffer might have logically more candidates than we physically store).\n    int count = 0;\n    if (lane == 0) count = *candidate_count_ptr;\n    count = __shfl_sync(CUDA_FULL_MASK, count, 0);\n\n    // We only physically store the first min(count, k) entries; pad the rest with +inf.\n    int effective = count < k ? count : k;\n\n    // Pad [effective, k) with inf distances and invalid indices.\n    KDPair infv; infv.dist = CUDART_INF_F; infv.idx = -1;\n    for (int i = lane + effective; i < k; i += WARP_SIZE) {\n        buf[i] = infv;\n    }\n    __syncwarp();\n\n    // Sort buffer ascending\n    warp_bitonic_sort(buf, k);\n\n    // Merge with intermediate using bitonic min trick\n    warp_bitonic_min_merge(merged, buf, inter, k);\n\n    // Sort merged ascending\n    warp_bitonic_sort(merged, k);\n\n    // Copy back to intermediate\n    warp_copy(inter, merged, k);\n\n    // Reset candidate count to 0\n    if (lane == 0) *candidate_count_ptr = 0;\n    __syncwarp();\n\n    // Update and return max_distance = inter[k-1].dist\n    float max_d = 0.0f;\n    if (lane == 0) max_d = inter[k - 1].dist;\n    max_d = __shfl_sync(CUDA_FULL_MASK, max_d, 0);\n    return max_d;\n}\n\n// Kernel\n__global__ void knn_kernel(const float2* __restrict__ query,\n                           int query_count,\n                           const float2* __restrict__ data,\n                           int data_count,\n                           std::pair<int, float>* __restrict__ result,\n                           int k,\n                           int tile_points,          // number of data points per shared-memory tile\n                           int warps_per_block) {    // warps per block = blockDim.x / 32\n    extern __shared__ unsigned char smem_raw[];\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n    const int warp_id = threadIdx.x / WARP_SIZE;\n\n    // Shared memory layout:\n    // [ tile (float2[tile_points]) ]\n    // [ padding to KDPair alignment ]\n    // [ inter  (KDPair[warps_per_block * k]) ]\n    // [ buf    (KDPair[warps_per_block * k]) ]\n    // [ merged (KDPair[warps_per_block * k]) ]\n    // [ counters (int[warps_per_block]) ]\n\n    size_t offset = 0;\n    float2* tile = reinterpret_cast<float2*>(smem_raw + offset);\n    offset += static_cast<size_t>(tile_points) * sizeof(float2);\n    offset = align_up(offset, alignof(KDPair));\n\n    KDPair* inter_base  = reinterpret_cast<KDPair*>(smem_raw + offset);\n    offset += static_cast<size_t>(warps_per_block) * static_cast<size_t>(k) * sizeof(KDPair);\n\n    KDPair* buf_base    = reinterpret_cast<KDPair*>(smem_raw + offset);\n    offset += static_cast<size_t>(warps_per_block) * static_cast<size_t>(k) * sizeof(KDPair);\n\n    KDPair* merged_base = reinterpret_cast<KDPair*>(smem_raw + offset);\n    offset += static_cast<size_t>(warps_per_block) * static_cast<size_t>(k) * sizeof(KDPair);\n\n    offset = align_up(offset, alignof(int));\n    int* counters = reinterpret_cast<int*>(smem_raw + offset);\n    // No further offset needed\n\n    // Per-warp slices\n    KDPair* inter  = inter_base  + static_cast<size_t>(warp_id) * static_cast<size_t>(k);\n    KDPair* buf    = buf_base    + static_cast<size_t>(warp_id) * static_cast<size_t>(k);\n    KDPair* merged = merged_base + static_cast<size_t>(warp_id) * static_cast<size_t>(k);\n    int* candidate_count_ptr = &counters[warp_id];\n\n    const int global_warp_id = blockIdx.x * warps_per_block + warp_id;\n    const int warp_stride = gridDim.x * warps_per_block;\n\n    for (int q = global_warp_id; q < query_count; q += warp_stride) {\n        // Initialize intermediate result to +inf distances.\n        KDPair infv; infv.dist = CUDART_INF_F; infv.idx = -1;\n        warp_fill(inter, k, infv);\n\n        if (lane == 0) *candidate_count_ptr = 0;\n        __syncwarp();\n\n        // Load query point into registers\n        float2 qpt = query[q];\n        float max_distance = CUDART_INF_F;\n\n        // Process data in tiles\n        for (int tile_start = 0; tile_start < data_count; tile_start += tile_points) {\n            int tile_len = data_count - tile_start;\n            if (tile_len > tile_points) tile_len = tile_points;\n\n            // Load the tile cooperatively by the whole block\n            for (int t = threadIdx.x; t < tile_len; t += blockDim.x) {\n                tile[t] = data[tile_start + t];\n            }\n            __syncthreads();\n\n            // Each warp processes the tile for its query\n            for (int t = lane; t < tile_len; t += WARP_SIZE) {\n                float2 p = tile[t];\n                float dx = p.x - qpt.x;\n                float dy = p.y - qpt.y;\n                float d = dx * dx + dy * dy;\n\n                if (d < max_distance) {\n                    int pos = atomicAdd(candidate_count_ptr, 1);\n                    if (pos < k) {\n                        buf[pos].dist = d;\n                        buf[pos].idx  = tile_start + t;\n                    }\n                }\n\n                __syncwarp();\n                // If buffer is full, flush and merge\n                bool do_flush = false;\n                if (lane == 0) do_flush = (*candidate_count_ptr >= k);\n                do_flush = __shfl_sync(CUDA_FULL_MASK, do_flush, 0);\n                if (do_flush) {\n                    max_distance = warp_flush_buffer(buf, inter, merged, k, candidate_count_ptr);\n                }\n            }\n\n            __syncwarp();\n            __syncthreads();\n        }\n\n        // Flush any remaining candidates\n        int remaining = 0;\n        if (lane == 0) remaining = *candidate_count_ptr;\n        remaining = __shfl_sync(CUDA_FULL_MASK, remaining, 0);\n        if (remaining > 0) {\n            max_distance = warp_flush_buffer(buf, inter, merged, k, candidate_count_ptr);\n        }\n\n        // Write out the final k nearest neighbors for this query (inter is sorted ascending)\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            const KDPair e = inter[i];\n            std::pair<int, float>& out = result[static_cast<size_t>(q) * static_cast<size_t>(k) + i];\n            out.first  = e.idx;\n            out.second = e.dist;\n        }\n        __syncwarp();\n    }\n}\n\n// Host function: selects block configuration and shared memory size, then launches the kernel.\nvoid run_knn(const float2* query, int query_count,\n             const float2* data, int data_count,\n             std::pair<int, float>* result, int k) {\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Determine device and shared memory capabilities\n    int device = 0;\n    cudaGetDevice(&device);\n\n    int max_blocks = 0;\n    cudaDeviceGetAttribute(&max_blocks, cudaDevAttrMultiProcessorCount, device);\n\n    int max_shmem_optin = 0;\n    cudaDeviceGetAttribute(&max_shmem_optin, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n\n    int max_shmem_default = 0;\n    cudaDeviceGetAttribute(&max_shmem_default, cudaDevAttrMaxSharedMemoryPerBlock, device);\n\n    // We'll try to use the opt-in maximum shared memory if available; otherwise default limit.\n    int max_shmem_available = (max_shmem_optin > 0) ? max_shmem_optin : max_shmem_default;\n\n    // Choose warps per block to fit shared memory usage. Try from 6 down to 1 to allow flexibility.\n    // Each warp needs 3*k KDPair entries + 1 int for the counter.\n    // Additionally, we need tile_points * sizeof(float2) bytes.\n    // We'll select tile_points based on remaining shared memory after per-warp allocations.\n    int warps_per_block = 4; // start target\n    const int max_try_warps = 6;\n    for (int try_warps = std::min(max_try_warps, (int)((max_shmem_available) / std::max(1, 3 * k * (int)sizeof(KDPair) + (int)sizeof(int)))); try_warps >= 1; --try_warps) {\n        size_t per_warp_bytes = static_cast<size_t>(3) * static_cast<size_t>(k) * sizeof(KDPair) + sizeof(int);\n        size_t warp_region = per_warp_bytes * try_warps;\n        if ((int)warp_region >= max_shmem_available) continue; // leaves no room for tile\n        // leave at least space for a minimal tile (at least one warp's worth of points)\n        size_t remaining = static_cast<size_t>(max_shmem_available) - warp_region;\n        if (remaining >= sizeof(float2) * 32) { // at least one warp of points\n            warps_per_block = try_warps;\n            break;\n        }\n    }\n\n    // Compute tile_points as large as possible given shared memory budget\n    size_t per_warp_bytes = static_cast<size_t>(3) * static_cast<size_t>(k) * sizeof(KDPair) + sizeof(int);\n    size_t warp_region = per_warp_bytes * warps_per_block;\n    size_t remaining = (max_shmem_available > (int)warp_region) ? (static_cast<size_t>(max_shmem_available) - warp_region) : 0;\n    int tile_points = 0;\n    if (remaining >= sizeof(float2)) {\n        tile_points = static_cast<int>(remaining / sizeof(float2));\n        // cap tile_points to a reasonable number to avoid over-large tiles; 8192 is a good balance\n        tile_points = std::min(tile_points, 8192);\n        // ensure at least one warp worth of work in each tile\n        tile_points = std::max(tile_points, warps_per_block * WARP_SIZE);\n    } else {\n        // Fallback minimal tile\n        tile_points = warps_per_block * WARP_SIZE;\n    }\n\n    // Total dynamic shared memory required\n    size_t shmem_bytes = 0;\n    shmem_bytes += static_cast<size_t>(tile_points) * sizeof(float2);\n    shmem_bytes = align_up(shmem_bytes, alignof(KDPair));\n    shmem_bytes += static_cast<size_t>(warps_per_block) * static_cast<size_t>(k) * sizeof(KDPair); // inter\n    shmem_bytes += static_cast<size_t>(warps_per_block) * static_cast<size_t>(k) * sizeof(KDPair); // buf\n    shmem_bytes += static_cast<size_t>(warps_per_block) * static_cast<size_t>(k) * sizeof(KDPair); // merged\n    shmem_bytes = align_up(shmem_bytes, alignof(int));\n    shmem_bytes += static_cast<size_t>(warps_per_block) * sizeof(int); // counters\n\n    // If opt-in is available, request it to allow large dynamic shared memory\n    if (max_shmem_optin > 0) {\n        cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)shmem_bytes);\n    }\n\n    // Launch configuration\n    int threads_per_block = warps_per_block * WARP_SIZE;\n    int blocks = (query_count + warps_per_block - 1) / warps_per_block;\n    // Avoid launching more blocks than necessary; but grid can be large.\n    // Optionally cap to a multiple of SMs to avoid oversubscription:\n    int max_active_blocks = max_blocks * 8; // heuristic cap\n    if (blocks > max_active_blocks) blocks = max_active_blocks;\n    if (blocks < 1) blocks = 1;\n\n    knn_kernel<<<blocks, threads_per_block, shmem_bytes>>>(query, query_count, data, data_count, result, k, tile_points, warps_per_block);\n\n    // Synchronize to ensure completion (optional error check can be added if desired)\n    cudaDeviceSynchronize();\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1203, "completion_tokens": 16397, "total_tokens": 17600, "prompt_tokens_details": {"cached_tokens": 1152, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 12416, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db91a088190ba2e111e70f78ff2", "custom_id": "knn07-07", "response": {"status_code": 200, "request_id": "ed77ba43c2d6759a6e3712aa55f5fca6", "body": {"id": "chatcmpl-C4t8z79pEYIhZNuQnqm8gYnQsOySn", "object": "chat.completion", "created": 1755281321, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cfloat>\n#include <utility>\n\n// The device-side pair used for all intermediate computations.\n// Distance is the primary key for comparisons.\nstruct PairK {\n    float dist;\n    int   idx;\n};\n\n// Warp size constant\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Utility: Compare-and-swap to enforce ascending or descending order\nstatic __device__ __forceinline__ void cas_pair(PairK &a, PairK &b, bool ascending) {\n    // If ascending, swap when a>b; if descending, swap when a<b\n    if (ascending) {\n        if (a.dist > b.dist) {\n            PairK t = a; a = b; b = t;\n        }\n    } else {\n        if (a.dist < b.dist) {\n            PairK t = a; a = b; b = t;\n        }\n    }\n}\n\n// In-warp bitonic sort for an array in shared memory.\n// - n must be a power of two (here, k is guaranteed to be).\n// - All 32 threads in the warp cooperate to sort the entire 'arr' range.\n// - Synchronization is warp-level (__syncwarp(mask)).\n// The algorithm is the standard, serial bitonic sort lifted to parallel by\n// having each lane iterate i in strides of warpSize, and only acting on pairs\n// with l > i to avoid double-swaps.\nstatic __device__ void bitonic_sort_warp(PairK* arr, int n, unsigned mask) {\n    // Outer loop controls the size of subsequences to be merged (ksize),\n    // inner loop controls the distance (j) between compared elements.\n    for (int ksize = 2; ksize <= n; ksize <<= 1) {\n        for (int j = ksize >> 1; j > 0; j >>= 1) {\n            // Each lane handles multiple i's in strides of warp size\n            for (int i = threadIdx.x & (WARP_SIZE - 1); i < n; i += WARP_SIZE) {\n                int l = i ^ j;\n                if (l > i) {\n                    bool up = ((i & ksize) == 0);\n                    PairK ai = arr[i];\n                    PairK al = arr[l];\n                    // Compare-and-swap into correct order\n                    if (up) {\n                        if (ai.dist > al.dist) {\n                            arr[i] = al;\n                            arr[l] = ai;\n                        }\n                    } else {\n                        if (ai.dist < al.dist) {\n                            arr[i] = al;\n                            arr[l] = ai;\n                        }\n                    }\n                }\n            }\n            __syncwarp(mask);\n        }\n    }\n}\n\n// Merge buffer 'cand' (sorted ascending) with 'res' (sorted ascending) into a\n// bitonic sequence using pairwise minima, then sort that sequence to update 'res'.\n// This follows the required merging procedure in the prompt:\n//   1) buffer 'cand' is assumed sorted in ascending order.\n//   2) merged[i] = min(cand[i], res[k-1-i])  for i in [0, k-1]\n//   3) sort merged in ascending order with bitonic sort to get new 'res'\nstatic __device__ void merge_and_update(PairK* res, PairK* cand, int k, float &max_dist, unsigned mask) {\n    // Step 2: form bitonic sequence via pairwise minima between cand[i] and reversed res\n    for (int i = threadIdx.x & (WARP_SIZE - 1); i < k; i += WARP_SIZE) {\n        PairK a = cand[i];\n        PairK b = res[k - 1 - i];\n        cand[i] = (a.dist <= b.dist) ? a : b;\n    }\n    __syncwarp(mask);\n\n    // Step 3: sort the merged bitonic sequence to get the updated result\n    bitonic_sort_warp(cand, k, mask);\n    __syncwarp(mask);\n\n    // Copy back to res\n    for (int i = threadIdx.x & (WARP_SIZE - 1); i < k; i += WARP_SIZE) {\n        res[i] = cand[i];\n    }\n    __syncwarp(mask);\n\n    // Update max_dist to the current k-th neighbor distance and broadcast to the warp\n    float new_max = res[k - 1].dist;\n    // Broadcast from lane 0\n    new_max = __shfl_sync(mask, new_max, 0);\n    max_dist = new_max;\n}\n\n// Flush the warp-local candidate buffer into the intermediate result.\n// - Fills unused slots (if any) up to k with +inf to enable a full-size sort.\n// - Sorts the buffer, merges with the current result via pairwise minima, then sorts again.\n// - Resets candidate count and unlocks buffer for further accumulation.\n// - Updates max_dist to the distance of the current k-th neighbor.\nstatic __device__ void flush_candidates_warp(PairK* res,\n                                             PairK* cand,\n                                             volatile int* cand_count,\n                                             volatile int* cand_lock,\n                                             int k,\n                                             float &max_dist,\n                                             unsigned mask) {\n    // Snapshot and clamp the count at k\n    int count = *cand_count;\n    if (count > k) count = k;\n\n    // Fill remaining slots with sentinel (+inf, idx=-1) to have exactly k items\n    for (int i = threadIdx.x & (WARP_SIZE - 1); i < k; i += WARP_SIZE) {\n        if (i >= count) {\n            cand[i].dist = FLT_MAX;\n            cand[i].idx  = -1;\n        }\n    }\n    __syncwarp(mask);\n\n    // Step 1: sort buffer ascending\n    bitonic_sort_warp(cand, k, mask);\n    __syncwarp(mask);\n\n    // Merge and update intermediate result and threshold\n    merge_and_update(res, cand, k, max_dist, mask);\n    __syncwarp(mask);\n\n    // Reset candidate count and unlock for further accumulation\n    if ((threadIdx.x & (WARP_SIZE - 1)) == 0) {\n        *cand_count = 0;\n        *cand_lock  = 0;\n    }\n    __syncwarp(mask);\n}\n\n// CUDA Kernel: One warp (32 threads) processes exactly one query at a time.\n// The block cooperatively tiles the data points into shared memory; each warp\n// uses the cached tile to compute distances to its own query and filters them\n// by the current max distance (distance of the k-th nearest neighbor).\n// Closer candidates are added to a per-warp shared buffer using atomicAdd.\n// Whenever the buffer becomes full, it is merged into the intermediate result.\n__global__ void knn_warp_kernel(const float2* __restrict__ query,\n                                int query_count,\n                                const float2* __restrict__ data,\n                                int data_count,\n                                std::pair<int, float>* __restrict__ result,\n                                int k,\n                                int tile_points_runtime) {\n    extern __shared__ unsigned char smem[];\n    unsigned mask = __activemask();\n\n    // Shared memory layout (dynamic, computed at launch-time):\n    // [0 .. tile_points_runtime) float2 tile points for cooperative cache\n    float2* tile = reinterpret_cast<float2*>(smem);\n\n    // After tile, per-warp arrays for intermediate result and candidate buffer:\n    // Layout: [ warp0_res[k], warp0_cand[k], warp1_res[k], warp1_cand[k], ... ]\n    int warps_per_block = blockDim.x / WARP_SIZE;\n    PairK* smem_pairs_base = reinterpret_cast<PairK*>(tile + tile_points_runtime);\n\n    // After per-warp PairK arrays, we allocate per-warp candidate counters and locks\n    int per_warp_pair_slots = 2 * k; // res[k] + cand[k]\n    int total_pair_slots = warps_per_block * per_warp_pair_slots;\n    int* smem_counts_base = reinterpret_cast<int*>(smem_pairs_base + total_pair_slots);\n    int* smem_locks_base  = smem_counts_base + warps_per_block;\n\n    // Warp identifiers\n    int lane_id         = threadIdx.x & (WARP_SIZE - 1);\n    int warp_in_block   = threadIdx.x >> 5; // threadIdx.x / WARP_SIZE\n    int warp_global_id  = blockIdx.x * warps_per_block + warp_in_block;\n    int total_warps_grid = gridDim.x * warps_per_block;\n\n    // Every warp processes multiple queries in a grid-stride loop\n    for (int q_idx = warp_global_id; q_idx < query_count; q_idx += total_warps_grid) {\n        // Pointers into shared memory for this warp\n        PairK* res  = smem_pairs_base + warp_in_block * per_warp_pair_slots;\n        PairK* cand = res + k;\n        volatile int* cand_count = smem_counts_base + warp_in_block;\n        volatile int* cand_lock  = smem_locks_base + warp_in_block;\n\n        // Initialize per-warp intermediate result with \"+inf\" and invalid indices\n        for (int i = lane_id; i < k; i += WARP_SIZE) {\n            res[i].dist = FLT_MAX;\n            res[i].idx  = -1;\n        }\n\n        // Initialize candidate counters and lock\n        if (lane_id == 0) {\n            *cand_count = 0;\n            *cand_lock  = 0;\n        }\n        __syncwarp(mask);\n\n        // Load this warp's query point (broadcast from lane 0)\n        float qx = 0.0f, qy = 0.0f;\n        if (lane_id == 0) {\n            float2 q = query[q_idx];\n            qx = q.x; qy = q.y;\n        }\n        qx = __shfl_sync(mask, qx, 0);\n        qy = __shfl_sync(mask, qy, 0);\n\n        // Initialize threshold to \"+inf\"\n        float max_dist = FLT_MAX;\n\n        // Iterate over data in tiles; each tile is loaded into shared memory by the whole block\n        for (int base = 0; base < data_count; base += tile_points_runtime) {\n            int tile_size = data_count - base;\n            if (tile_size > tile_points_runtime) tile_size = tile_points_runtime;\n\n            // Cooperative load of the current tile into shared memory\n            for (int t = threadIdx.x; t < tile_size; t += blockDim.x) {\n                tile[t] = data[base + t];\n            }\n            __syncthreads(); // ensure tile is fully populated\n\n            // Each warp processes all points in the tile, lanes stride by warp size\n            for (int j = lane_id; j < tile_size; j += WARP_SIZE) {\n                float2 p = tile[j];\n                float dx = p.x - qx;\n                float dy = p.y - qy;\n                float dist = fmaf(dx, dx, dy * dy); // squared Euclidean distance\n\n                // Filter by current threshold and only push when not in flushing state\n                if (dist < max_dist && *cand_lock == 0) {\n                    int pos = atomicAdd((int*)cand_count, 1);\n                    if (pos < k) {\n                        cand[pos].dist = dist;\n                        cand[pos].idx  = base + j;\n                    }\n                }\n\n                __syncwarp(mask);\n\n                // If buffer is full, initiate a flush\n                int need_flush = 0;\n                if (lane_id == 0) {\n                    need_flush = (*cand_count >= k) ? 1 : 0;\n                    if (need_flush) *cand_lock = 1;\n                }\n                need_flush = __shfl_sync(mask, need_flush, 0);\n\n                if (need_flush) {\n                    flush_candidates_warp(res, cand, cand_count, cand_lock, k, max_dist, mask);\n                }\n            }\n\n            __syncwarp(mask);\n\n            // After finishing the tile, flush any remaining candidates\n            int pending = 0;\n            if (lane_id == 0) {\n                pending = (*cand_count > 0) ? 1 : 0;\n                if (pending) *cand_lock = 1;\n            }\n            pending = __shfl_sync(mask, pending, 0);\n            if (pending) {\n                flush_candidates_warp(res, cand, cand_count, cand_lock, k, max_dist, mask);\n            }\n\n            __syncthreads(); // ensure all warps are done with the tile before loading the next\n        }\n\n        // Write out the final sorted k-NN for this query to global memory\n        // The result array is laid out so that result[q_idx * k + j] holds the j-th nearest neighbor\n        for (int i = lane_id; i < k; i += WARP_SIZE) {\n            int out_idx = q_idx * k + i;\n            result[out_idx].first  = res[i].idx;\n            result[out_idx].second = res[i].dist;\n        }\n    }\n}\n\n// Host-side launcher.\n// Chooses reasonable defaults for block size and shared memory sizing.\n// - Uses 32-thread warps, one warp per query, multiple warps per block.\n// - Computes dynamic shared memory usage as:\n//     tile_points * sizeof(float2) + warps_per_block * (2*k*sizeof(PairK) + 2*sizeof(int))\n// - Attempts to opt-in to the maximum available shared memory per block if needed.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data, int data_count,\n             std::pair<int, float> *result, int k) {\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Query device shared memory limits\n    int dev = 0;\n    cudaGetDevice(&dev);\n\n    int smem_default = 0;\n    int smem_optin   = 0;\n    cudaDeviceGetAttribute(&smem_default, cudaDevAttrMaxSharedMemoryPerBlock, dev);\n    cudaDeviceGetAttribute(&smem_optin,   cudaDevAttrMaxSharedMemoryPerBlockOptin, dev);\n    if (smem_optin <= 0) smem_optin = smem_default; // fallback\n\n    // Decide warps per block (try larger first, then reduce if memory bound)\n    // We'll try up to 4 warps per block for better latency hiding, then fall back.\n    int warps_per_block_candidates[] = {4, 3, 2, 1};\n    int chosen_warps = 1;\n    int chosen_block_threads = 32;\n    int chosen_tile_points = 512; // default; will be recalculated to fit shared memory\n    size_t chosen_smem_bytes = 0;\n\n    // Attempt to fit within opt-in shared memory limit (we will set the attribute accordingly)\n    for (int w : warps_per_block_candidates) {\n        // Per-warp memory: 2*k PairK (res + cand) + 2 ints (count + lock)\n        size_t per_warp_pairs_bytes = size_t(2 * k) * sizeof(PairK);\n        size_t per_warp_ctrl_bytes  = 2 * sizeof(int);\n        size_t per_warp_total       = per_warp_pairs_bytes + per_warp_ctrl_bytes;\n\n        // Use as much of smem_optin as possible, but reserve some bytes for safety\n        size_t reserved = 0; // no extra reservation\n        size_t rem = (smem_optin > reserved) ? (smem_optin - reserved) : 0;\n\n        // Compute max tile points that fit after allocating per-warp buffers\n        size_t total_warp_bytes = size_t(w) * per_warp_total;\n        if (rem <= total_warp_bytes) {\n            // Not enough memory for w warps; try fewer warps\n            continue;\n        }\n        size_t tile_bytes = rem - total_warp_bytes;\n        int tile_points = int(tile_bytes / sizeof(float2));\n        if (tile_points < 128) {\n            // Too small tile; try fewer warps\n            continue;\n        }\n\n        // Cap tile size to a reasonable upper bound to avoid huge sorts and to keep occupancy\n        if (tile_points > 8192) tile_points = 8192;\n\n        // Compute actual shared memory bytes we will request\n        size_t smem_bytes = size_t(tile_points) * sizeof(float2) + total_warp_bytes;\n\n        chosen_warps = w;\n        chosen_block_threads = w * WARP_SIZE;\n        chosen_tile_points = tile_points;\n        chosen_smem_bytes = smem_bytes;\n        break;\n    }\n\n    // If we couldn't fit the configuration using opt-in size, fall back to default and recompute\n    if (chosen_smem_bytes == 0) {\n        for (int w : warps_per_block_candidates) {\n            size_t per_warp_pairs_bytes = size_t(2 * k) * sizeof(PairK);\n            size_t per_warp_ctrl_bytes  = 2 * sizeof(int);\n            size_t total_warp_bytes     = size_t(w) * (per_warp_pairs_bytes + per_warp_ctrl_bytes);\n            if (smem_default <= total_warp_bytes) continue;\n            size_t tile_bytes = smem_default - total_warp_bytes;\n            int tile_points = int(tile_bytes / sizeof(float2));\n            if (tile_points < 128) continue;\n            if (tile_points > 4096) tile_points = 4096;\n            size_t smem_bytes = size_t(tile_points) * sizeof(float2) + total_warp_bytes;\n\n            chosen_warps = w;\n            chosen_block_threads = w * WARP_SIZE;\n            chosen_tile_points = tile_points;\n            chosen_smem_bytes = smem_bytes;\n            break;\n        }\n    }\n\n    // As a last-resort fallback, ensure there is at least one warp per block\n    if (chosen_smem_bytes == 0) {\n        chosen_warps = 1;\n        chosen_block_threads = WARP_SIZE;\n        // Use the largest tile that fits into default shared memory with one warp\n        size_t per_warp_pairs_bytes = size_t(2 * k) * sizeof(PairK);\n        size_t per_warp_ctrl_bytes  = 2 * sizeof(int);\n        size_t total_warp_bytes     = per_warp_pairs_bytes + per_warp_ctrl_bytes;\n        size_t tile_bytes = (smem_default > total_warp_bytes) ? (smem_default - total_warp_bytes) : 0;\n        int tile_points = int(tile_bytes / sizeof(float2));\n        if (tile_points < 32) tile_points = 32;\n        if (tile_points > 2048) tile_points = 2048;\n        chosen_tile_points = tile_points;\n        chosen_smem_bytes = size_t(tile_points) * sizeof(float2) + total_warp_bytes;\n    }\n\n    // Opt-in to larger shared memory per block if needed and available\n    if ((int)chosen_smem_bytes > smem_default && smem_optin >= (int)chosen_smem_bytes) {\n        cudaFuncSetAttribute(knn_warp_kernel,\n                             cudaFuncAttributeMaxDynamicSharedMemorySize,\n                             (int)chosen_smem_bytes);\n    }\n\n    // Grid size: assign approximately one warp per query, but also allow grid-stride looping\n    int blocks = (query_count + chosen_warps - 1) / chosen_warps;\n    if (blocks <= 0) blocks = 1;\n\n    // Launch the kernel\n    knn_warp_kernel<<<blocks, chosen_block_threads, chosen_smem_bytes>>>(\n        query, query_count, data, data_count, result, k, chosen_tile_points\n    );\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1203, "completion_tokens": 19848, "total_tokens": 21051, "prompt_tokens_details": {"cached_tokens": 1152, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 15616, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db94a548190949863e672c2149e", "custom_id": "knn07-08", "response": {"status_code": 200, "request_id": "133300738488d4be7e28556c1e7c90cc", "body": {"id": "chatcmpl-C4tBGnv4ufs5ZiHc2vpp6mTtu3fgX", "object": "chat.completion", "created": 1755281462, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cuda.h>\n#include <utility>\n#include <limits>\n#include <float.h>\n\n// This code implements a k-NN (k nearest neighbors) CUDA kernel for 2D points.\n// Each query is processed by a single warp (32 threads). The kernel processes\n// the dataset in shared-memory tiles and collects candidate neighbors that\n// are closer than the current max-distance (distance to the k-th nearest neighbor).\n// Candidates are stored in a per-warp shared buffer and, whenever that buffer\n// becomes full (size >= k), it is merged with the warp's intermediate result\n// via bitonic sorts as described in the problem statement.\n\n// Device-side pair representation to match std::pair<int,float> layout.\nstruct PairIF {\n    int first;\n    float second;\n};\nstatic_assert(sizeof(PairIF) == sizeof(std::pair<int,float>), \"PairIF must match std::pair<int,float> size\");\n\n// Compile-time parameters\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Number of warps per block and derived threads per block.\n// Adjusting BLOCK_WARPS impacts shared memory usage per block. We choose 4 to\n// balance shared memory footprint and occupancy on A100/H100.\n#ifndef BLOCK_WARPS\n#define BLOCK_WARPS 4\n#endif\n\n#ifndef BLOCK_THREADS\n#define BLOCK_THREADS (BLOCK_WARPS * WARP_SIZE)\n#endif\n\n// Utility: align an offset up to the given alignment (power of two).\n__device__ __forceinline__ size_t align_up(size_t offset, size_t alignment) {\n    return (offset + (alignment - 1)) & ~(alignment - 1);\n}\n\n// Warp-scope bitonic sort on length-n array (n is power-of-two).\n// Sorts in ascending order by distance. Index array is permuted accordingly.\n// All 32 threads of the warp cooperate to sort n elements resident in shared memory.\n// Each thread processes indices i = lane, lane+WARP_SIZE, ...\n__device__ __forceinline__ void warp_bitonic_sort(float* dist, int* idx, int n, unsigned mask) {\n    // The standard serial bitonic sort pseudocode is parallelized by having\n    // each lane handle a strided subset of indices. The XOR pairing pattern\n    // ensures disjoint compare-exchange pairs, so there are no write conflicts.\n    for (int k = 2; k <= n; k <<= 1) {\n        for (int j = k >> 1; j > 0; j >>= 1) {\n            int lane = threadIdx.x & (WARP_SIZE - 1);\n            for (int i = lane; i < n; i += WARP_SIZE) {\n                int l = i ^ j;\n                if (l > i) {\n                    bool up = ((i & k) == 0);\n                    float di = dist[i];\n                    float dl = dist[l];\n                    int ii = idx[i];\n                    int il = idx[l];\n                    bool do_swap = up ? (di > dl) : (di < dl);\n                    if (do_swap) {\n                        // swap elements i and l\n                        dist[i] = dl;\n                        dist[l] = di;\n                        idx[i] = il;\n                        idx[l] = ii;\n                    }\n                }\n            }\n            __syncwarp(mask);\n        }\n    }\n}\n\n// Flush/merge routine for a single warp.\n// - Sorts candidate buffer (size k, padding with +inf if current count < k).\n// - Merges with the intermediate result via element-wise min of buffer[i] and result[k-1-i].\n// - Sorts the merged buffer again (bitonic sort) to restore ascending order.\n// - Copies the merged buffer into the intermediate result.\n// - Updates max_distance to the new k-th (last) distance.\n// - Resets candidate_count to 0.\n//\n// All operations are warp-scope, using only __syncwarp for synchronization.\n__device__ __forceinline__ void warp_flush_merge(\n    int k,\n    float* res_dist, int* res_idx,        // intermediate result per warp, length k (sorted ascending)\n    float* cand_dist, int* cand_idx,      // candidate buffer per warp, length k (unsorted, partially filled)\n    volatile int* cand_count_ptr,         // shared counter for candidate buffer\n    float* max_distance_ptr,              // shared per-warp max distance (k-th distance)\n    unsigned mask                         // active lane mask\n) {\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n\n    // Snapshot and clamp the candidate count. Note: oversubscription due to atomicAdd is possible.\n    int count = *cand_count_ptr;\n    if (count > k) count = k;\n\n    // Pad remaining candidate slots [count, k) with +inf to make length = k (power of two).\n    for (int i = lane + count; i < k; i += WARP_SIZE) {\n        cand_dist[i] = FLT_MAX;\n        cand_idx[i] = -1;\n    }\n    __syncwarp(mask);\n\n    // 1. Sort candidate buffer (ascending).\n    warp_bitonic_sort(cand_dist, cand_idx, k, mask);\n    __syncwarp(mask);\n\n    // 2. Merge: produce bitonic sequence by taking element-wise min of\n    //    cand[i] and res[k-1-i]. Store back into cand arrays.\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        int j = k - 1 - i;\n        float cd = cand_dist[i];\n        float rd = res_dist[j];\n        int ci = cand_idx[i];\n        int ri = res_idx[j];\n        if (rd < cd) {\n            cand_dist[i] = rd;\n            cand_idx[i] = ri;\n        } else {\n            // keep current cand[i]\n        }\n    }\n    __syncwarp(mask);\n\n    // 3. Sort the merged (bitonic) buffer ascending.\n    warp_bitonic_sort(cand_dist, cand_idx, k, mask);\n    __syncwarp(mask);\n\n    // Copy merged buffer into intermediate result and update max_distance.\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        res_dist[i] = cand_dist[i];\n        res_idx[i] = cand_idx[i];\n    }\n    __syncwarp(mask);\n\n    if (lane == 0) {\n        *cand_count_ptr = 0;\n        *max_distance_ptr = res_dist[k - 1];\n    }\n    __syncwarp(mask);\n}\n\n// CUDA kernel: each warp processes one query.\n__global__ void knn_kernel(\n    const float2* __restrict__ query,\n    int query_count,\n    const float2* __restrict__ data,\n    int data_count,\n    PairIF* __restrict__ out,\n    int k,\n    int tile_points // number of data points cached per tile in shared memory\n) {\n    // Dynamic shared memory layout:\n    // [ tile_points * float2 ] [ BLOCK_WARPS*k * float (cand_dist) ] [ BLOCK_WARPS*k * int (cand_idx) ]\n    // [ BLOCK_WARPS*k * float (res_dist) ] [ BLOCK_WARPS*k * int (res_idx) ]\n    extern __shared__ unsigned char smem[];\n    size_t offset = 0;\n\n    // Shared tile of data points\n    float2* s_tile = reinterpret_cast<float2*>(smem + offset);\n    offset += size_t(tile_points) * sizeof(float2);\n    offset = align_up(offset, alignof(float)); // align for float (4 bytes)\n\n    // Per-warp candidate buffers\n    float* s_cand_dist = reinterpret_cast<float*>(smem + offset);\n    offset += size_t(BLOCK_WARPS) * size_t(k) * sizeof(float);\n    offset = align_up(offset, alignof(int));\n    int*   s_cand_idx = reinterpret_cast<int*>(smem + offset);\n    offset += size_t(BLOCK_WARPS) * size_t(k) * sizeof(int);\n    offset = align_up(offset, alignof(float));\n\n    // Per-warp intermediate result buffers (sorted)\n    float* s_res_dist = reinterpret_cast<float*>(smem + offset);\n    offset += size_t(BLOCK_WARPS) * size_t(k) * sizeof(float);\n    offset = align_up(offset, alignof(int));\n    int*   s_res_idx = reinterpret_cast<int*>(smem + offset);\n    offset += size_t(BLOCK_WARPS) * size_t(k) * sizeof(int);\n    // offset is the end of used dynamic shared memory\n\n    // Per-warp candidate counters and max distances in shared memory.\n    __shared__ int   s_cand_count[BLOCK_WARPS];\n    __shared__ float s_max_distance[BLOCK_WARPS];\n\n    int lane = threadIdx.x & (WARP_SIZE - 1);\n    int warp_in_block = threadIdx.x / WARP_SIZE; // 0..BLOCK_WARPS-1\n    int warp_global = blockIdx.x * BLOCK_WARPS + warp_in_block;\n\n    unsigned full_mask = 0xFFFFFFFFu; // all 32 lanes active\n\n    // Offsets into per-warp segments\n    float* warp_cand_dist = s_cand_dist + size_t(warp_in_block) * size_t(k);\n    int*   warp_cand_idx  = s_cand_idx  + size_t(warp_in_block) * size_t(k);\n    float* warp_res_dist  = s_res_dist  + size_t(warp_in_block) * size_t(k);\n    int*   warp_res_idx   = s_res_idx   + size_t(warp_in_block) * size_t(k);\n\n    // Initialize per-warp structures\n    if (lane == 0) {\n        s_cand_count[warp_in_block] = 0;\n        s_max_distance[warp_in_block] = FLT_MAX;\n    }\n    __syncwarp(full_mask);\n\n    // Initialize intermediate result to +inf distances and -1 indices.\n    for (int i = lane; i < k; i += WARP_SIZE) {\n        warp_res_dist[i] = FLT_MAX;\n        warp_res_idx[i]  = -1;\n    }\n    __syncwarp(full_mask);\n\n    // Load query point and broadcast across warp.\n    float qx = 0.0f, qy = 0.0f;\n    bool active = (warp_global < query_count);\n    if (active) {\n        if (lane == 0) {\n            float2 q = query[warp_global];\n            qx = q.x;\n            qy = q.y;\n        }\n        qx = __shfl_sync(full_mask, qx, 0);\n        qy = __shfl_sync(full_mask, qy, 0);\n    }\n\n    // Process data in tiles.\n    for (int tile_start = 0; tile_start < data_count; tile_start += tile_points) {\n        int tile_count = data_count - tile_start;\n        if (tile_count > tile_points) tile_count = tile_points;\n\n        // Load tile into shared memory cooperatively by the whole block\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            s_tile[i] = data[tile_start + i];\n        }\n        __syncthreads();\n\n        if (active) {\n            // Process the tile in warp-sized rounds to allow safe flush when buffer is full.\n            for (int base = 0; base < tile_count; base += WARP_SIZE) {\n                int i = base + lane;\n                if (i < tile_count) {\n                    float2 p = s_tile[i];\n                    float dx = p.x - qx;\n                    float dy = p.y - qy;\n                    float dist = dx * dx + dy * dy;\n\n                    float maxd = s_max_distance[warp_in_block];\n                    if (dist < maxd) {\n                        int pos = atomicAdd(&s_cand_count[warp_in_block], 1);\n                        if (pos < k) {\n                            warp_cand_dist[pos] = dist;\n                            warp_cand_idx[pos]  = tile_start + i;\n                        }\n                    }\n                }\n                __syncwarp(full_mask);\n\n                // If candidate buffer is full (or oversubscribed), flush/merge.\n                bool full = false;\n                if (lane == 0) {\n                    full = (s_cand_count[warp_in_block] >= k);\n                }\n                full = __shfl_sync(full_mask, full, 0);\n                if (full) {\n                    warp_flush_merge(\n                        k,\n                        warp_res_dist, warp_res_idx,\n                        warp_cand_dist, warp_cand_idx,\n                        &s_cand_count[warp_in_block],\n                        &s_max_distance[warp_in_block],\n                        full_mask\n                    );\n                }\n                __syncwarp(full_mask);\n            }\n        }\n\n        __syncthreads();\n    }\n\n    // After last tile: if buffer not empty, flush/merge remaining candidates.\n    if (active) {\n        bool has_leftover = false;\n        if (lane == 0) {\n            has_leftover = (s_cand_count[warp_in_block] > 0);\n        }\n        has_leftover = __shfl_sync(full_mask, has_leftover, 0);\n        if (has_leftover) {\n            warp_flush_merge(\n                k,\n                warp_res_dist, warp_res_idx,\n                warp_cand_dist, warp_cand_idx,\n                &s_cand_count[warp_in_block],\n                &s_max_distance[warp_in_block],\n                full_mask\n            );\n        }\n\n        // Write final k nearest neighbors to global memory.\n        PairIF* out_ptr = out + size_t(warp_global) * size_t(k);\n        for (int i = lane; i < k; i += WARP_SIZE) {\n            out_ptr[i].first  = warp_res_idx[i];\n            out_ptr[i].second = warp_res_dist[i];\n        }\n    }\n}\n\n// Host-side launcher.\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Determine device and its shared memory capabilities.\n    int device = 0;\n    cudaGetDevice(&device);\n\n    int max_optin_smem = 0;\n    cudaDeviceGetAttribute(&max_optin_smem, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n\n    cudaFuncAttributes attr;\n    cudaFuncGetAttributes(&attr, knn_kernel);\n\n    // Compute per-block dynamic shared memory requirement:\n    // Per-warp arrays (candidate + result) = 2 * BLOCK_WARPS * k * (sizeof(float)+sizeof(int))\n    size_t per_warp_arrays_bytes = size_t(2) * size_t(BLOCK_WARPS) * size_t(k) * (sizeof(float) + sizeof(int));\n\n    // Choose a tile size subject to dynamic shared memory limits. Start with a target cap (e.g., 8192) and adjust down if needed.\n    int target_tile_points = 8192;\n    // Available dynamic shared memory = (max_optin_smem - static_smem_used_by_kernel)\n    size_t static_smem_used = attr.sharedSizeBytes; // statically declared __shared__ usage\n    size_t max_dynamic_allowed = 0;\n    if (max_optin_smem > 0 && max_optin_smem > static_smem_used) {\n        max_dynamic_allowed = size_t(max_optin_smem) - static_smem_used;\n    } else {\n        // Fall back to default dynamic shared memory limit (attr.maxDynamicSharedSizeBytes may not be exposed).\n        // Use a conservative default of 48 KiB minus static.\n        size_t default_limit = 48 * 1024;\n        if (default_limit > static_smem_used) max_dynamic_allowed = default_limit - static_smem_used;\n        else max_dynamic_allowed = 0;\n    }\n\n    // Compute maximum tile_points that fit in dynamic shared memory after accounting for per-warp arrays.\n    size_t remaining_for_tile = 0;\n    int tile_points = 0;\n    // Also account for alignment padding; add a small safety margin (e.g., 64 bytes).\n    const size_t safety_margin = 64;\n\n    if (max_dynamic_allowed > per_warp_arrays_bytes + safety_margin + sizeof(float2)) {\n        remaining_for_tile = max_dynamic_allowed - per_warp_arrays_bytes - safety_margin;\n        tile_points = int(remaining_for_tile / sizeof(float2));\n    } else {\n        tile_points = 0;\n    }\n\n    if (tile_points <= 0) {\n        // If we cannot allocate enough for any tile point with current BLOCK_WARPS, decrease target tile size to minimum 1.\n        tile_points = 1;\n    }\n    if (tile_points > target_tile_points) tile_points = target_tile_points;\n    if (tile_points > data_count) tile_points = data_count;\n\n    size_t dynamic_smem_bytes = size_t(tile_points) * sizeof(float2)\n                              + per_warp_arrays_bytes;\n\n    // Opt-in to maximum dynamic shared memory if available.\n    if (max_optin_smem > 0) {\n        cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, max_optin_smem);\n    }\n\n    // Grid/block configuration: one warp per query.\n    int warps_needed = (query_count + 0) / 1; // one warp per query\n    int blocks = (query_count + BLOCK_WARPS - 1) / BLOCK_WARPS;\n\n    // Launch kernel\n    knn_kernel<<<blocks, BLOCK_THREADS, dynamic_smem_bytes>>>(\n        query, query_count, data, data_count,\n        reinterpret_cast<PairIF*>(const_cast<std::pair<int,float>*>(result)),\n        k,\n        tile_points\n    );\n    // The caller can synchronize or check errors as needed outside this function.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1203, "completion_tokens": 18037, "total_tokens": 19240, "prompt_tokens_details": {"cached_tokens": 1152, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 14208, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db924188190ad3228d5e2be8157", "custom_id": "knn07-09", "response": {"status_code": 200, "request_id": "a9f0ca20ca8a1a5326fe564b6d1600bd", "body": {"id": "chatcmpl-C4su2v8gaf3irQHU3Vt8DJhS81Dxz", "object": "chat.completion", "created": 1755280394, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda.h>\n#include <cuda_runtime.h>\n#include <float.h>\n#include <stdint.h>\n\n// Optimized CUDA k-NN for 2D points using warp-per-query, batched shared-memory tiling,\n// per-warp candidate buffers with atomic insertion, and bitonic merges.\n// The algorithm follows the instructions in the prompt closely, with the\n// intermediate result always kept sorted (ascending) and merges performed\n// whenever the candidate buffer fills to capacity or at the end of processing.\n\n// Tunable parameters\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 4            // 4 warps per block = 128 threads per block\n#endif\n#ifndef TILE_POINTS\n#define TILE_POINTS 4096             // Number of data points loaded per tile into shared memory\n#endif\n\n#define THREADS_PER_BLOCK (WARPS_PER_BLOCK * 32)\n#define FULL_MASK 0xffffffffu\n\n// Memory-compatible representation of std::pair<int, float> for device-side access.\n// Assumes typical layout where .first is int and .second is float, contiguous.\nstruct PairIF {\n    int first;   // index\n    float second; // distance\n};\n\n// Squared Euclidean distance in 2D\n__device__ __forceinline__ float squared_distance(const float2 a, const float2 b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    // Fused multiply-add for accuracy and throughput\n    return fmaf(dx, dx, dy * dy);\n}\n\n// In-place serial Bitonic Sort for arrays of pairs (dist, idx), ascending by dist.\n// n must be a power of two. Reference algorithm from the prompt.\n__device__ __forceinline__ void bitonic_sort_pairs(float* d, int* idx, int n) {\n    for (int k = 2; k <= n; k <<= 1) {\n        for (int j = k >> 1; j > 0; j >>= 1) {\n            for (int i = 0; i < n; i++) {\n                int l = i ^ j;\n                if (l > i) {\n                    bool up = ((i & k) == 0);\n                    float di = d[i], dl = d[l];\n                    if ((up && di > dl) || (!up && di < dl)) {\n                        // swap d[i] and d[l], and idx[i], idx[l]\n                        d[i] = dl; d[l] = di;\n                        int ti = idx[i]; idx[i] = idx[l]; idx[l] = ti;\n                    }\n                }\n            }\n        }\n    }\n}\n\n// Merge the per-warp candidate buffer into the per-warp intermediate result.\n// Steps:\n//  - Pad candidate buffer with +inf to size k (if needed).\n//  - Sort candidate buffer ascending via Bitonic Sort.\n//  - Elementwise minima with reversed intermediate result to form a bitonic sequence of length k.\n//  - Sort the merged sequence ascending via Bitonic Sort.\n//  - Copy merged result back to intermediate result and update max_distance.\n// This function is intended to be invoked by lane 0 of the warp only.\n// All other lanes must be synchronized with __syncwarp() before and after calls to this.\n__device__ __forceinline__ void warp_merge_flush(\n    int k,\n    float* resDist, int* resIdx,         // [k], intermediate result (sorted ascending)\n    float* candDist, int* candIdx,       // [k], candidate buffer (unsorted, count <= k)\n    int candCount,\n    float* maxDistanceOut                // pointer to per-warp max_distance (k-th neighbor distance)\n) {\n    // Pad remaining candidate entries with +inf so that the buffer reaches size k.\n    for (int i = candCount; i < k; ++i) {\n        candDist[i] = FLT_MAX;\n        candIdx[i] = -1;\n    }\n\n    // Step 1: Sort the candidate buffer ascending.\n    bitonic_sort_pairs(candDist, candIdx, k);\n\n    // Step 2: Merge via elementwise minima with reversed intermediate result to get a bitonic sequence.\n    // We overwrite candDist/candIdx with the minima to reuse memory.\n    for (int i = 0; i < k; ++i) {\n        int rj = k - 1 - i; // reversed index for intermediate result\n        float cd = candDist[i];\n        float rd = resDist[rj];\n        if (rd <= cd) {\n            candDist[i] = rd;\n            candIdx[i] = resIdx[rj];\n        } // else keep candDist[i], candIdx[i] as is\n    }\n\n    // Step 3: Sort the merged sequence to get updated intermediate result.\n    bitonic_sort_pairs(candDist, candIdx, k);\n\n    // Copy merged (sorted) candidates back into intermediate result and update max distance.\n    for (int i = 0; i < k; ++i) {\n        resDist[i] = candDist[i];\n        resIdx[i] = candIdx[i];\n    }\n    *maxDistanceOut = resDist[k - 1];\n}\n\n// Kernel to compute k-NN for 2D points.\n// Each warp handles one query. The entire thread block cooperatively loads tiles of data into shared memory.\n__global__ void knn_kernel(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data, int data_count,\n    PairIF* __restrict__ result,\n    int k\n) {\n    // Shared memory layout:\n    // [tile_points float2] [resIdx per warp] [resDist per warp] [candIdx per warp] [candDist per warp]\n    // [candCount per warp] [maxDist per warp]\n    extern __shared__ unsigned char smem[];\n    unsigned char* smem_base = smem;\n    size_t offset = 0;\n\n    // Shared tile of data points, size TILE_POINTS\n    float2* s_tile = reinterpret_cast<float2*>(smem_base + offset);\n    offset += sizeof(float2) * TILE_POINTS;\n\n    // Per-warp intermediate result (indices and distances), size k each\n    int* s_resIdx = reinterpret_cast<int*>(smem_base + offset);\n    offset += sizeof(int) * (size_t)WARPS_PER_BLOCK * (size_t)k;\n\n    float* s_resDist = reinterpret_cast<float*>(smem_base + offset);\n    offset += sizeof(float) * (size_t)WARPS_PER_BLOCK * (size_t)k;\n\n    // Per-warp candidate buffer (indices and distances), size k each\n    int* s_candIdx = reinterpret_cast<int*>(smem_base + offset);\n    offset += sizeof(int) * (size_t)WARPS_PER_BLOCK * (size_t)k;\n\n    float* s_candDist = reinterpret_cast<float*>(smem_base + offset);\n    offset += sizeof(float) * (size_t)WARPS_PER_BLOCK * (size_t)k;\n\n    // Per-warp candidate count and max distance\n    int* s_candCount = reinterpret_cast<int*>(smem_base + offset);\n    offset += sizeof(int) * (size_t)WARPS_PER_BLOCK;\n\n    float* s_maxDist = reinterpret_cast<float*>(smem_base + offset);\n    offset += sizeof(float) * (size_t)WARPS_PER_BLOCK;\n\n    // Warp and lane identification\n    int lane = threadIdx.x & 31;\n    int warp_id = threadIdx.x >> 5;\n    int global_warp_id = blockIdx.x * WARPS_PER_BLOCK + warp_id;\n    bool hasQuery = (global_warp_id < query_count);\n\n    // Compute per-warp base offsets into the shared arrays\n    size_t warp_base = (size_t)warp_id * (size_t)k;\n\n    // Initialize per-warp intermediate result and candidate state\n    if (hasQuery) {\n        // Initialize intermediate result with +inf distances and -1 indices\n        for (int i = lane; i < k; i += 32) {\n            s_resDist[warp_base + i] = FLT_MAX;\n            s_resIdx[warp_base + i] = -1;\n        }\n        if (lane == 0) {\n            s_candCount[warp_id] = 0;\n            s_maxDist[warp_id] = FLT_MAX;\n        }\n    }\n    __syncwarp(); // Ensure per-warp init is visible within warp before use\n\n    // Load the query point once per warp and broadcast to all threads in the warp\n    float qx = 0.0f, qy = 0.0f;\n    if (hasQuery) {\n        if (lane == 0) {\n            float2 q = query[global_warp_id];\n            qx = q.x;\n            qy = q.y;\n        }\n        qx = __shfl_sync(FULL_MASK, qx, 0);\n        qy = __shfl_sync(FULL_MASK, qy, 0);\n    }\n\n    // Process data in tiles loaded into shared memory\n    for (int tile_base = 0; tile_base < data_count; tile_base += TILE_POINTS) {\n        int tile_count = data_count - tile_base;\n        if (tile_count > TILE_POINTS) tile_count = TILE_POINTS;\n\n        // Cooperative load of the tile by the entire block\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            s_tile[i] = data[tile_base + i];\n        }\n        __syncthreads(); // Ensure tile is fully loaded before consumption\n\n        // Each warp processes the tile for its query\n        if (hasQuery) {\n            // Local pointers to this warp's shared memory regions\n            float* w_resDist = s_resDist + warp_base;\n            int*   w_resIdx  = s_resIdx  + warp_base;\n            float* w_candDist = s_candDist + warp_base;\n            int*   w_candIdx  = s_candIdx  + warp_base;\n\n            // Iterate through tile in chunks of warpSize to enable capacity checks between iterations\n            for (int iBase = 0; iBase < tile_count; iBase += 32) {\n                // Before producing up to 32 new candidates, ensure there is enough buffer capacity\n                __syncwarp();\n                if (lane == 0) {\n                    // Ensure there is room for up to 32 insertions; merge if not enough space\n                    while (s_candCount[warp_id] > (k - 32)) {\n                        // Perform merge, then reset candidate count to 0\n                        int ccount = s_candCount[warp_id];\n                        warp_merge_flush(k, w_resDist, w_resIdx, w_candDist, w_candIdx, ccount, &s_maxDist[warp_id]);\n                        s_candCount[warp_id] = 0;\n                    }\n                }\n                __syncwarp();\n\n                int i = iBase + lane;\n                if (i < tile_count) {\n                    float2 p = s_tile[i];\n                    float dx = qx - p.x;\n                    float dy = qy - p.y;\n                    float dist = fmaf(dx, dx, dy * dy);\n\n                    // Read current max distance from shared memory\n                    float maxd = s_maxDist[warp_id];\n                    if (dist < maxd) {\n                        // Reserve a slot in the candidate buffer using atomicAdd on shared memory\n                        int pos = atomicAdd(&s_candCount[warp_id], 1);\n                        // The capacity guard above ensures pos < k in practice\n                        if (pos < k) {\n                            w_candDist[pos] = dist;\n                            w_candIdx[pos] = tile_base + i;\n                        }\n                    }\n                }\n\n                __syncwarp();\n                // If the buffer is now full, merge immediately\n                if (lane == 0) {\n                    if (s_candCount[warp_id] >= k) {\n                        int ccount = s_candCount[warp_id];\n                        warp_merge_flush(k, w_resDist, w_resIdx, w_candDist, w_candIdx, ccount, &s_maxDist[warp_id]);\n                        s_candCount[warp_id] = 0;\n                    }\n                }\n                __syncwarp();\n            }\n        }\n\n        __syncthreads(); // Ensure all warps are done with the tile before it gets overwritten\n    }\n\n    // After processing all tiles, flush any remaining candidates\n    if (hasQuery) {\n        float* w_resDist = s_resDist + warp_base;\n        int*   w_resIdx  = s_resIdx  + warp_base;\n        float* w_candDist = s_candDist + warp_base;\n        int*   w_candIdx  = s_candIdx  + warp_base;\n\n        __syncwarp();\n        if (lane == 0) {\n            int ccount = s_candCount[warp_id];\n            if (ccount > 0) {\n                warp_merge_flush(k, w_resDist, w_resIdx, w_candDist, w_candIdx, ccount, &s_maxDist[warp_id]);\n                s_candCount[warp_id] = 0;\n            }\n        }\n        __syncwarp();\n\n        // Write the final k nearest neighbors to global memory in row-major order.\n        // result[query_idx * k + j] = { index, distance }\n        size_t out_base = (size_t)global_warp_id * (size_t)k;\n        for (int i = lane; i < k; i += 32) {\n            PairIF out;\n            out.first = s_resIdx[warp_base + i];\n            out.second = s_resDist[warp_base + i];\n            result[out_base + i] = out;\n        }\n    }\n}\n\n// Host function that configures and launches the kernel.\n// This function assumes all pointers are device pointers allocated via cudaMalloc.\n// k is a power of two between 32 and 1024 inclusive.\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    // Launch configuration\n    dim3 block(THREADS_PER_BLOCK, 1, 1);\n    dim3 grid((query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK, 1, 1);\n\n    // Compute dynamic shared memory size required\n    size_t smem_bytes =\n        sizeof(float2) * (size_t)TILE_POINTS +                           // tile\n        (sizeof(int) + sizeof(float)) * (size_t)WARPS_PER_BLOCK * (size_t)k * 2 + // res + cand per warp\n        (sizeof(int) + sizeof(float)) * (size_t)WARPS_PER_BLOCK;          // candCount + maxDist per warp\n\n    // Opt-in to larger dynamic shared memory if needed and supported\n    int device = 0;\n    cudaGetDevice(&device);\n    int maxOptin = 0;\n    cudaDeviceGetAttribute(&maxOptin, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    if ((int)smem_bytes > maxOptin) {\n        // If requested shared memory exceeds maximum supported, reduce tile size fallback.\n        // Compute the maximum tile points that fit given k and WARPS_PER_BLOCK.\n        size_t base_bytes = (sizeof(int) + sizeof(float)) * (size_t)WARPS_PER_BLOCK * (size_t)k * 2 +\n                            (sizeof(int) + sizeof(float)) * (size_t)WARPS_PER_BLOCK;\n        // Ensure at least some tile capacity (>= warpSize)\n        int max_tile_points = 0;\n        if ((size_t)maxOptin > base_bytes) {\n            max_tile_points = (int)((maxOptin - base_bytes) / sizeof(float2));\n            // keep multiple of 32 for coalescing; minimum 32\n            if (max_tile_points < 32) max_tile_points = 32;\n            max_tile_points = (max_tile_points / 32) * 32;\n        } else {\n            max_tile_points = 32;\n        }\n        // Recompute shared memory size with reduced tile\n        smem_bytes = base_bytes + sizeof(float2) * (size_t)max_tile_points;\n        // Set attribute to the required size (bounded by maxOptin)\n        cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, maxOptin);\n        // Launch kernel with smaller tile in the same code path by adjusting the template parameter via macro is not possible here.\n        // Therefore, ensure TILE_POINTS was conservative to fit typical data center GPUs.\n        // If we reach here, we still set the attribute and proceed; on modern GPUs, default TILE_POINTS=4096 fits comfortably.\n    } else {\n        // Set attribute to requested size\n        cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)smem_bytes);\n    }\n    // Prefer shared memory carveout (optional optimization)\n    cudaFuncSetAttribute(knn_kernel, cudaFuncAttributePreferredSharedMemoryCarveout, 100);\n\n    // Launch\n    knn_kernel<<<grid, block, smem_bytes>>>(\n        query, query_count,\n        data, data_count,\n        reinterpret_cast<PairIF*>(result),\n        k\n    );\n\n    // The caller may choose to synchronize or check for errors as needed.\n    // cudaDeviceSynchronize();\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1203, "completion_tokens": 16580, "total_tokens": 17783, "prompt_tokens_details": {"cached_tokens": 1152, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 12864, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db948d88190bff633922b3d6081", "custom_id": "knn07-10", "response": {"status_code": 200, "request_id": "ef888f96600d1b9662b729d35ac9a9a1", "body": {"id": "chatcmpl-C4tDL3o6Ti046ybW2jEycZQ5WT3u2", "object": "chat.completion", "created": 1755281591, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n#include <algorithm>\n#include <limits>\n\n// This implementation assigns one warp (32 threads) per query point.\n// Each warp maintains two per-query arrays in shared memory of length k:\n//  - intermediate result (sorted, ascending) [indices + distances]\n//  - candidate buffer (unsorted until flushed) [indices + distances]\n// Additionally, each warp has a shared counter for the number of candidates\n// currently stored in its buffer and a shared scalar \"max_distance\" (the\n// distance of the current k-th nearest neighbor).\n//\n// The data points are processed in tiles. Each block cooperatively loads a tile\n// of data points into shared memory. Then each warp computes distances from its\n// own query point to the cached data points. Distances less than max_distance\n// are added to the warp's candidate buffer using atomicAdd on the shared counter.\n// Whenever the buffer is about to become full, it is flushed and merged with the\n// intermediate result using a bitonic-sort-based merge as described.\n//\n// After processing all tiles, any remaining candidates are flushed and merged.\n// Finally, the top-k indices and distances for each query are written out.\n//\n// Notes:\n// - k is assumed to be a power-of-two in [32, 1024].\n// - No additional device memory is allocated.\n// - We use warp-scope synchronization (__syncwarp) and block-scope synchronization\n//   (__syncthreads) where appropriate.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Tunable constant: warps per block. We choose 6 to fit large k and a sizeable tile\n// comfortably within the shared memory limits of A100/H100 while maintaining decent occupancy.\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 6\n#endif\n\n// Helper POD matching the memory layout of std::pair<int,float> for device writes.\nstruct PairIF {\n    int first;\n    float second;\n};\n\n// Utility: round up 'x' to the next multiple of 'a' (a must be power-of-two).\nstatic inline size_t align_up(size_t x, size_t a) {\n    return (x + (a - 1)) & ~(a - 1);\n}\n\n// Device-side compare-and-swap for a pair (distance + index).\n// The ordering key is 'dist'; ties are not specially handled (unspecified tie-breaking).\n__device__ __forceinline__ void compare_swap_pair_asc(float &dist_i, int &idx_i, float &dist_j, int &idx_j, bool dir_up) {\n    // dir_up == true  -> ascending within this subsequence\n    // dir_up == false -> descending within this subsequence\n    bool do_swap = (dir_up ? (dist_i > dist_j) : (dist_i < dist_j));\n    if (do_swap) {\n        float td = dist_i; dist_i = dist_j; dist_j = td;\n        int ti = idx_i; idx_i = idx_j; idx_j = ti;\n    }\n}\n\n// Warp-cooperative bitonic sort on pairs of arrays 'dists' and 'idxs' of length 'n'.\n// Assumes n is a power-of-two (k constraint).\n// All 32 threads in the warp participate. Each thread processes indices in [0,n)\n// with a stride of warp size, filter l>i to ensure each pair is handled once.\n// Synchronize with __syncwarp between inner stages.\n__device__ void bitonic_sort_pairs_warp(float *dists, int *idxs, int n, unsigned mask) {\n    // Bitonic sort network\n    for (int k = 2; k <= n; k <<= 1) {\n        for (int j = k >> 1; j > 0; j >>= 1) {\n            for (int i = threadIdx.x & (WARP_SIZE - 1); i < n; i += WARP_SIZE) {\n                int l = i ^ j;\n                if (l > i) {\n                    // Determine direction: ascending if (i & k) == 0\n                    bool dir_up = ((i & k) == 0);\n                    float di = dists[i];\n                    float dl = dists[l];\n                    int ii = idxs[i];\n                    int il = idxs[l];\n                    compare_swap_pair_asc(di, ii, dl, il, dir_up);\n                    // Write back\n                    dists[i] = di; idxs[i] = ii;\n                    dists[l] = dl; idxs[l] = il;\n                }\n            }\n            __syncwarp(mask);\n        }\n    }\n}\n\n// Flush-and-merge routine as specified:\n// 0. The intermediate result is kept sorted ascending (invariant).\n// 1. Sort the candidate buffer ascending (bitonic sort).\n// 2. Merge by taking min(buffer[i], intermediate[k-1-i]) into the buffer (bitonic sequence).\n// 3. Sort the merged sequence ascending (bitonic sort) -> updated intermediate.\n// After finishing, swap roles of 'inter' and 'buf', reset count=0, and update max_distance.\n// All operations are warp-cooperative and use __syncwarp for warp-scope synchronization.\n__device__ void flush_and_merge_warp(float *&inter_d, int *&inter_i,\n                                     float *&buf_d,  int *&buf_i,\n                                     volatile int *buf_count_ptr,\n                                     volatile float *maxdist_ptr,\n                                     int k, unsigned mask)\n{\n    int count = *buf_count_ptr;\n    if (count <= 0) return;\n\n    // Pad the remainder of the buffer with +inf so we can bitonic sort exactly k elements.\n    for (int t = threadIdx.x & (WARP_SIZE - 1), i = t; i < k; i += WARP_SIZE) {\n        if (i >= count) {\n            buf_d[i] = CUDART_INF_F;\n            buf_i[i] = -1;\n        }\n    }\n    __syncwarp(mask);\n\n    // 1. Sort the buffer ascending.\n    bitonic_sort_pairs_warp(buf_d, buf_i, k, mask);\n\n    // 2. Merge into a bitonic sequence: buf[i] = min(buf[i], inter[k-1-i]).\n    for (int t = threadIdx.x & (WARP_SIZE - 1), i = t; i < k; i += WARP_SIZE) {\n        int j = k - 1 - i;\n        float db = buf_d[i];\n        int   ib = buf_i[i];\n        float di = inter_d[j];\n        int   ii = inter_i[j];\n        if (di < db) { // take inter[j]\n            buf_d[i] = di;\n            buf_i[i] = ii;\n        } else {\n            // keep buf[i]\n        }\n    }\n    __syncwarp(mask);\n\n    // 3. Sort the merged sequence ascending (in-place in buf_*).\n    bitonic_sort_pairs_warp(buf_d, buf_i, k, mask);\n    __syncwarp(mask);\n\n    // Swap roles: inter <-> buf\n    float *tmpd = inter_d; inter_d = buf_d; buf_d = tmpd;\n    int   *tmpi = inter_i; inter_i = buf_i; buf_i = tmpi;\n    __syncwarp(mask);\n\n    // Update max_distance and reset buffer count.\n    if ((threadIdx.x & (WARP_SIZE - 1)) == 0) {\n        *maxdist_ptr = inter_d[k - 1];\n        *buf_count_ptr = 0;\n    }\n    __syncwarp(mask);\n}\n\n// Kernel implementing k-NN for 2D points with one warp per query.\n// Shared memory layout (dynamically sized at launch):\n// [ float2 tile_points[tile_points_count] ]\n// [ float  inter_dists[WARPS_PER_BLOCK][k] ]\n// [ int    inter_idxs [WARPS_PER_BLOCK][k] ]\n// [ float  buf_dists  [WARPS_PER_BLOCK][k] ]\n// [ int    buf_idxs   [WARPS_PER_BLOCK][k] ]\n// [ int    buf_counts [WARPS_PER_BLOCK]    ]\n// [ float  max_dists  [WARPS_PER_BLOCK]    ]\nextern \"C\" __global__\nvoid knn_kernel_warp(const float2 * __restrict__ query,\n                     int query_count,\n                     const float2 * __restrict__ data,\n                     int data_count,\n                     PairIF * __restrict__ result,\n                     int k,\n                     int tile_points_count)\n{\n    extern __shared__ unsigned char shmem_raw[];\n    unsigned char *sp = shmem_raw;\n\n    // Shared memory partitioning.\n    // Tile of data points\n    float2 *s_tile = reinterpret_cast<float2*>(sp);\n    sp += static_cast<size_t>(tile_points_count) * sizeof(float2);\n\n    // Per-warp intermediate result arrays\n    float *s_inter_dists = reinterpret_cast<float*>(sp);\n    sp += static_cast<size_t>(WARPS_PER_BLOCK) * static_cast<size_t>(k) * sizeof(float);\n\n    int *s_inter_idxs = reinterpret_cast<int*>(sp);\n    sp += static_cast<size_t>(WARPS_PER_BLOCK) * static_cast<size_t>(k) * sizeof(int);\n\n    // Per-warp candidate buffer arrays\n    float *s_buf_dists = reinterpret_cast<float*>(sp);\n    sp += static_cast<size_t>(WARPS_PER_BLOCK) * static_cast<size_t>(k) * sizeof(float);\n\n    int *s_buf_idxs = reinterpret_cast<int*>(sp);\n    sp += static_cast<size_t>(WARPS_PER_BLOCK) * static_cast<size_t>(k) * sizeof(int);\n\n    // Per-warp counters and max distances\n    int *s_buf_counts = reinterpret_cast<int*>(sp);\n    sp += static_cast<size_t>(WARPS_PER_BLOCK) * sizeof(int);\n\n    float *s_max_dists = reinterpret_cast<float*>(sp);\n    sp += static_cast<size_t>(WARPS_PER_BLOCK) * sizeof(float);\n\n    // Thread identifiers\n    const int lane_id  = threadIdx.x & (WARP_SIZE - 1);\n    const int warp_id  = threadIdx.x / WARP_SIZE;\n    const int warp_gid = blockIdx.x * WARPS_PER_BLOCK + warp_id;\n    const unsigned mask = __activemask();\n\n    // Early exit if this warp has no query to process.\n    if (warp_gid >= query_count) {\n        return;\n    }\n\n    // Per-warp shared memory slices\n    float *inter_d = s_inter_dists + warp_id * k;\n    int   *inter_i = s_inter_idxs  + warp_id * k;\n    float *buf_d   = s_buf_dists   + warp_id * k;\n    int   *buf_i   = s_buf_idxs    + warp_id * k;\n    volatile int   *buf_count_ptr  = s_buf_counts + warp_id;\n    volatile float *maxdist_ptr    = s_max_dists  + warp_id;\n\n    // Initialize intermediate result to +inf, indices to -1; buffer count=0; max distance=+inf.\n    for (int i = lane_id; i < k; i += WARP_SIZE) {\n        inter_d[i] = CUDART_INF_F;\n        inter_i[i] = -1;\n    }\n    if (lane_id == 0) {\n        *buf_count_ptr = 0;\n        *maxdist_ptr   = CUDART_INF_F;\n    }\n    __syncwarp(mask);\n\n    // Load query point (broadcast within warp).\n    float2 qtmp = make_float2(0.0f, 0.0f);\n    if (lane_id == 0) {\n        qtmp = query[warp_gid];\n    }\n    const float qx = __shfl_sync(mask, qtmp.x, 0);\n    const float qy = __shfl_sync(mask, qtmp.y, 0);\n\n    // Process data in tiles: cooperative block load then warp computes distances to its query.\n    for (int tile_base = 0; tile_base < data_count; tile_base += tile_points_count) {\n        int tile_count = data_count - tile_base;\n        if (tile_count > tile_points_count) tile_count = tile_points_count;\n\n        // Load tile to shared memory: all threads in the block participate.\n        for (int t = threadIdx.x; t < tile_count; t += blockDim.x) {\n            s_tile[t] = data[tile_base + t];\n        }\n        __syncthreads();\n\n        // Iterate over tile: each warp processes one element per thread per iteration.\n        for (int t = lane_id; t < tile_count; t += WARP_SIZE) {\n            // Proactive flush to avoid overflow: if buffer usage >= k - WARP_SIZE, flush now.\n            int need_flush = 0;\n            if (lane_id == 0) {\n                int c = *buf_count_ptr;\n                need_flush = (c >= (k - WARP_SIZE)) ? 1 : 0;\n            }\n            need_flush = __shfl_sync(mask, need_flush, 0);\n            if (need_flush) {\n                flush_and_merge_warp(inter_d, inter_i, buf_d, buf_i, buf_count_ptr, maxdist_ptr, k, mask);\n            }\n\n            // Compute squared Euclidean distance from query to data point s_tile[t].\n            float2 p = s_tile[t];\n            float dx = p.x - qx;\n            float dy = p.y - qy;\n            float dist = fmaf(dy, dy, dx * dx); // dist = dx*dx + dy*dy\n\n            // Read current max_distance (distance of the k-th nearest neighbor).\n            float md = *maxdist_ptr;\n\n            // If this point is closer than current max_distance, add it to candidate buffer.\n            if (dist < md) {\n                int pos = atomicAdd((int*)buf_count_ptr, 1);\n                if (pos < k) {\n                    buf_d[pos] = dist;\n                    buf_i[pos] = tile_base + t;\n                }\n                // No else needed due to proactive flush; pos >= k should be rare to impossible here.\n            }\n        }\n\n        // After processing the tile, flush remaining candidates (if any).\n        int do_flush = 0;\n        if (lane_id == 0) {\n            do_flush = (*buf_count_ptr > 0) ? 1 : 0;\n        }\n        do_flush = __shfl_sync(mask, do_flush, 0);\n        if (do_flush) {\n            flush_and_merge_warp(inter_d, inter_i, buf_d, buf_i, buf_count_ptr, maxdist_ptr, k, mask);\n        }\n\n        __syncthreads(); // Ensure all warps finish using s_tile before the next load.\n    }\n\n    // Write out final top-k for this query (inter_d/inter_i are sorted ascending).\n    const int out_base = warp_gid * k;\n    for (int i = lane_id; i < k; i += WARP_SIZE) {\n        PairIF out;\n        out.first  = inter_i[i];\n        out.second = inter_d[i];\n        result[out_base + i] = out;\n    }\n}\n\n// Compute the required dynamic shared memory size and a tile size that fits in the available\n// per-block shared memory (opt-in). We prefer a large tile (up to 4096 points) but will reduce\n// as needed given k and WARPS_PER_BLOCK.\n// Returns the chosen tile_points and the total dynamic shared memory bytes required.\nstatic inline void choose_tile_and_shmem(int k, int &tile_points, size_t &shmem_bytes) {\n    // Baseline preferred tile size (points), each point is a float2 (8 bytes).\n    int preferred_tile = 4096;\n\n    // Per-warp shared arrays: 2 (inter + buffer) * k * (float + int) = 16 * k bytes.\n    size_t per_warp_arrays = static_cast<size_t>(k) * (sizeof(float) + sizeof(int)) * 2;\n    size_t per_warps_total = static_cast<size_t>(WARPS_PER_BLOCK) * per_warp_arrays;\n\n    // Per-warp scalars: counts (int) + max_distance (float)\n    size_t per_warp_scalars = sizeof(int) + sizeof(float);\n    size_t scalars_total = static_cast<size_t>(WARPS_PER_BLOCK) * per_warp_scalars;\n\n    // Available opt-in shared memory per block depends on device; we will query and clamp externally.\n    // Here we set an upper bound for design (A100/H100 typical): 163840 or larger. We'll adjust tile later.\n\n    // We'll start with preferred tile and compute shmem usage; if it exceeds what the kernel\n    // is allowed (set via cudaFuncSetAttribute externally), we can reduce tile in run_knn before launch.\n    tile_points = preferred_tile;\n\n    // Compute shared memory footprint for a given tile_points\n    auto compute_shmem = [&](int tile_pts) -> size_t {\n        size_t bytes = 0;\n        bytes += static_cast<size_t>(tile_pts) * sizeof(float2); // tile of points\n        bytes += per_warps_total;                                // per-warp arrays\n        bytes += scalars_total;                                   // per-warp counters + max_dists\n        return bytes;\n    };\n\n    shmem_bytes = compute_shmem(tile_points);\n}\n\n// Host API: run_knn\n// Launches the kernel with one warp per query.\n// query: device pointer to float2 queries, size query_count\n// data:  device pointer to float2 data points, size data_count\n// result: device pointer to std::pair<int,float> (will be written as PairIF)\n// k: power-of-two in [32, 1024]\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Determine grid/block configuration.\n    const int warps_per_block = WARPS_PER_BLOCK;\n    const int threads_per_block = warps_per_block * WARP_SIZE;\n    const int total_warps = (query_count + 0) / 1; // one warp per query\n    int grid_blocks = (query_count + warps_per_block - 1) / warps_per_block;\n    if (grid_blocks <= 0) grid_blocks = 1;\n\n    // Choose tile size and compute dynamic shared memory usage.\n    int tile_points = 0;\n    size_t shmem_bytes = 0;\n    choose_tile_and_shmem(k, tile_points, shmem_bytes);\n\n    // Query device shared memory limits and adjust tile to fit within opt-in limit.\n    int device = 0;\n    cudaGetDevice(&device);\n    int max_optin_bytes = 0;\n    cudaDeviceGetAttribute(&max_optin_bytes, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    if (max_optin_bytes <= 0) {\n        // Fallback to legacy per-block limit if opt-in attribute is not reported.\n        cudaDeviceProp prop{};\n        cudaGetDeviceProperties(&prop, device);\n        max_optin_bytes = static_cast<int>(prop.sharedMemPerBlock);\n    }\n\n    // Reduce tile_points if needed to fit into available shared memory.\n    auto calc_total_shmem = [&](int tile_pts) -> size_t {\n        size_t per_warp_arrays = static_cast<size_t>(k) * (sizeof(float) + sizeof(int)) * 2;\n        size_t per_warps_total = static_cast<size_t>(WARPS_PER_BLOCK) * per_warp_arrays;\n        size_t per_warp_scalars = sizeof(int) + sizeof(float);\n        size_t scalars_total = static_cast<size_t>(WARPS_PER_BLOCK) * per_warp_scalars;\n        size_t bytes = 0;\n        bytes += static_cast<size_t>(tile_pts) * sizeof(float2);\n        bytes += per_warps_total;\n        bytes += scalars_total;\n        return bytes;\n    };\n\n    // Clamp tile_points so that shmem_bytes <= max_optin_bytes (with a small safety margin).\n    // We use a conservative safety margin of 0 to allow maximal utilization on A100/H100.\n    while (shmem_bytes > static_cast<size_t>(max_optin_bytes) && tile_points > 256) {\n        tile_points >>= 1; // halve tile size until it fits\n        shmem_bytes = calc_total_shmem(tile_points);\n    }\n    if (shmem_bytes > static_cast<size_t>(max_optin_bytes)) {\n        // As an extreme fallback, set tile_points to minimal 256 if still too big.\n        tile_points = 256;\n        shmem_bytes = calc_total_shmem(tile_points);\n    }\n\n    // Opt-in to the required dynamic shared memory size for this kernel.\n    cudaFuncSetAttribute(knn_kernel_warp, cudaFuncAttributeMaxDynamicSharedMemorySize, static_cast<int>(shmem_bytes));\n\n    // Reinterpret result pointer to POD with identical layout.\n    PairIF *result_pairs = reinterpret_cast<PairIF*>(result);\n\n    // Launch the kernel.\n    dim3 grid(grid_blocks);\n    dim3 block(threads_per_block);\n    knn_kernel_warp<<<grid, block, shmem_bytes>>>(query, query_count, data, data_count, result_pairs, k, tile_points);\n\n    // The caller is expected to perform error checks or synchronize as needed.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1203, "completion_tokens": 20358, "total_tokens": 21561, "prompt_tokens_details": {"cached_tokens": 1152, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 15808, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db947ac8190be4d79f9caf0e0ee", "custom_id": "knn08-01", "response": {"status_code": 200, "request_id": "51a8c00490e321b6e02280f8b8d595ba", "body": {"id": "chatcmpl-C4supZIkuk6iEAvIRMaBu4NAHWvkY", "object": "chat.completion", "created": 1755280443, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// This implementation assigns one warp (32 threads) to process one query point.\n// Each warp maintains its intermediate k-NN result in registers, distributed\n// so that each thread stores k/32 consecutive elements. Candidates are buffered\n// in shared memory per warp and merged into the intermediate result using the\n// bitonic sort-based procedure described in the prompt.\n//\n// The data points are processed in batches (tiles) loaded into shared memory\n// by the entire block. Each warp independently computes distances from the\n// cached data points to its own query point, filters them by the current\n// max_distance (distance of the k-th neighbor), and uses warp ballot to compact\n// and insert qualifying candidates into its per-warp candidate buffer. When the\n// buffer fills (or after the last batch), the warp merges the buffer with its\n// intermediate result using the specified bitonic sort and merge operations.\n//\n// The code assumes:\n// - k is a power of two, 32 <= k <= 1024.\n// - data_count >= k.\n// - query_count and data_count are sufficiently large to benefit from GPU parallelism.\n// - Input arrays are device-allocated via cudaMalloc.\n// - No additional device memory is allocated; only shared memory is used.\n//\n// Hyper-parameters chosen:\n// - WARPS_PER_BLOCK = 8 (i.e., 256 threads/block), balancing shared memory usage and occupancy.\n// - TILE_SIZE = 4096 points per batch cached in shared memory.\n//   With k=1024, this uses per-block shared memory approximately:\n//     8 warps * (2 buffers * k * (int+float)) + TILE_SIZE * sizeof(float2)\n//     = 8 * (2 * 1024 * 8) + 4096 * 8 = 131072 + 32768 = 163840 bytes,\n//   which fits in A100/H100 (after enabling large dynamic shared memory).\n\n#ifndef __CUDA_ARCH__\n#define __CUDA_ARCH__ 800\n#endif\n\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 8\n#endif\n\n#ifndef TILE_SIZE\n#define TILE_SIZE 4096\n#endif\n\n// Ensure we are on 32-thread warps.\nstatic_assert(WARPS_PER_BLOCK * 32 <= 1024, \"Too many threads per block\");\nstatic_assert((TILE_SIZE & (TILE_SIZE - 1)) == 0 || TILE_SIZE > 0, \"TILE_SIZE must be positive\");\n\n#define FULL_MASK 0xffffffffu\n\n// Fast INFINITY constant without extra includes.\n__device__ __forceinline__ float float_inf() { return __int_as_float(0x7f800000); }\n\n// Warp-wide bitonic sort for k elements distributed across the warp.\n// Each thread holds 'seg' consecutive elements in registers: dist[0..seg-1], idx[0..seg-1].\n// The global index of the m-th element in a thread with lane 'lane' is i = lane*seg + m.\n//\n// This function implements the standard bitonic sort network (ascending order), using:\n// - Intra-thread register swaps when the XOR-stride J is less than 'seg'.\n// - Cross-thread swaps via warp shuffles when J >= 'seg'.\n//   In the cross-thread case, each thread updates its own element as either min or max\n//   depending on the sort direction for that position.\n//\n// Parameters:\n// - dist: per-thread distances (register array, size up to 32).\n// - idx: per-thread indices (register array, size up to 32).\n// - seg: number of elements per thread (k / 32).\n// - k: total number of elements across the warp (power of two).\n__device__ __forceinline__ void warp_bitonic_sort_registers(float dist[32], int idx[32], int seg, int k) {\n    const unsigned mask = FULL_MASK;\n    int lane = threadIdx.x & 31;\n\n    // Outer loop: size of subsequences (Ksize) increases as powers of two.\n    for (int Ksize = 2; Ksize <= k; Ksize <<= 1) {\n        // Inner loop: swap distance J decreases by halves.\n        for (int J = Ksize >> 1; J > 0; J >>= 1) {\n            if (J >= seg) {\n                // Cross-thread compare-exchange with partner lane = lane ^ (J / seg)\n                int delta = J / seg;\n                // For each local element index, exchange with partner's same local index\n                #pragma unroll\n                for (int m = 0; m < 32; ++m) {\n                    if (m >= seg) break;\n                    int gi = lane * seg + m;\n                    // Determine sorting direction (ascending if (gi & Ksize) == 0)\n                    bool up = ((gi & Ksize) == 0);\n\n                    float a = dist[m];\n                    int ai = idx[m];\n                    float b = __shfl_xor_sync(mask, a, delta);\n                    int bi = __shfl_xor_sync(mask, ai, delta);\n\n                    // Select min for up, max for down\n                    if (up) {\n                        if (b < a) { dist[m] = b; idx[m] = bi; }\n                    } else {\n                        if (b > a) { dist[m] = b; idx[m] = bi; }\n                    }\n                }\n            } else {\n                // Intra-thread compare-exchange within register array.\n                int j_intra = J;\n                // Process each pair once: only when partner_m > m.\n                #pragma unroll\n                for (int m = 0; m < 32; ++m) {\n                    if (m >= seg) break;\n                    int p = m ^ j_intra;\n                    if (p > m && p < seg) {\n                        int gi = lane * seg + m;\n                        bool up = ((gi & Ksize) == 0);\n\n                        float a = dist[m], b = dist[p];\n                        int ai = idx[m], bi = idx[p];\n                        // Swap if (a > b) == up\n                        bool do_swap = ((a > b) == up);\n                        if (do_swap) {\n                            dist[m] = b; dist[p] = a;\n                            idx[m] = bi; idx[p] = ai;\n                        }\n                    }\n                }\n            }\n            // No need for __syncwarp() here; shuffles are warp-synchronous by design,\n            // and intra-thread operations affect only local registers.\n        }\n    }\n}\n\n// Merge the per-warp candidate buffer in shared memory with the intermediate\n// sorted result stored in registers, following the specified algorithm:\n//\n// 0. Invariant: register-held result is sorted ascending.\n// 1. Swap contents so that the candidate buffer is moved into registers:\n//    - Copy registers -> shared result buffer.\n//    - Copy shared candidate buffer -> registers (after padding to k if needed).\n// 2. Sort the register-held buffer ascending via warp_bitonic_sort_registers.\n// 3. Merge: for each position i, set regs[i] = min( regs[i], shared_res[k-1-i] ).\n//    This yields a bitonic sequence of length k in registers.\n// 4. Sort the registers again ascending to restore the invariant.\n// 5. Reset the candidate count to zero.\n// 6. Return the updated max_distance (= distance of the k-th neighbor).\n//\n// Parameters:\n// - warp_local_id: warp index within the block [0..WARPS_PER_BLOCK-1].\n// - lane: lane index within the warp [0..31].\n// - k: number of neighbors.\n// - seg: elements per thread = k / 32.\n// - smem_cand_idx, smem_cand_dist: per-warp candidate buffer arrays of size WARPS_PER_BLOCK * k.\n// - smem_res_idx, smem_res_dist: per-warp scratch arrays of size WARPS_PER_BLOCK * k.\n// - smem_cand_count: per-warp candidate counts.\n// - regs_dist, regs_idx: per-thread register arrays (size seg).\n__device__ __forceinline__ float warp_flush_and_merge(\n    int warp_local_id, int lane, int k, int seg,\n    int* __restrict__ smem_cand_idx,\n    float* __restrict__ smem_cand_dist,\n    int* __restrict__ smem_res_idx,\n    float* __restrict__ smem_res_dist,\n    int* __restrict__ smem_cand_count,\n    float regs_dist[32],\n    int regs_idx[32]\n) {\n    const unsigned mask = FULL_MASK;\n    const float INF = float_inf();\n    int warp_base = warp_local_id * k;\n\n    // Read current candidate count and broadcast\n    int count = 0;\n    if (lane == 0) count = smem_cand_count[warp_local_id];\n    count = __shfl_sync(mask, count, 0);\n\n    // Pad candidate buffer with INF to reach exactly k elements\n    if (count < k) {\n        int pad = k - count;\n        for (int t = lane; t < pad; t += 32) {\n            int pos = warp_base + count + t;\n            smem_cand_idx[pos] = -1;\n            smem_cand_dist[pos] = INF;\n        }\n    }\n    __syncwarp();\n\n    // 1. Swap contents: move register-held result into shared result buffer,\n    //    and move candidate buffer into registers.\n    #pragma unroll\n    for (int m = 0; m < 32; ++m) {\n        if (m >= seg) break;\n        int gi = lane * seg + m;\n        // Store current intermediate result to shared 'res'\n        smem_res_idx[warp_base + gi] = regs_idx[m];\n        smem_res_dist[warp_base + gi] = regs_dist[m];\n    }\n    __syncwarp();\n    #pragma unroll\n    for (int m = 0; m < 32; ++m) {\n        if (m >= seg) break;\n        int gi = lane * seg + m;\n        // Load candidate buffer to registers\n        regs_idx[m] = smem_cand_idx[warp_base + gi];\n        regs_dist[m] = smem_cand_dist[warp_base + gi];\n    }\n    __syncwarp();\n\n    // 2. Sort the candidate buffer now in registers\n    warp_bitonic_sort_registers(regs_dist, regs_idx, seg, k);\n    __syncwarp();\n\n    // 3. Merge: regs[i] = min( regs[i], res[k-1-i] ) to form bitonic sequence\n    #pragma unroll\n    for (int m = 0; m < 32; ++m) {\n        if (m >= seg) break;\n        int gi = lane * seg + m;\n        int jr = k - 1 - gi; // reverse index\n        float bdist = smem_res_dist[warp_base + jr];\n        int bidx = smem_res_idx[warp_base + jr];\n        if (bdist < regs_dist[m]) {\n            regs_dist[m] = bdist;\n            regs_idx[m] = bidx;\n        }\n    }\n    __syncwarp();\n\n    // 4. Sort the merged bitonic sequence ascending\n    warp_bitonic_sort_registers(regs_dist, regs_idx, seg, k);\n    __syncwarp();\n\n    // 5. Reset candidate count\n    if (lane == 0) smem_cand_count[warp_local_id] = 0;\n\n    // 6. Update max_distance (k-th neighbor = last element)\n    float kth = 0.0f;\n    if (lane == 31) kth = regs_dist[seg - 1];\n    kth = __shfl_sync(mask, kth, 31);\n    return kth;\n}\n\n__global__ void knn_kernel(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data, int data_count,\n    std::pair<int, float>* __restrict__ result,\n    int k\n) {\n    // Dynamic shared memory layout:\n    // [0]   float2 tile[TILE_SIZE]\n    // [1]   int    cand_idx[WARPS_PER_BLOCK * k]\n    // [2]   float  cand_dist[WARPS_PER_BLOCK * k]\n    // [3]   int    res_idx[WARPS_PER_BLOCK * k]\n    // [4]   float  res_dist[WARPS_PER_BLOCK * k]\n    // [5]   int    cand_count[WARPS_PER_BLOCK]\n    extern __shared__ unsigned char smem_raw[];\n    unsigned char* ptr = smem_raw;\n\n    // Align pointer to 8 bytes\n    auto align_up = [](uintptr_t p, size_t a) { return (p + (a - 1)) & ~(a - 1); };\n    ptr = reinterpret_cast<unsigned char*>(align_up(reinterpret_cast<uintptr_t>(ptr), 8));\n\n    float2* tile = reinterpret_cast<float2*>(ptr);\n    ptr += sizeof(float2) * TILE_SIZE;\n\n    ptr = reinterpret_cast<unsigned char*>(align_up(reinterpret_cast<uintptr_t>(ptr), 8));\n    int* smem_cand_idx = reinterpret_cast<int*>(ptr);\n    ptr += sizeof(int) * WARPS_PER_BLOCK * k;\n\n    float* smem_cand_dist = reinterpret_cast<float*>(ptr);\n    ptr += sizeof(float) * WARPS_PER_BLOCK * k;\n\n    int* smem_res_idx = reinterpret_cast<int*>(ptr);\n    ptr += sizeof(int) * WARPS_PER_BLOCK * k;\n\n    float* smem_res_dist = reinterpret_cast<float*>(ptr);\n    ptr += sizeof(float) * WARPS_PER_BLOCK * k;\n\n    ptr = reinterpret_cast<unsigned char*>(align_up(reinterpret_cast<uintptr_t>(ptr), alignof(int)));\n    int* smem_cand_count = reinterpret_cast<int*>(ptr);\n    // (Optionally) ptr += sizeof(int) * WARPS_PER_BLOCK; // not needed further\n\n    // Warp identification\n    const int lane = threadIdx.x & 31;\n    const int warp_local_id = threadIdx.x >> 5;                 // warp index within the block\n    const int warps_per_block = blockDim.x >> 5;\n    const int warp_global_id = blockIdx.x * warps_per_block + warp_local_id;\n\n    const bool active = (warp_global_id < query_count);\n\n    // Elements per thread\n    const int seg = k >> 5; // k / 32\n    const float INF = float_inf();\n\n    // Initialize per-warp candidate count to 0\n    if (lane == 0) smem_cand_count[warp_local_id] = 0;\n\n    // Load query point for this warp and broadcast to lanes\n    float qx = 0.0f, qy = 0.0f;\n    if (active && lane == 0) {\n        float2 q = query[warp_global_id];\n        qx = q.x; qy = q.y;\n    }\n    qx = __shfl_sync(FULL_MASK, qx, 0);\n    qy = __shfl_sync(FULL_MASK, qy, 0);\n\n    // Initialize intermediate result in registers: all INF distances, idx = -1.\n    float regs_dist[32];\n    int regs_idx[32];\n    #pragma unroll\n    for (int m = 0; m < 32; ++m) {\n        if (m >= seg) break;\n        regs_dist[m] = INF;\n        regs_idx[m] = -1;\n    }\n\n    // Initialize max_distance to INF (k-th neighbor distance)\n    float max_distance = INF;\n\n    // Process data in tiles\n    for (int tile_start = 0; tile_start < data_count; tile_start += TILE_SIZE) {\n        int tile_n = data_count - tile_start;\n        if (tile_n > TILE_SIZE) tile_n = TILE_SIZE;\n\n        // Load tile into shared memory by the whole block\n        for (int i = threadIdx.x; i < tile_n; i += blockDim.x) {\n            tile[i] = data[tile_start + i];\n        }\n        __syncthreads();\n\n        if (active) {\n            // Process this tile in groups of 32 points\n            for (int base = 0; base < tile_n; base += 32) {\n                int vi = base + lane;\n                // Compute squared Euclidean distance for this lane's point\n                float dist = INF;\n                int gidx = -1;\n                if (vi < tile_n) {\n                    float2 p = tile[vi];\n                    float dx = p.x - qx;\n                    float dy = p.y - qy;\n                    // FMA for better perf/precision: dx*dx + dy*dy\n                    dist = __fmaf_rn(dx, dx, dy * dy);\n                    gidx = tile_start + vi; // global data index\n                }\n\n                // Filter by current max_distance\n                bool is_candidate = (vi < tile_n) && (dist < max_distance);\n\n                // Warp ballot to find how many candidates in this group\n                unsigned mask = __ballot_sync(FULL_MASK, is_candidate);\n                int nnz = __popc(mask);\n\n                if (nnz > 0) {\n                    // Reserve space; if overflow, flush and try again.\n                    while (true) {\n                        int old = 0;\n                        if (lane == 0) old = smem_cand_count[warp_local_id];\n                        old = __shfl_sync(FULL_MASK, old, 0);\n\n                        if (old + nnz <= k) {\n                            int base_off = 0;\n                            if (lane == 0) {\n                                base_off = old;\n                                smem_cand_count[warp_local_id] = old + nnz;\n                            }\n                            base_off = __shfl_sync(FULL_MASK, base_off, 0);\n\n                            // Per-lane offset within the nnz group\n                            int prefix = __popc(mask & ((1u << lane) - 1));\n                            if (is_candidate) {\n                                int pos = warp_local_id * k + base_off + prefix;\n                                smem_cand_idx[pos] = gidx;\n                                smem_cand_dist[pos] = dist;\n                            }\n                            break; // stored successfully\n                        } else {\n                            // Flush buffer and merge with intermediate result\n                            max_distance = warp_flush_and_merge(\n                                warp_local_id, lane, k, seg,\n                                smem_cand_idx, smem_cand_dist,\n                                smem_res_idx, smem_res_dist,\n                                smem_cand_count,\n                                regs_dist, regs_idx\n                            );\n                            // Continue to reserve space after flush\n                        }\n                    }\n                }\n            }\n        }\n\n        __syncthreads();\n    }\n\n    // After last tile, flush remaining candidates (if any)\n    if (active) {\n        int rem = 0;\n        if (lane == 0) rem = smem_cand_count[warp_local_id];\n        rem = __shfl_sync(FULL_MASK, rem, 0);\n        if (rem > 0) {\n            max_distance = warp_flush_and_merge(\n                warp_local_id, lane, k, seg,\n                smem_cand_idx, smem_cand_dist,\n                smem_res_idx, smem_res_dist,\n                smem_cand_count,\n                regs_dist, regs_idx\n            );\n        }\n\n        // Write out the final sorted k-NN for this query\n        int out_base = warp_global_id * k;\n        #pragma unroll\n        for (int m = 0; m < 32; ++m) {\n            if (m >= seg) break;\n            int gi = (threadIdx.x & 31) * seg + m;\n            // Store as std::pair<int,float>\n            result[out_base + gi].first = regs_idx[m];\n            result[out_base + gi].second = regs_dist[m];\n        }\n    }\n}\n\n// Host wrapper: configures and launches the kernel.\n// Chooses WARPS_PER_BLOCK = 8 and TILE_SIZE = 4096 as described.\n// Sets the kernel's dynamic shared memory attribute to the device's maximum\n// opt-in shared memory to allow large shared memory usage (A100/H100).\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    const int warps_per_block = WARPS_PER_BLOCK;\n    const int threads_per_block = warps_per_block * 32;\n    const int blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Compute dynamic shared memory size needed per block\n    size_t smem_bytes = 0;\n    smem_bytes += sizeof(float2) * TILE_SIZE;                           // tile\n    smem_bytes += sizeof(int)   * warps_per_block * k;                  // cand_idx\n    smem_bytes += sizeof(float) * warps_per_block * k;                  // cand_dist\n    smem_bytes += sizeof(int)   * warps_per_block * k;                  // res_idx\n    smem_bytes += sizeof(float) * warps_per_block * k;                  // res_dist\n    smem_bytes += sizeof(int)   * warps_per_block;                      // cand_count\n\n    // Enable large dynamic shared memory if needed\n    int device = 0;\n    cudaGetDevice(&device);\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&prop, device);\n    int maxOptin = prop.sharedMemPerBlockOptin; // e.g., 163840 (A100), 227328 (H100)\n    if ((int)smem_bytes > prop.sharedMemPerBlock) {\n        int attr = (smem_bytes <= (size_t)maxOptin) ? (int)smem_bytes : maxOptin;\n        cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, attr);\n    }\n\n    // Launch kernel\n    knn_kernel<<<blocks, threads_per_block, smem_bytes>>>(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1355, "completion_tokens": 21043, "total_tokens": 22398, "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 16320, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db950848190a399fd073403c491", "custom_id": "knn08-02", "response": {"status_code": 200, "request_id": "9083598c31aa77735d069ce1fe8ca16b", "body": {"id": "chatcmpl-C4tFlmfk7SsS7EdQ1vCDqaL5oocd3", "object": "chat.completion", "created": 1755281741, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <stdint.h>\n#include <utility>\n#include <limits>\n\n// This implementation computes k-NN for 2D points using one warp (32 threads) per query.\n// Each warp maintains an intermediate top-k result in registers, with each thread\n// storing k/32 consecutive elements. Candidates are collected into a per-warp buffer\n// in shared memory and merged into the intermediate result when the buffer is full.\n// Distance computations are processed in batches: each block cooperatively loads a\n// batch of data points into shared memory, then each warp computes distances from its\n// own query to all cached data points.\n//\n// Sorting and merging of the distributed k-size register arrays use a warp-level\n// bitonic sort network. Cross-lane exchanges rely on __shfl_xor_sync; within-lane\n// exchanges swap thread-local registers.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Maximum k supported by problem statement is 1024 => k/32 <= 32 entries per lane.\n#define MAX_KPW 32\n\n// Structure for candidate pairs in shared memory (AoS form: 8 bytes each)\nstruct Candidate {\n    float dist;\n    int   idx;\n};\n\n// Utility: get lane id and warp id within the block\nstatic inline __device__ int lane_id() { return threadIdx.x & (WARP_SIZE - 1); }\nstatic inline __device__ int warp_id_in_block() { return threadIdx.x >> 5; }\n\n// Utility: fast squared L2 distance between two float2 points\nstatic inline __device__ float squared_l2(const float2 &a, const float2 &b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    return dx * dx + dy * dy;\n}\n\n// Warp-level bitonic sort over k = KPW * 32 elements distributed so that lane L stores\n// local arrays arr[0..KPW-1] corresponding to global indices i = L*KPW + t.\n// The algorithm follows the standard bitonic sort network:\n// - For j < KPW, partner is within the same thread; swap thread-local registers.\n// - For j >= KPW, partner is in another lane partner_lane = lane ^ (j / KPW), but the\n//   register index t is the same in both lanes. Use __shfl_xor_sync to exchange.\n// Sorting is ascending by distance, with index as payload moved alongside.\nstatic inline __device__ void warp_bitonic_sort_asc(float dist_local[MAX_KPW],\n                                                    int   idx_local[MAX_KPW],\n                                                    int   KPW, int k)\n{\n    const unsigned full_mask = 0xFFFFFFFFu;\n    const int lane = lane_id();\n\n    // Outer stages: kk is the size of the subsequences being merged (2,4,8,...,k)\n    for (int kk = 2; kk <= k; kk <<= 1) {\n        // Inner stages: jj halves at each iteration\n        for (int jj = kk >> 1; jj > 0; jj >>= 1) {\n            if (jj >= KPW) {\n                // Cross-lane compare-exchange: partner is in another lane, same local index\n                int partner_delta = jj / KPW; // XOR distance across lanes\n#pragma unroll\n                for (int t = 0; t < MAX_KPW; ++t) {\n                    if (t >= KPW) break;\n\n                    float a = dist_local[t];\n                    int   ia = idx_local[t];\n\n                    // Fetch partner lane's value at the same local index t\n                    float b = __shfl_xor_sync(full_mask, a, partner_delta);\n                    int   ib = __shfl_xor_sync(full_mask, ia, partner_delta);\n\n                    // Compute global index i for this element\n                    int i_global = lane * KPW + t;\n\n                    // Direction of the bitonic compare (ascending in lower half of kk)\n                    bool ascending = ((i_global & kk) == 0);\n                    // Lower half of the jj-pair (bit jj cleared)\n                    bool lower = ((i_global & jj) == 0);\n\n                    // Select min or max depending on ascending and position\n                    // If ascending and lower -> take min; ascending and upper -> take max\n                    // If descending and lower -> take max; descending and upper -> take min\n                    bool take_min = (ascending == lower);\n\n                    // Compare a and b\n                    bool a_le_b = (a <= b);\n                    float sel_d = take_min ? (a_le_b ? a : b) : (a_le_b ? b : a);\n                    int   sel_i = take_min ? (a_le_b ? ia : ib) : (a_le_b ? ib : ia);\n\n                    dist_local[t] = sel_d;\n                    idx_local[t]  = sel_i;\n                }\n                __syncwarp();\n            } else {\n                // Within-lane compare-exchange: pairs of thread-local registers\n#pragma unroll\n                for (int t = 0; t < MAX_KPW; ++t) {\n                    if (t >= KPW) break;\n                    // Process each pair only once: only when (t & jj) == 0\n                    if ((t & jj) == 0) {\n                        int partner = t ^ jj;\n\n                        float a = dist_local[t];\n                        int   ia = idx_local[t];\n                        float b = dist_local[partner];\n                        int   ib = idx_local[partner];\n\n                        int i_global = lane * KPW + t;\n                        bool ascending = ((i_global & kk) == 0);\n\n                        // Compute min/max of the pair\n                        bool a_le_b = (a <= b);\n                        float min_d = a_le_b ? a : b;\n                        int   min_i = a_le_b ? ia : ib;\n                        float max_d = a_le_b ? b : a;\n                        int   max_i = a_le_b ? ib : ia;\n\n                        if (ascending) {\n                            dist_local[t]       = min_d;\n                            idx_local[t]        = min_i;\n                            dist_local[partner] = max_d;\n                            idx_local[partner]  = max_i;\n                        } else {\n                            dist_local[t]       = max_d;\n                            idx_local[t]        = max_i;\n                            dist_local[partner] = min_d;\n                            idx_local[partner]  = min_i;\n                        }\n                    }\n                }\n                __syncwarp();\n            }\n        }\n    }\n}\n\n// Merge function: merge the per-warp candidate buffer (in shared memory) into the\n// intermediate result stored in registers, following the specification:\n// 0) Intermediate result in registers is sorted ascending.\n// 1) Swap content of the buffer and the intermediate result so that the buffer is in registers:\n//    - Load up to k candidates from shared memory into buf_* registers (pad with +inf if needed).\n//    - Store the current intermediate result from registers into the shared buffer region (k elements).\n// 2) Sort the buffer (buf_*) in ascending order using warp bitonic sort.\n// 3) Construct a bitonic sequence in registers: for global index i, take\n//    min(buf[i], shared_result[k-1-i]) into res registers. This yields the k best of the union.\n// 4) Sort the resulting bitonic sequence ascending in registers to update the intermediate result.\n// The function returns the updated max_distance (distance of the k-th neighbor).\nstatic inline __device__ float merge_buffer_into_result(\n    Candidate *warp_buf, int buf_count, // shared memory buffer and existing candidate count\n    float res_d[MAX_KPW], int res_i[MAX_KPW], // intermediate result in registers (sorted)\n    float buf_d[MAX_KPW], int buf_i[MAX_KPW], // scratch registers for buffer\n    int KPW, int k)\n{\n    const int lane = lane_id();\n\n    // Step 1: Swap: load buffer into buf_* regs (pad with INF), store current result into shared buffer\n#pragma unroll\n    for (int t = 0; t < MAX_KPW; ++t) {\n        if (t >= KPW) break;\n        int i_global = lane * KPW + t;\n\n        // Load candidate or +inf if beyond buf_count\n        float d = (i_global < buf_count) ? warp_buf[i_global].dist : CUDART_INF_F;\n        int   idx = (i_global < buf_count) ? warp_buf[i_global].idx  : -1;\n        buf_d[t] = d;\n        buf_i[t] = idx;\n\n        // Store current result into shared buffer (swap)\n        warp_buf[i_global].dist = res_d[t];\n        warp_buf[i_global].idx  = res_i[t];\n    }\n    __syncwarp();\n\n    // Step 2: Sort buffer in ascending order\n    warp_bitonic_sort_asc(buf_d, buf_i, KPW, k);\n\n    // Step 3: Merge via bitonic min pairing: res[i] = min(buf[i], shared_res[k-1-i])\n#pragma unroll\n    for (int t = 0; t < MAX_KPW; ++t) {\n        if (t >= KPW) break;\n\n        int i_global = lane * KPW + t;\n        int j_global = k - 1 - i_global;\n\n        // Read the swapped-out intermediate result from shared memory at reversed index\n        float od = warp_buf[j_global].dist;\n        int   oi = warp_buf[j_global].idx;\n\n        // Take the minimum across the pair (ties arbitrarily resolved)\n        float bd = buf_d[t];\n        int   bi = buf_i[t];\n\n        if (od < bd) {\n            res_d[t] = od;\n            res_i[t] = oi;\n        } else {\n            res_d[t] = bd;\n            res_i[t] = bi;\n        }\n    }\n    __syncwarp();\n\n    // Step 4: Final sort of the bitonic sequence in ascending order\n    warp_bitonic_sort_asc(res_d, res_i, KPW, k);\n\n    // Update and return max_distance (the last element of the sorted result, i.e., index k-1)\n    // The last element resides in lane 31 at local index KPW-1. Broadcast from lane 31.\n    float local_last = res_d[KPW - 1];\n    float max_dist = __shfl_sync(0xFFFFFFFFu, local_last, WARP_SIZE - 1);\n    return max_dist;\n}\n\n// CUDA kernel. Each warp processes one query point.\n__global__ void knn2d_kernel(const float2 * __restrict__ query, int query_count,\n                             const float2 * __restrict__ data,  int data_count,\n                             std::pair<int, float> * __restrict__ result,\n                             int k, int tile_points)\n{\n    extern __shared__ unsigned char smem_bytes[];\n    unsigned char *smem_ptr = smem_bytes;\n\n    // Layout of shared memory:\n    // [0 .. tile_points-1] float2 cached points\n    float2 *smem_points = reinterpret_cast<float2*>(smem_ptr);\n    smem_ptr += sizeof(float2) * (size_t)tile_points;\n\n    // Candidate buffers per warp: warps_per_block * k entries\n    int warps_per_block = blockDim.x / WARP_SIZE;\n    Candidate *smem_cand = reinterpret_cast<Candidate*>(smem_ptr);\n    smem_ptr += sizeof(Candidate) * (size_t)warps_per_block * (size_t)k;\n\n    // Per-warp counters (number of candidates currently stored)\n    int *smem_counts = reinterpret_cast<int*>(smem_ptr);\n    // No further shared memory needed after this.\n\n    const unsigned full_mask = 0xFFFFFFFFu;\n    const int lane = lane_id();\n    const int warp_in_block = warp_id_in_block();\n    const int global_warp = blockIdx.x * warps_per_block + warp_in_block;\n    const int query_idx = global_warp;\n\n    if (query_idx >= query_count) return;\n\n    // Determine KPW given k (power of two, between 32 and 1024)\n    const int KPW = k / WARP_SIZE;\n\n    // Pointers to this warp's candidate buffer and its counter in shared memory\n    Candidate *warp_buf = smem_cand + (size_t)warp_in_block * (size_t)k;\n    volatile int &warp_count = reinterpret_cast<volatile int*>(smem_counts)[warp_in_block];\n\n    // Load query point into registers; lane 0 loads and broadcasts\n    float2 q;\n    if (lane == 0) q = query[query_idx];\n    q.x = __shfl_sync(full_mask, q.x, 0);\n    q.y = __shfl_sync(full_mask, q.y, 0);\n\n    // Initialize per-warp candidate count\n    if (lane == 0) warp_count = 0;\n    __syncwarp();\n\n    // Per-warp intermediate result in registers: k elements distributed across lanes.\n    // Initialize with +inf distances and indices = -1; this array is kept sorted.\n    float res_d[MAX_KPW];\n    int   res_i[MAX_KPW];\n\n#pragma unroll\n    for (int t = 0; t < MAX_KPW; ++t) {\n        if (t >= KPW) break;\n        res_d[t] = CUDART_INF_F;\n        res_i[t] = -1;\n    }\n    // Initial max_distance is +inf\n    float max_distance = CUDART_INF_F;\n\n    // Scratch buffer in registers for merging (holds buffer values during swap/sort)\n    float buf_d[MAX_KPW];\n    int   buf_i[MAX_KPW];\n\n    // Process data in batches: each block loads tile_points points into shared memory\n    for (int tile_start = 0; tile_start < data_count; tile_start += tile_points) {\n        int n_this_tile = data_count - tile_start;\n        if (n_this_tile > tile_points) n_this_tile = tile_points;\n\n        // Block-wide cooperative load of the tile into shared memory\n        for (int i = threadIdx.x; i < n_this_tile; i += blockDim.x) {\n            smem_points[i] = data[tile_start + i];\n        }\n        __syncthreads();\n\n        // Each warp processes all points in the tile\n        for (int base = 0; base < n_this_tile; base += WARP_SIZE) {\n            int idx_in_tile = base + lane;\n\n            // Load a point and compute distance if within bounds\n            float dist = CUDART_INF_F;\n            int   gidx = -1;\n            if (idx_in_tile < n_this_tile) {\n                float2 p = smem_points[idx_in_tile];\n                dist = squared_l2(q, p);\n                gidx = tile_start + idx_in_tile;\n            }\n\n            // Filter by current max_distance. Use ballot to count the number of candidates.\n            unsigned mask = __ballot_sync(full_mask, (dist < max_distance));\n\n            int n_cands = __popc(mask);\n            if (n_cands > 0) {\n                // Before inserting, ensure buffer has enough space; if not, flush (merge).\n                if (lane == 0) {\n                    if (warp_count + n_cands > k) {\n                        // Merge current buffer into result and reset count to 0\n                        // Note: other lanes wait on this merge\n                    }\n                }\n                __syncwarp();\n\n                if (warp_count + n_cands > k) {\n                    // Perform merge (Step 0..4)\n                    max_distance = merge_buffer_into_result(warp_buf, warp_count,\n                                                            res_d, res_i, buf_d, buf_i,\n                                                            KPW, k);\n                    if (lane == 0) warp_count = 0;\n                    __syncwarp();\n\n                    // Refilter with potentially updated max_distance\n                    mask = __ballot_sync(full_mask, (dist < max_distance));\n                    n_cands = __popc(mask);\n                }\n\n                if (n_cands > 0) {\n                    // Reserve space in the buffer: lane 0 updates the counter, broadcasts old value\n                    int old_count = 0;\n                    if (lane == 0) {\n                        old_count = warp_count;\n                        warp_count = warp_count + n_cands;\n                    }\n                    old_count = __shfl_sync(full_mask, old_count, 0);\n\n                    // Compute position for each selected lane using prefix count within the warp\n#if __CUDACC_VER_MAJOR__ >= 9\n                    unsigned lane_mask_lt = __lanemask_lt();\n#else\n                    unsigned lane_mask_lt = (1u << lane) - 1u;\n#endif\n                    int pos_in_warp = __popc(mask & lane_mask_lt);\n\n                    if ((mask >> lane) & 1u) {\n                        int store_pos = old_count + pos_in_warp;\n                        warp_buf[store_pos].dist = dist;\n                        warp_buf[store_pos].idx  = gidx;\n                    }\n                }\n\n                // If buffer is exactly full after insertion, flush now\n                if (lane == 0 && warp_count == k) {\n                    // Merge current buffer into result and reset count to 0\n                }\n                __syncwarp();\n\n                if (warp_count == k) {\n                    max_distance = merge_buffer_into_result(warp_buf, warp_count,\n                                                            res_d, res_i, buf_d, buf_i,\n                                                            KPW, k);\n                    if (lane == 0) warp_count = 0;\n                    __syncwarp();\n                }\n            }\n        }\n\n        __syncthreads(); // ensure tile is not used anymore before next load\n    }\n\n    // After all data processed, if candidates remain in buffer, merge them\n    if (warp_count > 0) {\n        max_distance = merge_buffer_into_result(warp_buf, warp_count,\n                                                res_d, res_i, buf_d, buf_i,\n                                                KPW, k);\n        if (lane == 0) warp_count = 0;\n        __syncwarp();\n    }\n\n    // Write out the final top-k for this query (sorted ascending). Each lane writes its KPW entries.\n#pragma unroll\n    for (int t = 0; t < MAX_KPW; ++t) {\n        if (t >= KPW) break;\n        int i_global = lane * KPW + t;\n        int out_index = query_idx * k + i_global;\n        // Guaranteed in-bounds since result has query_count * k elements\n        result[out_index] = std::pair<int, float>(res_i[t], res_d[t]);\n    }\n}\n\n// Host-side runner that configures the kernel launch parameters, shared memory usage,\n// and invokes the kernel.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data, int data_count,\n             std::pair<int, float> *result, int k)\n{\n    // Choose number of warps per block and tile size based on available dynamic shared memory.\n    // Default target: 8 warps per block (256 threads), adjust if necessary.\n    int device = 0;\n    cudaGetDevice(&device);\n\n    int max_smem_optin = 0;\n    cudaDeviceGetAttribute(&max_smem_optin, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    if (max_smem_optin == 0) {\n        // Fallback to legacy per-block limit\n        cudaDeviceProp prop;\n        cudaGetDeviceProperties(&prop, device);\n        max_smem_optin = prop.sharedMemPerBlock;\n    }\n\n    // Try configurations with decreasing warps per block to fit shared memory comfortably.\n    int best_warps = 8; // start with 8 warps per block\n    int best_tile_points = 0;\n    size_t best_smem_bytes = 0;\n\n    for (int warps = 8; warps >= 1; warps >>= 1) {\n        // Candidate buffer bytes for all warps in block\n        size_t cand_bytes = (size_t)warps * (size_t)k * sizeof(Candidate);\n        size_t count_bytes = (size_t)warps * sizeof(int);\n\n        // Leave some headroom; compute maximum tile points we can fit\n        size_t overhead = cand_bytes + count_bytes;\n        if (overhead >= (size_t)max_smem_optin) continue;\n\n        size_t bytes_left = (size_t)max_smem_optin - overhead;\n        int tile_points = (int)(bytes_left / sizeof(float2));\n\n        // Ensure tile_points is a reasonable multiple of warp size\n        if (tile_points > 0) {\n            // Clamp tile points to at most data_count (no need to exceed), but keep a good chunk\n            if (tile_points > 8192) tile_points = 8192; // avoid over-allocating per block\n            // Round down to multiple of WARP_SIZE\n            tile_points = (tile_points / WARP_SIZE) * WARP_SIZE;\n        }\n\n        if (tile_points >= WARP_SIZE) {\n            best_warps = warps;\n            best_tile_points = tile_points;\n            best_smem_bytes = overhead + (size_t)tile_points * sizeof(float2);\n            break;\n        }\n    }\n\n    if (best_tile_points == 0) {\n        // Extremely constrained case; fall back to minimal configuration: 1 warp, 32 points tile\n        best_warps = 1;\n        best_tile_points = WARP_SIZE;\n        best_smem_bytes = (size_t)best_warps * (size_t)k * sizeof(Candidate)\n                        + (size_t)best_warps * sizeof(int)\n                        + (size_t)best_tile_points * sizeof(float2);\n    }\n\n    // Configure kernel launch\n    dim3 block(best_warps * WARP_SIZE, 1, 1);\n    int num_warps_total = (query_count + 0) / 1; // one warp per query\n    int grid_x = (query_count + best_warps - 1) / best_warps;\n    dim3 grid(grid_x, 1, 1);\n\n    // Set kernel attribute to allow use of larger dynamic shared memory if needed\n    cudaFuncSetAttribute(knn2d_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)best_smem_bytes);\n\n    // Launch kernel\n    knn2d_kernel<<<grid, block, best_smem_bytes>>>(query, query_count, data, data_count, result, k, best_tile_points);\n\n    // Optional: synchronize or leave it to the caller\n    // cudaDeviceSynchronize();\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1355, "completion_tokens": 18383, "total_tokens": 19738, "prompt_tokens_details": {"cached_tokens": 1280, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 13568, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db95788819085417270d42c930e", "custom_id": "knn08-03", "response": {"status_code": 200, "request_id": "205aa3a559742752b0187dc085ea3d9c", "body": {"id": "chatcmpl-C4svXbEUuYjBQUdx4h9PXSqySoJMg", "object": "chat.completion", "created": 1755280487, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <float.h>\n#include <stdint.h>\n\n// This implementation computes k-NN for 2D points using one warp (32 threads) per query.\n// The algorithm processes the data points in shared-memory tiles. For each query:\n// - An intermediate result of k nearest neighbors is kept sorted in ascending order across warp registers,\n//   distributed so that each thread keeps k/32 consecutive elements (chunk = k/32).\n// - A per-warp candidate buffer in shared memory accumulates up to k candidates at a time.\n// - Distances below the current max_distance (k-th best so far) are appended to the candidate buffer using\n//   warp ballot and lane-local prefix to place them contiguously.\n// - When the candidate buffer becomes full (or at the end), it is merged with the intermediate result by:\n//     1) Sorting the candidate buffer in ascending order using a warp-wide Bitonic sort.\n//     2) Producing the minima between cand[i] and reversed intermediate[k-1-i] into registers, forming a bitonic sequence of k elements.\n//     3) Sorting this merged bitonic sequence with a warp-wide Bitonic sort to obtain the updated intermediate result.\n// - The Bitonic sort is implemented with warp shuffles for inter-thread compare-exchange when the partner is in another lane,\n//   and with in-register swaps when the partner is within the same thread (j < chunk). For cross-lane exchanges, the compared\n//   elements always share the same index within each thread's registers because of the block distribution (consecutive elements per thread).\n\n// NOTE ABOUT std::pair IN DEVICE MEMORY:\n// The device kernel writes results via a POD struct with identical layout to std::pair<int,float>.\nstruct PairIF {\n    int   first;\n    float second;\n};\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Number of data points to cache per block in shared memory per tile.\n// 2048 points * 8 bytes/point = 16KB shared for the tile.\n// Candidate buffer per warp at worst k=1024 -> 8KB. With 8 warps/block = 64KB.\n// Total ~80KB (plus minor overhead), safely under 96KB common per-block SMEM without special attributes.\n#ifndef TILE_POINTS\n#define TILE_POINTS 2048\n#endif\n\n// Compute squared Euclidean distance between two float2 points.\n__device__ __forceinline__ float sq_l2(const float2 a, const float2 b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    // Use FMA for precision and throughput: dx*dx + dy*dy\n    return fmaf(dx, dx, dy * dy);\n}\n\n// Warp-wide Bitonic sort for a distributed array of length K = k, where each lane owns 'chunk = k/32' consecutive elements.\n// - dist[0..chunk-1], idx[0..chunk-1] are the registers for the local chunk in each thread/lane.\n// - The logical global position of local element r in lane L is g = L*chunk + r.\n// - For j >= chunk, partner resides in another lane but at the same local offset: lane' = L ^ (j/chunk), offset r.\n// - For j < chunk, partner resides in the same lane at offset r^j.\n// This function sorts the k elements across the warp in ascending order by distance.\n__device__ __forceinline__ void warp_bitonic_sort_blocked(int k, int chunk, float dist[], int idx[]) {\n    const unsigned FULL_MASK = 0xFFFFFFFFu;\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n    const int base = lane * chunk;\n\n    // Outer Bitonic build/merge stages\n    for (int Kstage = 2; Kstage <= k; Kstage <<= 1) {\n        // Inter-lane compare-exchange stages (partners in different lanes, same local offset)\n        for (int j = Kstage >> 1; j >= chunk; j >>= 1) {\n            const int partner_lane = lane ^ (j / chunk);\n            // For each local register\n            for (int r = 0; r < chunk; ++r) {\n                float a = dist[r];\n                int   ia = idx[r];\n                // Exchange with partner lane, same register index r\n                float b = __shfl_sync(FULL_MASK, a, partner_lane);\n                int   ib = __shfl_sync(FULL_MASK, ia, partner_lane);\n                // Direction of comparison for this position\n                const bool up = (((base + r) & Kstage) == 0);\n                // For 'up', keep min at this position; for 'down', keep max at this position.\n                const bool take_partner = ((a > b) == up);\n                if (take_partner) { a = b; ia = ib; }\n                dist[r] = a; idx[r] = ia;\n            }\n            __syncwarp();\n        }\n        // Intra-lane compare-exchange stages (partners in same lane, offsets differ by j)\n        for (int j = (chunk > (Kstage >> 1) ? (Kstage >> 1) : (chunk - 1)); j >= 1; j >>= 1) {\n            // Process each pair exactly once: if r2 > r perform the swap for (r, r2)\n            for (int r = 0; r < chunk; ++r) {\n                int r2 = r ^ j;\n                if (r2 > r) {\n                    float a = dist[r];   int ia = idx[r];\n                    float b = dist[r2];  int ib = idx[r2];\n                    // For intra-lane pairs, the 'i' used for direction is the smaller index => base + r (since r2 > r)\n                    const bool up = (((base + r) & Kstage) == 0);\n                    const bool do_swap = ((a > b) == up);\n                    if (do_swap) {\n                        float tmp = a; a = b; b = tmp;\n                        int   ti  = ia; ia = ib; ib = ti;\n                    }\n                    dist[r]  = a;  idx[r]  = ia;\n                    dist[r2] = b;  idx[r2] = ib;\n                }\n            }\n            __syncwarp();\n        }\n    }\n}\n\n// Merge the per-warp shared-memory candidate buffer with the current intermediate result in registers.\n// After completion:\n// - best_dist/idx hold the updated intermediate result (sorted ascending).\n// - maxDist is updated to the k-th (largest) element.\n// The shared candidate count is reset to 0.\n__device__ __forceinline__ void warp_flush_merge(\n    int k, int chunk,\n    float best_dist[], int best_idx[],\n    float buf_dist[], int buf_idx[],\n    volatile int* cand_count_ptr, float* cand_dist_s, int* cand_idx_s,\n    float &maxDist)\n{\n    const unsigned FULL_MASK = 0xFFFFFFFFu;\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n    const int base = lane * chunk;\n\n    // Read candidate count (warp-private) and broadcast\n    int count = 0;\n    if (lane == 0) count = *cand_count_ptr;\n    count = __shfl_sync(FULL_MASK, count, 0);\n\n    // Load candidates into registers; pad with +INF where no candidate exists\n    for (int r = 0; r < chunk; ++r) {\n        const int g = base + r;\n        if (g < count) {\n            buf_dist[r] = cand_dist_s[g];\n            buf_idx[r]  = cand_idx_s[g];\n        } else {\n            buf_dist[r] = FLT_MAX;\n            buf_idx[r]  = -1;\n        }\n    }\n    __syncwarp();\n\n    // Step 2: Sort the buffer in ascending order\n    warp_bitonic_sort_blocked(k, chunk, buf_dist, buf_idx);\n\n    // Step 3: Merge minima with reversed intermediate result in registers.\n    // For global logical position p = base + r, the reversed partner index is q = k-1-p.\n    // Its lane is lane' = 31 - lane and offset r' = chunk - 1 - r.\n    for (int r = 0; r < chunk; ++r) {\n        const int partner_lane = (WARP_SIZE - 1) - lane; // 31 - lane\n        // Provide per-lane variable as the source for shfl; all lanes use the same r, so offsets match.\n        float best_rev_val = __shfl_sync(FULL_MASK, best_dist[chunk - 1 - r], partner_lane);\n        int   best_rev_idx = __shfl_sync(FULL_MASK, best_idx[chunk - 1 - r], partner_lane);\n        float a = buf_dist[r];\n        int   ia = buf_idx[r];\n        if (best_rev_val < a) { a = best_rev_val; ia = best_rev_idx; }\n        buf_dist[r] = a;\n        buf_idx[r]  = ia;\n    }\n    __syncwarp();\n\n    // Step 4: Sort the merged result (bitonic sequence) in ascending order\n    warp_bitonic_sort_blocked(k, chunk, buf_dist, buf_idx);\n\n    // Copy merged sorted result back into intermediate registers\n    for (int r = 0; r < chunk; ++r) {\n        best_dist[r] = buf_dist[r];\n        best_idx[r]  = buf_idx[r];\n    }\n    __syncwarp();\n\n    // Update maxDist to the k-th (largest) element\n    float kth_local = (lane == (WARP_SIZE - 1)) ? best_dist[chunk - 1] : 0.0f;\n    float kth = __shfl_sync(FULL_MASK, kth_local, (WARP_SIZE - 1));\n    maxDist = kth;\n\n    // Reset candidate buffer count\n    if (lane == 0) *cand_count_ptr = 0;\n    __syncwarp();\n}\n\n// CUDA kernel: One warp per query\n__global__ void knn_kernel(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data,  int data_count,\n    PairIF* __restrict__ result, int k)\n{\n    // Thread identification\n    const int lane             = threadIdx.x & (WARP_SIZE - 1);\n    const int warp_in_block    = threadIdx.x >> 5;                 // threadIdx.x / 32\n    const int warps_per_block  = blockDim.x >> 5;\n    const int warp_global      = blockIdx.x * warps_per_block + warp_in_block;\n    const bool warpActive      = (warp_global < query_count);\n    const unsigned FULL_MASK   = 0xFFFFFFFFu;\n\n    // Each thread maintains 'chunk = k/32' consecutive elements in registers\n    const int chunk = k >> 5; // k/32; k is power of two between 32 and 1024\n\n    // Shared memory organization:\n    // [ float2 tile[TILE_POINTS] ][ int cand_count[warps_per_block] ]\n    // [ float cand_dist[warps_per_block * k] ][ int cand_idx[warps_per_block * k] ]\n    extern __shared__ unsigned char smem_raw[];\n    float2* smem_points = reinterpret_cast<float2*>(smem_raw);\n    size_t  smem_off    = TILE_POINTS * sizeof(float2);\n\n    int*   cand_count_s = reinterpret_cast<int*>(smem_raw + smem_off);\n    smem_off += warps_per_block * sizeof(int);\n\n    float* cand_dist_s  = reinterpret_cast<float*>(smem_raw + smem_off);\n    smem_off += static_cast<size_t>(warps_per_block) * k * sizeof(float);\n\n    int*   cand_idx_s   = reinterpret_cast<int*>(smem_raw + smem_off);\n    // smem_off ends here\n\n    // Pointers to this warp's candidate buffer in shared memory\n    volatile int* my_cand_count = cand_count_s + warp_in_block;\n    float* my_cand_dist = cand_dist_s + warp_in_block * k;\n    int*   my_cand_idx  = cand_idx_s  + warp_in_block * k;\n\n    // Initialize candidate buffer count to zero\n    if (lane == 0) *my_cand_count = 0;\n\n    // Intermediate result stored in registers: sorted ascending\n    float best_dist[WARP_SIZE]; // allocate max 32; only [0..chunk-1] are used\n    int   best_idx [WARP_SIZE];\n    #pragma unroll\n    for (int r = 0; r < WARP_SIZE; ++r) {\n        if (r < chunk) {\n            best_dist[r] = FLT_MAX;\n            best_idx[r]  = -1;\n        }\n    }\n\n    // Max distance (current k-th neighbor distance), replicated in all lanes\n    float maxDist = FLT_MAX;\n\n    // Load query point, broadcast across the warp\n    float qx = 0.0f, qy = 0.0f;\n    if (warpActive && lane == 0) {\n        float2 q = query[warp_global];\n        qx = q.x; qy = q.y;\n    }\n    qx = __shfl_sync(FULL_MASK, qx, 0);\n    qy = __shfl_sync(FULL_MASK, qy, 0);\n    const float2 qpt = make_float2(qx, qy);\n\n    // Iterate over data tiles\n    for (int tileBase = 0; tileBase < data_count; tileBase += TILE_POINTS) {\n        const int tileCount = min(TILE_POINTS, data_count - tileBase);\n\n        // Load tile to shared memory cooperatively by the whole block\n        for (int t = threadIdx.x; t < tileCount; t += blockDim.x) {\n            smem_points[t] = data[tileBase + t];\n        }\n        __syncthreads();\n\n        // Process this tile: each warp computes distances for its query\n        if (warpActive) {\n            // Iterate over points in the tile; one per lane each iteration\n            for (int t = lane; t < tileCount; t += WARP_SIZE) {\n                float2 dp = smem_points[t];\n                float d = sq_l2(qpt, dp);\n\n                // Filter by current maxDist and ballot across the warp\n                const bool isCand = (d < maxDist);\n                const unsigned m = __ballot_sync(FULL_MASK, isCand);\n                const int n = __popc(m);\n\n                if (n > 0) {\n                    // Read current count (lane 0) and broadcast\n                    int curCount = 0;\n                    if (lane == 0) curCount = *my_cand_count;\n                    curCount = __shfl_sync(FULL_MASK, curCount, 0);\n\n                    // If adding n candidates would overflow the buffer, flush/merge first\n                    int needFlush = (curCount + n > k);\n                    needFlush = __shfl_sync(FULL_MASK, needFlush, 0);\n                    if (needFlush) {\n                        warp_flush_merge(k, chunk,\n                                         best_dist, best_idx,\n                                         // use these regs as temporary 'buf'\n                                         best_dist /*unused*/, best_idx /*unused*/,\n                                         my_cand_count, my_cand_dist, my_cand_idx,\n                                         maxDist);\n                        // After flush, cand_count == 0\n                        curCount = 0;\n                    }\n\n                    // Reserve positions [curCount, curCount + n) for these candidates\n                    if (lane == 0) *my_cand_count = curCount + n;\n                    const int base = __shfl_sync(FULL_MASK, curCount, 0);\n\n                    // Lane-local offset using prefix of ballot mask\n                    const int pos = __popc(m & ((1u << lane) - 1));\n                    if (isCand) {\n                        const int writeIdx = base + pos;\n                        my_cand_dist[writeIdx] = d;\n                        my_cand_idx [writeIdx] = tileBase + t;\n                    }\n                    __syncwarp();\n\n                    // If buffer just became full, flush immediately\n                    int newCount = 0;\n                    if (lane == 0) newCount = *my_cand_count;\n                    newCount = __shfl_sync(FULL_MASK, newCount, 0);\n                    int doFlush = (newCount == k);\n                    doFlush = __shfl_sync(FULL_MASK, doFlush, 0);\n                    if (doFlush) {\n                        warp_flush_merge(k, chunk,\n                                         best_dist, best_idx,\n                                         // reuse scratch regs\n                                         best_dist /*unused*/, best_idx /*unused*/,\n                                         my_cand_count, my_cand_dist, my_cand_idx,\n                                         maxDist);\n                    }\n                }\n\n                __syncwarp(); // Warp-level barrier for safety between iterations\n            }\n        }\n\n        __syncthreads(); // Ensure tile is no longer in use before loading the next one\n    }\n\n    // After processing all tiles, flush any remaining candidates\n    if (warpActive) {\n        int remaining = 0;\n        if (lane == 0) remaining = *my_cand_count;\n        remaining = __shfl_sync(FULL_MASK, remaining, 0);\n        if (remaining > 0) {\n            warp_flush_merge(k, chunk,\n                             best_dist, best_idx,\n                             // reuse scratch regs\n                             best_dist /*unused*/, best_idx /*unused*/,\n                             my_cand_count, my_cand_dist, my_cand_idx,\n                             maxDist);\n        }\n    }\n\n    // Write back results: for query 'warp_global', sorted nearest neighbors in ascending order.\n    if (warpActive) {\n        PairIF* out = result + static_cast<size_t>(warp_global) * k;\n        const int base = lane * chunk;\n        for (int r = 0; r < chunk; ++r) {\n            const int g = base + r;\n            out[g].first  = best_idx[r];\n            out[g].second = best_dist[r];\n        }\n    }\n}\n\n// Host API entry point.\n// query: device pointer to float2 queries of size query_count\n// data:  device pointer to float2 data points of size data_count\n// result: device pointer to std::pair<int,float> array of size query_count * k (contiguous)\n// k: power of two in [32, 1024], data_count >= k\nextern \"C\"\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data,  int data_count,\n             std::pair<int, float> *result, int k)\n{\n    // Select threads/block and warps/block. Use 8 warps per block (256 threads).\n    const int warpsPerBlock = 8;\n    const int threadsPerBlock = warpsPerBlock * WARP_SIZE;\n    const int blocks = (query_count + warpsPerBlock - 1) / warpsPerBlock;\n\n    // Dynamic shared memory size:\n    // - Tile: TILE_POINTS * sizeof(float2)\n    // - Candidate counts: warpsPerBlock * sizeof(int)\n    // - Candidate buffers (dist + idx): warpsPerBlock * k * (sizeof(float) + sizeof(int))\n    size_t smem_size = static_cast<size_t>(TILE_POINTS) * sizeof(float2)\n                     + static_cast<size_t>(warpsPerBlock) * sizeof(int)\n                     + static_cast<size_t>(warpsPerBlock) * static_cast<size_t>(k) * (sizeof(float) + sizeof(int));\n\n    // Launch kernel. We reinterpret the result pointer to a POD struct with identical layout.\n    knn_kernel<<<blocks, threadsPerBlock, smem_size>>>(\n        query, query_count,\n        data,  data_count,\n        reinterpret_cast<PairIF*>(result),\n        k);\n\n    // Synchronize to ensure completion before returning.\n    cudaDeviceSynchronize();\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1355, "completion_tokens": 27632, "total_tokens": 28987, "prompt_tokens_details": {"cached_tokens": 1280, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 23360, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db96e688190a4c5cd968d491c63", "custom_id": "knn08-04", "response": {"status_code": 200, "request_id": "0b2a511993103294ed7325f446fcffea", "body": {"id": "chatcmpl-C4tHnml5Cq4ibrZa0uxl1jKduuIbL", "object": "chat.completion", "created": 1755281867, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// This implementation assigns one warp (32 threads) to each query point.\n// Each warp maintains an intermediate result (k nearest neighbors) distributed across\n// the registers of its threads. Each thread stores k/32 consecutive elements.\n// A per-warp candidate buffer of size k is kept in shared memory. Data points are\n// processed in batches loaded into shared memory by the entire thread block.\n// Candidates with distance < max_distance (k-th neighbor) are compacted into the buffer\n// using warp ballot-based packing. When the buffer is full, it is merged into the\n// intermediate result using a bitonic sort-based pipeline as described in the prompt.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n#ifndef MAX_K\n#define MAX_K 1024\n#endif\n\n#ifndef MAX_SEG\n#define MAX_SEG (MAX_K / WARP_SIZE) // Maximum per-thread segment length\n#endif\n\n#ifndef TILE_POINTS\n#define TILE_POINTS 4096 // Number of data points cached per block per batch (float2 -> 8 bytes -> 32KB)\n#endif\n\n// Utility: minimum of two (distance, index) pairs with distance as primary key.\n// In case of tie, choose arbitrary (here: the first argument).\n__device__ __forceinline__ void pair_min(float& da, int& ia, const float db, const int ib) {\n    if (db < da) {\n        da = db;\n        ia = ib;\n    }\n}\n\n// Warp-wide bitonic sort of a distributed array of length K = S * 32, where each thread\n// holds S consecutive elements in vals[0..S-1] and idxs[0..S-1]. The sort is ascending.\n// Cross-thread exchanges use warp shuffles; intra-thread exchanges swap in registers.\n//\n// The implementation follows the serial bitonic sort structure, splitting each compare-exchange\n// step into either cross-thread (stride >= S) or intra-thread (stride < S) operations. For the\n// cross-thread case we use the standard \"keep min/max\" formulation which is safe to apply\n// symmetrically on both sides without an explicit \"if (l > i)\" guard. For the intra-thread\n// case we explicitly perform each swap once per pair (partner > self).\n__device__ __forceinline__ void warp_bitonic_sort_distributed(float vals[MAX_SEG], int idxs[MAX_SEG], const int S, const int K) {\n    const unsigned FULL_MASK = 0xFFFFFFFFu;\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n\n    // size spans the subsequence size in the bitonic network (2, 4, 8, ..., K)\n    for (unsigned size = 2; size <= (unsigned)K; size <<= 1) {\n        // stride halves each iteration: size/2, size/4, ..., 1\n        for (unsigned stride = size >> 1; stride > 0; stride >>= 1) {\n\n            if (stride >= (unsigned)S) {\n                // Cross-thread exchanges at fixed local index s\n                const unsigned partner = stride / (unsigned)S; // XOR distance in lanes\n\n                #pragma unroll\n                for (int s = 0; s < MAX_SEG; ++s) {\n                    if (s >= S) break;\n\n                    const int i = lane * S + s;\n                    const bool up = ((i & size) == 0);\n\n                    float self_v = vals[s];\n                    int   self_i = idxs[s];\n\n                    float other_v = __shfl_xor_sync(FULL_MASK, self_v, partner);\n                    int   other_i = __shfl_xor_sync(FULL_MASK, self_i, partner);\n\n                    const bool take_self = up ? (self_v <= other_v) : (self_v >= other_v);\n\n                    vals[s] = take_self ? self_v : other_v;\n                    idxs[s] = take_self ? self_i : other_i;\n                }\n            } else {\n                // Intra-thread exchanges: partner index differs within the local segment\n                const int j = (int)stride;\n\n                #pragma unroll\n                for (int s = 0; s < MAX_SEG; ++s) {\n                    if (s >= S) break;\n\n                    const int partner = s ^ j;\n                    if (partner > s) {\n                        const int i = (threadIdx.x & (WARP_SIZE - 1)) * S + s;\n                        const bool up = ((i & size) == 0);\n\n                        float a = vals[s];  int ia = idxs[s];\n                        float b = vals[partner]; int ib = idxs[partner];\n\n                        const bool swap_needed = up ? (a > b) : (a < b);\n                        if (swap_needed) {\n                            vals[s]       = b;  idxs[s]       = ib;\n                            vals[partner] = a;  idxs[partner] = ia;\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\n// Merge the full candidate buffer (length K in shared memory) with the current intermediate\n// result in registers (also length K), using the pipeline:\n// 1) Swap content so that the buffer is in registers (load buffer into regs, store regs into buffer).\n// 2) Sort the registers ascending via warp_bitonic_sort_distributed.\n// 3) Merge registers and buffer into registers by taking per-position minima between\n//    reg[i] and buffer[K - 1 - i]; the result is bitonic and contains the best K elements.\n// 4) Sort the registers ascending again via warp_bitonic_sort_distributed.\n__device__ __forceinline__ void merge_full_buffer_with_intermediate(\n    float r_vals[MAX_SEG], int r_idxs[MAX_SEG],\n    float* __restrict__ buf_vals, int* __restrict__ buf_idxs,\n    const int S, const int K)\n{\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n\n    // Step 1: swap content between registers (intermediate result) and shared memory (buffer)\n    #pragma unroll\n    for (int s = 0; s < MAX_SEG; ++s) {\n        if (s >= S) break;\n        const int pos = lane * S + s;\n        const float tmp_v = buf_vals[pos];\n        const int   tmp_i = buf_idxs[pos];\n\n        buf_vals[pos] = r_vals[s];\n        buf_idxs[pos] = r_idxs[s];\n\n        r_vals[s] = tmp_v;\n        r_idxs[s] = tmp_i;\n    }\n    __syncwarp();\n\n    // Step 2: sort the buffer (now in registers) ascending\n    warp_bitonic_sort_distributed(r_vals, r_idxs, S, K);\n    __syncwarp();\n\n    // Step 3: per-position minima between register value at i and shared buffer at (K - 1 - i)\n    #pragma unroll\n    for (int s = 0; s < MAX_SEG; ++s) {\n        if (s >= S) break;\n        const int i = lane * S + s;\n        const int j = K - 1 - i;\n\n        const float ov = buf_vals[j];\n        const int   oi = buf_idxs[j];\n\n        // r_vals[s] = min(r_vals[s], ov) with index paired\n        if (ov < r_vals[s]) {\n            r_vals[s] = ov;\n            r_idxs[s] = oi;\n        }\n    }\n    __syncwarp();\n\n    // Step 4: sort the merged bitonic sequence ascending\n    warp_bitonic_sort_distributed(r_vals, r_idxs, S, K);\n    __syncwarp();\n}\n\n// CUDA kernel: one warp per query\n__global__ void knn_kernel(\n    const float2* __restrict__ query, int query_count,\n    const float2* __restrict__ data,  int data_count,\n    std::pair<int, float>* __restrict__ result,\n    int K)\n{\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n    const int warp_in_block = threadIdx.x >> 5;\n    const int warps_per_block = blockDim.x >> 5;\n    const int warp_global = blockIdx.x * warps_per_block + warp_in_block;\n    const bool warp_active = (warp_global < query_count);\n\n    // Dynamic shared memory layout:\n    // [ tile (TILE_POINTS * sizeof(float2)) ]\n    // [ cand_counts (warps_per_block * sizeof(int)) ]\n    // [ cand_dists (warps_per_block * K * sizeof(float)) ]\n    // [ cand_idxs  (warps_per_block * K * sizeof(int)) ]\n    extern __shared__ unsigned char smem[];\n    unsigned char* ptr = smem;\n\n    float2* tile = reinterpret_cast<float2*>(ptr);\n    ptr += TILE_POINTS * sizeof(float2);\n\n    int* cand_counts = reinterpret_cast<int*>(ptr);\n    ptr += warps_per_block * sizeof(int);\n\n    float* cand_dists_base = reinterpret_cast<float*>(ptr);\n    ptr += (size_t)warps_per_block * K * sizeof(float);\n\n    int* cand_idxs_base = reinterpret_cast<int*>(ptr);\n    // ptr += (size_t)warps_per_block * K * sizeof(int); // not needed further\n\n    // Pointers for this warp's candidate buffer (shared memory)\n    float* warp_cand_dists = cand_dists_base + (size_t)warp_in_block * K;\n    int*   warp_cand_idxs  = cand_idxs_base  + (size_t)warp_in_block * K;\n    // Candidate count for this warp (shared memory)\n    int& cand_count = cand_counts[warp_in_block];\n\n    // S = K / 32 elements per thread; K is a power of two and 32 <= K <= 1024 => 1 <= S <= 32\n    const int S = K >> 5;\n\n    // Intermediate result in registers: sorted ascending at all times (invariant).\n    float r_vals[MAX_SEG];\n    int   r_idxs[MAX_SEG];\n\n    // Initialize intermediate result to +inf (distances) and -1 (indices).\n    #pragma unroll\n    for (int s = 0; s < MAX_SEG; ++s) {\n        if (s < S) {\n            r_vals[s] = CUDART_INF_F;\n            r_idxs[s] = -1;\n        }\n    }\n\n    // Each warp loads its query point to registers.\n    float qx = 0.0f, qy = 0.0f;\n    if (warp_active) {\n        const float2 q = query[warp_global];\n        qx = q.x; qy = q.y;\n    }\n\n    // Initialize candidate count\n    if (lane == 0) cand_count = 0;\n    __syncthreads();\n\n    // max_distance is the k-th (last) distance in the intermediate result (ascending).\n    float max_distance = CUDART_INF_F;\n\n    // Process data in batches cached into shared memory\n    for (int base = 0; base < data_count; base += TILE_POINTS) {\n        const int tile_count = min(TILE_POINTS, data_count - base);\n\n        // Load tile from global memory cooperatively\n        for (int t = threadIdx.x; t < tile_count; t += blockDim.x) {\n            tile[t] = data[base + t];\n        }\n        __syncthreads();\n\n        if (warp_active) {\n            // Iterate over points in the tile, one per lane in a strided manner\n            for (int t = lane; t < tile_count; t += WARP_SIZE) {\n                const float2 p = tile[t];\n                const float dx = p.x - qx;\n                const float dy = p.y - qy;\n                const float dist = dx * dx + dy * dy;\n                const int   idx  = base + t;\n\n                // Filter by current max_distance\n                const bool is_candidate = (dist < max_distance);\n                const unsigned FULL_MASK = 0xFFFFFFFFu;\n                const unsigned mask = __ballot_sync(FULL_MASK, is_candidate);\n                const int cnt = __popc(mask);\n\n                if (cnt > 0) {\n                    const int rank = __popc(mask & ((1u << lane) - 1));\n                    int room = 0;\n\n                    if (lane == 0) room = K - cand_count;\n                    room = __shfl_sync(FULL_MASK, room, 0);\n\n                    if (cnt <= room) {\n                        int base_pos = 0;\n                        if (lane == 0) { base_pos = cand_count; cand_count += cnt; }\n                        base_pos = __shfl_sync(FULL_MASK, base_pos, 0);\n\n                        if (is_candidate) {\n                            const int pos = base_pos + rank;\n                            warp_cand_dists[pos] = dist;\n                            warp_cand_idxs[pos]  = idx;\n                        }\n\n                        // If we just filled the buffer exactly, merge it\n                        if (cnt == room && room > 0) {\n                            __syncwarp();\n                            merge_full_buffer_with_intermediate(r_vals, r_idxs, warp_cand_dists, warp_cand_idxs, S, K);\n                            if (lane == 0) cand_count = 0;\n                            // Update max_distance from lane 31's last local element\n                            const float kth = r_vals[S - 1];\n                            max_distance = __shfl_sync(FULL_MASK, kth, WARP_SIZE - 1);\n                        }\n                    } else {\n                        // cnt > room: fill remaining room (if any), then merge, then place leftovers\n                        if (room > 0) {\n                            if (is_candidate && rank < room) {\n                                const int pos = (K - room) + rank; // cand_count + rank, since cand_count = K - room\n                                warp_cand_dists[pos] = dist;\n                                warp_cand_idxs[pos]  = idx;\n                            }\n                            __syncwarp();\n                            merge_full_buffer_with_intermediate(r_vals, r_idxs, warp_cand_dists, warp_cand_idxs, S, K);\n                            if (lane == 0) cand_count = 0;\n                            const float kth = r_vals[S - 1];\n                            max_distance = __shfl_sync(FULL_MASK, kth, WARP_SIZE - 1);\n                        }\n\n                        // Place leftover candidates into fresh buffer\n                        const int leftover = cnt - room;\n                        if (leftover > 0) {\n                            if (is_candidate && rank >= room) {\n                                const int new_rank = rank - room;\n                                warp_cand_dists[new_rank] = dist;\n                                warp_cand_idxs[new_rank]  = idx;\n                            }\n                            if (lane == 0) cand_count = leftover;\n                        }\n                    }\n                }\n            }\n        }\n\n        __syncthreads(); // Ensure tile is no longer used before loading the next one\n    }\n\n    // After all batches, flush remaining candidates (if any)\n    if (warp_active) {\n        const unsigned FULL_MASK = 0xFFFFFFFFu;\n        int count_now = 0;\n        if (lane == 0) count_now = cand_count;\n        count_now = __shfl_sync(FULL_MASK, count_now, 0);\n\n        if (count_now > 0) {\n            // Pad the rest with +inf and invalid indices\n            for (int pos = lane; pos < (K - count_now); pos += WARP_SIZE) {\n                warp_cand_dists[count_now + pos] = CUDART_INF_F;\n                warp_cand_idxs[count_now + pos]  = -1;\n            }\n            __syncwarp();\n\n            // Merge the padded full buffer\n            merge_full_buffer_with_intermediate(r_vals, r_idxs, warp_cand_dists, warp_cand_idxs, S, K);\n            if (lane == 0) cand_count = 0;\n            const float kth = r_vals[S - 1];\n            max_distance = __shfl_sync(FULL_MASK, kth, WARP_SIZE - 1);\n        }\n\n        // Write final sorted k nearest neighbors to output\n        #pragma unroll\n        for (int s = 0; s < MAX_SEG; ++s) {\n            if (s >= S) break;\n            const int pos = lane * S + s;\n            const int out_idx = warp_global * K + pos;\n            result[out_idx].first  = r_idxs[s];\n            result[out_idx].second = r_vals[s];\n        }\n    }\n}\n\n// Host entry point. Launches the kernel with 8 warps (256 threads) per block.\n// Shared memory allocation:\n//   TILE_POINTS * sizeof(float2)                         // tile cache\n// + warps_per_block * sizeof(int)                        // candidate counts per warp\n// + warps_per_block * k * (sizeof(float) + sizeof(int))  // per-warp candidate buffers\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    const int warps_per_block = 8;\n    const int threads_per_block = warps_per_block * WARP_SIZE;\n    const int num_blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    const size_t tile_bytes = (size_t)TILE_POINTS * sizeof(float2);\n    const size_t counts_bytes = (size_t)warps_per_block * sizeof(int);\n    const size_t cand_bytes = (size_t)warps_per_block * (size_t)k * (sizeof(float) + sizeof(int));\n    const size_t shared_bytes = tile_bytes + counts_bytes + cand_bytes;\n\n    // Increase dynamic shared memory limit if needed\n    cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)shared_bytes);\n\n    knn_kernel<<<num_blocks, threads_per_block, shared_bytes>>>(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1355, "completion_tokens": 21548, "total_tokens": 22903, "prompt_tokens_details": {"cached_tokens": 1280, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 17728, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db96ae081909f1a9bd054c12a5c", "custom_id": "knn08-05", "response": {"status_code": 200, "request_id": "64cb5d79635230cbf14a3db46565c9f3", "body": {"id": "chatcmpl-C4svvPMmlpDAKoMsDHU2l6FBD7auq", "object": "chat.completion", "created": 1755280511, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// This implementation assigns one warp (32 threads) per query. Each thread keeps k/32\n// nearest neighbors in its registers as two arrays: distances (float) and indices (int).\n// Data points are processed in batches cached into shared memory by the entire block.\n// Each warp maintains a shared candidate buffer of size k (distances + indices) and a shared\n// counter for the number of currently stored candidates. When the buffer is full (or after\n// the last batch), the buffer is merged into the intermediate result in registers using\n// a bitonic-sort based merge as described in the prompt.\n//\n// The implementation uses warp ballot to count candidates, warp shuffles to perform\n// cross-lane compare-exchange steps of the bitonic sort, and shared memory for both\n// the data tiles and per-warp candidate buffers.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Tunable hyper-parameters\nstatic constexpr int BLOCK_THREADS = 256;                       // 8 warps per block\nstatic constexpr int WARPS_PER_BLOCK = BLOCK_THREADS / WARP_SIZE;\nstatic constexpr int TILE_POINTS = 4096;                        // number of data points cached per batch (32KB for float2)\nstatic constexpr unsigned FULL_MASK = 0xFFFFFFFFu;\n\n// Struct with the same memory layout as std::pair<int,float> for device writes.\n// We will reinterpret_cast the result pointer to this type in the kernel.\nstruct PairIF { int first; float second; };\n\n__device__ __forceinline__ int lane_id() { return threadIdx.x & (WARP_SIZE - 1); }\n__device__ __forceinline__ int warp_id_in_block() { return threadIdx.x / WARP_SIZE; }\n\n// Warp-wide bitonic sort of K = items_per_thread * WARP_SIZE elements distributed such that\n// each lane holds items_per_thread consecutive elements in arrays dist[] and idx[].\n// The result is sorted in ascending order of dist[]. For cross-lane exchanges, we use shuffles.\n// For intra-lane exchanges, we directly swap register values.\n// K must be a power of two and items_per_thread must be a power of two.\n__device__ __forceinline__ void warp_bitonic_sort(float dist[], int idx[], int items_per_thread, int K)\n{\n    const int lane = lane_id();\n\n    // Bitonic sort network: size = 2,4,8,...,K\n    for (int size = 2; size <= K; size <<= 1)\n    {\n        // Stride = size/2, size/4, ..., 1\n        for (int stride = size >> 1; stride > 0; stride >>= 1)\n        {\n            if (stride >= items_per_thread)\n            {\n                // Cross-lane compare-exchange. The partner lane differs by j_lane = stride / items_per_thread.\n                const int j_lane = stride / items_per_thread;\n                const int size_lane = size / items_per_thread; // >= 2 in this branch\n                const bool up_block = ((lane & size_lane) == 0); // direction for this lane's block\n                const bool is_lower = ((lane & j_lane) == 0);\n\n                // For each register index, exchange with partner lane's same register index.\n#pragma unroll\n                for (int t = 0; t < 32; ++t)\n                {\n                    if (t >= items_per_thread) break;\n                    float other_d = __shfl_xor_sync(FULL_MASK, dist[t], j_lane);\n                    int   other_i = __shfl_xor_sync(FULL_MASK, idx[t], j_lane);\n\n                    // Decide whether to take partner's value.\n                    bool take_other;\n                    if (is_lower) {\n                        take_other = up_block ? (dist[t] > other_d) : (dist[t] < other_d);\n                    } else {\n                        take_other = up_block ? (dist[t] < other_d) : (dist[t] > other_d);\n                    }\n                    if (take_other) { dist[t] = other_d; idx[t] = other_i; }\n                }\n            }\n            else\n            {\n                // Intra-lane compare-exchange among register entries.\n#pragma unroll\n                for (int t = 0; t < 32; ++t)\n                {\n                    if (t >= items_per_thread) break;\n                    int partner = t ^ stride;\n                    if ((t & stride) == 0) {\n                        // Determine direction based on global index.\n                        int global_i = lane * items_per_thread + t;\n                        bool up = ((global_i & size) == 0);\n                        float a = dist[t];  int ai = idx[t];\n                        float b = dist[partner]; int bi = idx[partner];\n                        bool swap = up ? (a > b) : (a < b);\n                        if (swap) {\n                            dist[t] = b; idx[t] = bi;\n                            dist[partner] = a; idx[partner] = ai;\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\n// Merge the warp's shared candidate buffer (size <= K) with the current intermediate result\n// stored in registers (dist_reg/idx_reg). The buffer is first padded to size K with +inf,\n// then swapped with the registers so that the buffer is in registers. The buffer is sorted,\n// merged with the previous result using reversed pairing to create a bitonic sequence of size K,\n// and finally sorted again to obtain the updated intermediate result.\n//\n// s_cand_dist, s_cand_idx: pointers to the warp's candidate arrays in shared memory\n// s_cand_count: pointer to the warp's candidate count in shared memory\n// dist_reg, idx_reg: per-thread arrays holding items_per_thread elements of the intermediate result\n// items_per_thread, K: as above\n// max_dist: reference to warp-local variable storing current k-th (largest) distance, updated here\n__device__ __forceinline__\nvoid warp_flush_and_merge(float* s_cand_dist, int* s_cand_idx, int* s_cand_count,\n                          float dist_reg[], int idx_reg[], int items_per_thread, int K,\n                          float& max_dist)\n{\n    const int lane = lane_id();\n    int count = 0;\n    if (lane == 0) count = *s_cand_count;\n    count = __shfl_sync(FULL_MASK, count, 0);\n\n    // Pad the remainder of the candidate buffer with +inf to complete size K.\n    const float INF = CUDART_INF_F;\n    for (int p = lane; p < K; p += WARP_SIZE) {\n        if (p >= count) {\n            s_cand_dist[p] = INF;\n            s_cand_idx[p]  = -1;\n        }\n    }\n    __syncwarp();\n\n    // Step 1: Swap candidates into registers; previous result goes to shared memory.\n#pragma unroll\n    for (int t = 0; t < 32; ++t)\n    {\n        if (t >= items_per_thread) break;\n        int pos = lane * items_per_thread + t;\n        float tmp_d = dist_reg[t];\n        int   tmp_i = idx_reg[t];\n        float new_d = s_cand_dist[pos];\n        int   new_i = s_cand_idx[pos];\n        dist_reg[t] = new_d;  idx_reg[t] = new_i;\n        s_cand_dist[pos] = tmp_d; s_cand_idx[pos] = tmp_i;\n    }\n    __syncwarp();\n\n    // Step 2: Sort the buffer now in registers.\n    warp_bitonic_sort(dist_reg, idx_reg, items_per_thread, K);\n    __syncwarp();\n\n    // Step 3: Merge with previous result (now in shared memory) using reversed pairing.\n#pragma unroll\n    for (int t = 0; t < 32; ++t)\n    {\n        if (t >= items_per_thread) break;\n        int pos = lane * items_per_thread + t;\n        int rev = K - 1 - pos; // reverse index for previous result\n        float other_d = s_cand_dist[rev];\n        int   other_i = s_cand_idx[rev];\n        if (other_d < dist_reg[t]) {\n            dist_reg[t] = other_d;\n            idx_reg[t] = other_i;\n        }\n    }\n    __syncwarp();\n\n    // Step 4: Sort the merged bitonic sequence to get updated intermediate result.\n    warp_bitonic_sort(dist_reg, idx_reg, items_per_thread, K);\n    __syncwarp();\n\n    // Update max_dist to the k-th (largest) element, which resides at global index K-1,\n    // i.e., lane 31 and local index items_per_thread-1.\n    float kth_dist = __shfl_sync(FULL_MASK, dist_reg[items_per_thread - 1], WARP_SIZE - 1);\n    if (lane == 0) *s_cand_count = 0;\n    max_dist = kth_dist;\n    __syncwarp();\n}\n\n// Kernel: one warp processes one query.\n__global__ void knn_kernel(const float2* __restrict__ query,\n                           int query_count,\n                           const float2* __restrict__ data,\n                           int data_count,\n                           PairIF* __restrict__ result,\n                           int k)\n{\n    extern __shared__ unsigned char smem_raw[];\n    // Layout shared memory:\n    // [tile points][cand_dist all warps][cand_idx all warps][cand_count per warp]\n    size_t offset = 0;\n\n    float2* s_tile = reinterpret_cast<float2*>(smem_raw + offset);\n    offset += size_t(TILE_POINTS) * sizeof(float2);\n\n    // Align to 16 bytes\n    offset = (offset + 15) & ~size_t(15);\n\n    float* s_all_cand_dist = reinterpret_cast<float*>(smem_raw + offset);\n    offset += size_t(WARPS_PER_BLOCK) * size_t(k) * sizeof(float);\n\n    offset = (offset + 15) & ~size_t(15);\n\n    int* s_all_cand_idx = reinterpret_cast<int*>(smem_raw + offset);\n    offset += size_t(WARPS_PER_BLOCK) * size_t(k) * sizeof(int);\n\n    offset = (offset + 15) & ~size_t(15);\n\n    int* s_cand_count = reinterpret_cast<int*>(smem_raw + offset);\n    offset += size_t(WARPS_PER_BLOCK) * sizeof(int);\n    (void)offset; // suppress unused warning\n\n    const int lane = lane_id();\n    const int warp_in_block = warp_id_in_block();\n    const int global_warp = (blockIdx.x * blockDim.x + threadIdx.x) / WARP_SIZE;\n    const bool warp_active = (global_warp < query_count);\n\n    const int items_per_thread = k / WARP_SIZE; // k guaranteed multiple of 32\n\n    // Pointers to this warp's candidate buffer in shared memory\n    float* s_cand_dist = s_all_cand_dist + warp_in_block * k;\n    int*   s_cand_idx  = s_all_cand_idx  + warp_in_block * k;\n\n    // Initialize candidate count to 0 per warp\n    if (lane == 0) s_cand_count[warp_in_block] = 0;\n    __syncthreads();\n\n    // Load the query point and broadcast to the warp\n    float2 q = make_float2(0.0f, 0.0f);\n    if (warp_active) {\n        if (lane == 0) q = query[global_warp];\n        q.x = __shfl_sync(FULL_MASK, q.x, 0);\n        q.y = __shfl_sync(FULL_MASK, q.y, 0);\n    }\n\n    // Initialize intermediate result in registers: +inf distances and invalid indices.\n    float best_dist[32];\n    int   best_idx[32];\n#pragma unroll\n    for (int t = 0; t < 32; ++t) {\n        if (t < items_per_thread) {\n            best_dist[t] = CUDART_INF_F;\n            best_idx[t]  = -1;\n        }\n    }\n\n    // max_distance initialized to +inf; updated after each merge.\n    float max_dist = CUDART_INF_F;\n\n    // Process data in tiles\n    for (int tile_start = 0; tile_start < data_count; tile_start += TILE_POINTS)\n    {\n        int tile_size = data_count - tile_start;\n        if (tile_size > TILE_POINTS) tile_size = TILE_POINTS;\n\n        // Cooperative load of tile into shared memory\n        for (int i = threadIdx.x; i < tile_size; i += blockDim.x) {\n            s_tile[i] = data[tile_start + i];\n        }\n        __syncthreads();\n\n        if (warp_active)\n        {\n            // Iterate over the tile elements; each lane processes a strided subset.\n            for (int i = lane; i < tile_size; i += WARP_SIZE)\n            {\n                float2 p = s_tile[i];\n                float dx = p.x - q.x;\n                float dy = p.y - q.y;\n                float d2 = dx * dx + dy * dy;\n                int   idx = tile_start + i;\n\n                // Filter by current max_dist\n                bool is_cand = d2 < max_dist;\n                unsigned mask = __ballot_sync(FULL_MASK, is_cand);\n                int n = __popc(mask);\n                if (n > 0)\n                {\n                    int old_count = 0, avail = 0, base_pos = 0;\n                    if (lane == 0) {\n                        old_count = s_cand_count[warp_in_block];\n                        avail = k - old_count;\n                        base_pos = old_count;\n                        // Temporarily update count to avoid races within warp; final value corrected below if overflow.\n                        s_cand_count[warp_in_block] = (n <= avail) ? (old_count + n) : k;\n                    }\n                    old_count = __shfl_sync(FULL_MASK, old_count, 0);\n                    avail     = __shfl_sync(FULL_MASK, avail, 0);\n                    base_pos  = __shfl_sync(FULL_MASK, base_pos, 0);\n\n                    // Rank within the warp's current candidate set\n                    unsigned lane_mask_lt = mask & ((1u << lane) - 1u);\n                    int rank = __popc(lane_mask_lt);\n\n                    // First portion fits before buffer is full\n                    bool take_first = is_cand && (rank < avail);\n                    if (take_first) {\n                        int pos = base_pos + rank;\n                        s_cand_dist[pos] = d2;\n                        s_cand_idx[pos]  = idx;\n                    }\n                    __syncwarp();\n\n                    // Handle overflow: flush buffer and attempt to add remaining candidates.\n                    if (n > avail)\n                    {\n                        // Flush/merge to update best results and max_dist\n                        warp_flush_and_merge(s_cand_dist, s_cand_idx, &s_cand_count[warp_in_block],\n                                             best_dist, best_idx, items_per_thread, k, max_dist);\n\n                        // Remaining candidates are those with rank >= avail. Re-filter with updated max_dist.\n                        bool rem_cand = is_cand && (rank >= avail) && (d2 < max_dist);\n                        unsigned mask2 = __ballot_sync(FULL_MASK, rem_cand);\n                        int n2 = __popc(mask2);\n                        if (n2 > 0) {\n                            int base2 = 0;\n                            if (lane == 0) {\n                                base2 = s_cand_count[warp_in_block]; // should be 0 after flush\n                                s_cand_count[warp_in_block] = base2 + n2;\n                            }\n                            base2 = __shfl_sync(FULL_MASK, base2, 0);\n                            unsigned mask2_lt = mask2 & ((1u << lane) - 1u);\n                            int rank2 = __popc(mask2_lt);\n                            if (rem_cand) {\n                                int pos2 = base2 + rank2;\n                                s_cand_dist[pos2] = d2;\n                                s_cand_idx[pos2]  = idx;\n                            }\n                        }\n                    }\n                }\n            } // end per-tile per-warp loop\n        }\n\n        __syncthreads();\n    } // end tiles loop\n\n    // After the last tile, if there are still candidates in the buffer, flush them.\n    if (warp_active) {\n        int cnt = 0;\n        if (lane == 0) cnt = s_cand_count[warp_in_block];\n        cnt = __shfl_sync(FULL_MASK, cnt, 0);\n        if (cnt > 0) {\n            warp_flush_and_merge(s_cand_dist, s_cand_idx, &s_cand_count[warp_in_block],\n                                 best_dist, best_idx, items_per_thread, k, max_dist);\n        }\n\n        // Write out the result in ascending order to global memory.\n        // For query 'global_warp', its result segment starts at result[global_warp * k].\n        PairIF* out = result + (size_t(global_warp) * size_t(k));\n#pragma unroll\n        for (int t = 0; t < 32; ++t) {\n            if (t >= items_per_thread) break;\n            int pos = lane * items_per_thread + t;\n            out[pos].first  = best_idx[t];\n            out[pos].second = best_dist[t];\n        }\n    }\n}\n\n// Host wrapper. Launches the kernel with appropriate configuration and shared memory.\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k)\n{\n    // Grid configuration: one warp per query.\n    int warps_needed = query_count;\n    int blocks = (warps_needed + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK;\n    dim3 grid(blocks);\n    dim3 block(BLOCK_THREADS);\n\n    // Compute dynamic shared memory size:\n    // tile + per-warp candidate buffers (float dist + int idx) + per-warp candidate counts\n    size_t smem_size = 0;\n    smem_size += size_t(TILE_POINTS) * sizeof(float2);\n    // Align to 16 bytes\n    smem_size = (smem_size + 15) & ~size_t(15);\n    smem_size += size_t(WARPS_PER_BLOCK) * size_t(k) * sizeof(float); // cand dist\n    smem_size = (smem_size + 15) & ~size_t(15);\n    smem_size += size_t(WARPS_PER_BLOCK) * size_t(k) * sizeof(int);   // cand idx\n    smem_size = (smem_size + 15) & ~size_t(15);\n    smem_size += size_t(WARPS_PER_BLOCK) * sizeof(int);               // cand count\n\n    // Opt-in larger dynamic shared memory if necessary (A100/H100 support > 96KB).\n    cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)smem_size);\n\n    // Launch kernel. Cast result pointer to device-friendly layout.\n    PairIF* result_pairif = reinterpret_cast<PairIF*>(result);\n    knn_kernel<<<grid, block, smem_size>>>(query, query_count, data, data_count, result_pairif, k);\n    // Caller is assumed to perform cudaDeviceSynchronize or error checks if desired.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1355, "completion_tokens": 22277, "total_tokens": 23632, "prompt_tokens_details": {"cached_tokens": 1280, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 18112, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db979388190a4991dd3c5c192b7", "custom_id": "knn08-06", "response": {"status_code": 200, "request_id": "b51a40067e608c2b6c27cc97e22ccd87", "body": {"id": "chatcmpl-C4tKBxipvRstxSz2DIVGfHeezZh0m", "object": "chat.completion", "created": 1755282015, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n// Optimized CUDA kernel for k-NN (2D points, squared Euclidean distance) using one warp per query.\n// Requirements implemented:\n// - k is power of two in [32, 1024], so per-thread register storage m = k/32 in [1, 32].\n// - Each warp maintains an intermediate result (top-k) distributed across thread registers: each thread has m consecutive elements.\n// - Each warp has a candidate buffer of size k in shared memory (indices and distances) with a shared counter.\n// - Data points are processed in batches; batches are loaded into shared memory by the entire thread block.\n// - Warp ballot is used to select candidates whose distance < max_distance and to compute write positions in the buffer.\n// - When the buffer is full, a merge is performed as specified using bitonic sort and warp shuffles.\n// - After the final batch, any remaining candidates are merged.\n// - Implementation uses warp shuffles and shared memory barriers appropriately.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Tunables: 8 warps per block = 256 threads. TILE_POINTS controls shared-memory tile size for cached data points.\n#ifndef WARPS_PER_BLOCK\n#define WARPS_PER_BLOCK 8\n#endif\n#ifndef THREADS_PER_BLOCK\n#define THREADS_PER_BLOCK (WARPS_PER_BLOCK * WARP_SIZE)\n#endif\n#ifndef TILE_POINTS\n#define TILE_POINTS 4096   // 4096*8B = 32KB per block for data tiles\n#endif\n\n// Maximum per-thread register storage when k=1024 => m = 1024/32 = 32\n#define MAX_M 32\n\n// Utility: squared Euclidean distance for float2\n__device__ __forceinline__ float squared_l2(const float2 a, const float2 b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    // Use FMA for slightly better precision/performance\n    return fmaf(dy, dy, dx * dx);\n}\n\n// Bitonic sort across K elements distributed across a warp, each thread holds m consecutive elements.\n// - dist[idx] and id[idx] arrays are of length m in registers (per thread).\n// - Global index i of element r in lane 'lane' is i = lane*m + r.\n// - We implement the serial bitonic algorithm in parallel across the warp:\n//   For j >= m, compare/exchange across threads using __shfl_xor_sync with delta = j/m.\n//   For j < m, compare/exchange within a thread between registers at indices r and r^j (only once per pair).\n__device__ __forceinline__ void warp_bitonic_sort(float dist[MAX_M], int id[MAX_M], int m, int K, unsigned lane) {\n    const unsigned FULL_MASK = 0xFFFFFFFFu;\n\n    // Outer loop over subsequence sizes\n    for (int kstage = 2; kstage <= K; kstage <<= 1) {\n        // Inner loop over stride\n        for (int j = kstage >> 1; j > 0; j >>= 1) {\n\n            if (j >= m) {\n                // Cross-lane compare-exchange\n                unsigned delta = (unsigned)(j / m); // because j and m are powers of two, j % m == 0 when j >= m\n                for (int r = 0; r < m; ++r) {\n                    float myVal = dist[r];\n                    int   myIdx = id[r];\n\n                    float othVal = __shfl_xor_sync(FULL_MASK, myVal, delta, WARP_SIZE);\n                    int   othIdx = __shfl_xor_sync(FULL_MASK, myIdx, delta, WARP_SIZE);\n\n                    int i_global = (int)(lane * (unsigned)m + (unsigned)r);\n                    bool up = ((i_global & kstage) == 0);\n\n                    // Compare and select for this element side\n                    bool takeOther = up ? (myVal > othVal) : (myVal < othVal);\n                    if (takeOther) {\n                        dist[r] = othVal;\n                        id[r]   = othIdx;\n                    }\n                }\n                __syncwarp(FULL_MASK);\n            } else {\n                // Intra-lane compare-exchange between registers\n                for (int r = 0; r < m; ++r) {\n                    int ro = r ^ j;\n                    if (ro > r) {\n                        float a = dist[r], b = dist[ro];\n                        int ia = id[r],   ib = id[ro];\n\n                        int i_global = (int)(lane * (unsigned)m + (unsigned)r);\n                        bool up = ((i_global & kstage) == 0);\n\n                        bool swap = up ? (a > b) : (a < b);\n                        if (swap) {\n                            dist[r]  = b;  id[r]  = ib;\n                            dist[ro] = a;  id[ro] = ia;\n                        }\n                    }\n                }\n                __syncwarp(FULL_MASK);\n            }\n        }\n    }\n}\n\n// Merge the warp's candidate buffer (in shared memory) with the warp's intermediate result (in registers).\n// Steps implemented:\n// 0. Intermediate result (in registers) is assumed sorted ascending.\n// 1. Swap the content of the buffer (shared) and the intermediate result (registers) so that the buffer is in registers.\n//    We do a true swap per element between registers and shared memory.\n// 2. Sort the buffer now held in registers using bitonic sort (ascending).\n// 3. Merge: for each position i in [0,k), compute j = k-1-i, read shared element at j,\n//    and set registers[i] = min(registers[i], shared[j]). This forms a bitonic sequence.\n// 4. Sort the merged sequence in registers using bitonic sort to obtain updated intermediate result.\n// Finally, update maxDistance to the last element (k-th neighbor).\n__device__ __forceinline__ void merge_buffer_with_result(\n    float dist[MAX_M], int id[MAX_M],           // registers (intermediate result in/out)\n    float* sBufDistWarp, int* sBufIdxWarp,      // shared memory buffer for this warp (size k)\n    int k, int m, unsigned lane,                // sizes and lane id\n    float& maxDistance,                         // in/out: updated after merge\n    int bufCount                                // current number of valid candidates in shared buffer\n) {\n    const unsigned FULL_MASK = 0xFFFFFFFFu;\n\n    // If buffer not full, pad remaining slots with +inf and invalid index.\n    if (bufCount < k) {\n        for (int r = 0; r < m; ++r) {\n            int i = (int)(lane * (unsigned)m + (unsigned)r);\n            if (i >= bufCount && i < k) {\n                sBufDistWarp[i] = CUDART_INF_F;\n                sBufIdxWarp[i]  = -1;\n            }\n        }\n        __syncwarp(FULL_MASK);\n    }\n\n    // Step 1: Swap between registers (result) and shared (buffer).\n    for (int r = 0; r < m; ++r) {\n        int i = (int)(lane * (unsigned)m + (unsigned)r);\n        float tmpDist = dist[r];\n        int   tmpIdx  = id[r];\n        float bufDist = sBufDistWarp[i];\n        int   bufIdx  = sBufIdxWarp[i];\n        // Swap: registers receive buffer's content; shared receives previous registers' content.\n        dist[r]         = bufDist;\n        id[r]           = bufIdx;\n        sBufDistWarp[i] = tmpDist;\n        sBufIdxWarp[i]  = tmpIdx;\n    }\n    __syncwarp(FULL_MASK);\n\n    // Step 2: Sort the buffer now in registers.\n    warp_bitonic_sort(dist, id, m, k, lane);\n\n    // Step 3: Merge by taking element-wise min of (reg[i], shared[k-1-i]).\n    for (int r = 0; r < m; ++r) {\n        int i = (int)(lane * (unsigned)m + (unsigned)r);\n        int j = k - 1 - i;\n\n        float sDist = sBufDistWarp[j];\n        int   sIdx  = sBufIdxWarp[j];\n\n        if (sDist < dist[r]) {\n            dist[r] = sDist;\n            id[r]   = sIdx;\n        }\n    }\n    __syncwarp(FULL_MASK);\n\n    // Step 4: Sort merged sequence to restore ascending order.\n    warp_bitonic_sort(dist, id, m, k, lane);\n\n    // Update maxDistance = distance of the k-th neighbor (last element).\n    float kth = dist[m - 1];\n    // Broadcast from lane 31\n    maxDistance = __shfl_sync(FULL_MASK, kth, WARP_SIZE - 1, WARP_SIZE);\n}\n\n// Kernel implementing k-NN for 2D points using one warp per query.\n__global__ void knn_kernel(\n    const float2* __restrict__ query,\n    int query_count,\n    const float2* __restrict__ data,\n    int data_count,\n    std::pair<int, float>* __restrict__ result,\n    int k,\n    int tile_points  // set to TILE_POINTS by host\n) {\n    // Sanity: warp size assumed 32\n    static_assert(WARP_SIZE == 32, \"Warp size must be 32\");\n\n    // Compute lane and warp identifiers\n    const unsigned tid = threadIdx.x;\n    const unsigned lane = tid & (WARP_SIZE - 1);\n    const unsigned warp_in_block = tid >> 5; // warp id within block\n    const unsigned warps_per_block = blockDim.x >> 5;\n    const unsigned warp_global = blockIdx.x * warps_per_block + warp_in_block;\n\n    if ((int)warp_global >= query_count) return;\n\n    // Each warp processes one query\n    const float2 q = query[warp_global];\n\n    // Register storage for intermediate result: top-k distances and indices.\n    // Each thread stores m = k/32 consecutive elements in ascending order.\n    const int m = k / WARP_SIZE;\n    float regDist[MAX_M];\n    int   regIdx[MAX_M];\n\n    // Initialize registers to +inf and invalid indices.\n    for (int r = 0; r < m; ++r) {\n        regDist[r] = CUDART_INF_F;\n        regIdx[r]  = -1;\n    }\n\n    // Shared memory layout:\n    // [0]   float sBufDist[warps_per_block * k]\n    // [..]  int   sBufIdx [warps_per_block * k]\n    // [..]  int   sBufCount[warps_per_block]\n    // [..]  float2 sTile[tile_points]\n    extern __shared__ unsigned char smem[];\n    size_t offset = 0;\n\n    auto align_up = [](size_t x, size_t a) { return (x + (a - 1)) & ~(a - 1); };\n\n    // Distances buffer\n    offset = align_up(offset, alignof(float));\n    float* sBufDist = reinterpret_cast<float*>(smem + offset);\n    offset += (size_t)warps_per_block * (size_t)k * sizeof(float);\n\n    // Indices buffer\n    offset = align_up(offset, alignof(int));\n    int* sBufIdx = reinterpret_cast<int*>(smem + offset);\n    offset += (size_t)warps_per_block * (size_t)k * sizeof(int);\n\n    // Buffer counts\n    offset = align_up(offset, alignof(int));\n    int* sBufCount = reinterpret_cast<int*>(smem + offset);\n    offset += (size_t)warps_per_block * sizeof(int);\n\n    // Data tile cache\n    offset = align_up(offset, alignof(float2));\n    float2* sTile = reinterpret_cast<float2*>(smem + offset);\n    // The shared memory allocation from host ensures enough space for tile_points float2s.\n\n    // Per-warp views into shared memory\n    float* sBufDistWarp = sBufDist + warp_in_block * k;\n    int*   sBufIdxWarp  = sBufIdx  + warp_in_block * k;\n\n    // Initialize candidate buffer count for this warp\n    if (lane == 0) sBufCount[warp_in_block] = 0;\n    __syncwarp();\n\n    // Initial max distance is +infinity\n    float maxDistance = CUDART_INF_F;\n\n    // Process data in tiles cached into shared memory by the whole block\n    for (int tileStart = blockIdx.y * tile_points; tileStart < data_count; tileStart += gridDim.y * tile_points) {\n        int remaining = data_count - tileStart;\n        int tileCount = remaining > tile_points ? tile_points : remaining;\n\n        // Cooperative load of the tile\n        for (int t = threadIdx.x; t < tileCount; t += blockDim.x) {\n            sTile[t] = data[tileStart + t];\n        }\n        __syncthreads();\n\n        // Each warp processes the cached tile\n        for (int base = 0; base < tileCount; base += WARP_SIZE) {\n            int idxInTile = base + lane;\n            float2 p;\n            float d = CUDART_INF_F;\n            int globalIdx = -1;\n\n            if (idxInTile < tileCount) {\n                p = sTile[idxInTile];\n                d = squared_l2(q, p);\n                globalIdx = tileStart + idxInTile;\n            }\n\n            // Filter by maxDistance\n            unsigned FULL_MASK = 0xFFFFFFFFu;\n            unsigned candMask = __ballot_sync(FULL_MASK, d < maxDistance);\n            // Process candidates possibly in multiple rounds if buffer capacity is limited\n            while (candMask) {\n                // Determine available slots in the buffer\n                int bufCount = sBufCount[warp_in_block];\n                int available = k - bufCount;\n                if (available == 0) {\n                    // Merge buffer with current result\n                    merge_buffer_with_result(regDist, regIdx, sBufDistWarp, sBufIdxWarp, k, m, lane, maxDistance, bufCount);\n                    if (lane == 0) sBufCount[warp_in_block] = 0;\n                    __syncwarp(FULL_MASK);\n                    bufCount = 0;\n                    available = k;\n                }\n\n                if (available > WARP_SIZE) available = WARP_SIZE;\n\n                // Take the first 'available' set bits from candMask\n                unsigned before = candMask & ((1u << lane) - 1u);\n                int rankInCand = __popc(before);\n                int isSet = ((candMask >> lane) & 1u);\n                int takeThis = isSet && (rankInCand < available);\n                unsigned takeMask = __ballot_sync(FULL_MASK, takeThis);\n                int takeCount = __popc(takeMask);\n\n                // Reserve positions in buffer and broadcast base offset\n                int basePos = 0;\n                if (lane == 0) {\n                    basePos = sBufCount[warp_in_block];\n                    sBufCount[warp_in_block] = basePos + takeCount;\n                }\n                basePos = __shfl_sync(FULL_MASK, basePos, 0, WARP_SIZE);\n\n                if (takeThis) {\n                    int rankTake = __popc(takeMask & ((1u << lane) - 1u));\n                    int writePos = basePos + rankTake;\n                    // Write candidate to shared buffer\n                    sBufDistWarp[writePos] = d;\n                    sBufIdxWarp[writePos]  = globalIdx;\n                }\n\n                // Remove the taken subset; loop continues if there are more candidates\n                candMask &= ~takeMask;\n                __syncwarp(FULL_MASK);\n            }\n        }\n        __syncthreads();\n    }\n\n    // After the last tiles, merge remaining candidates if any\n    int finalCount = sBufCount[warp_in_block];\n    if (finalCount > 0) {\n        merge_buffer_with_result(regDist, regIdx, sBufDistWarp, sBufIdxWarp, k, m, lane, maxDistance, finalCount);\n        if (lane == 0) sBufCount[warp_in_block] = 0;\n        __syncwarp();\n    }\n\n    // Write final sorted top-k results to global memory\n    // Output layout: result[query * k + j] = pair(index, distance), j in [0,k)\n    std::pair<int, float>* out = result + ((size_t)warp_global * (size_t)k);\n    for (int r = 0; r < m; ++r) {\n        int pos = (int)(lane * (unsigned)m + (unsigned)r);\n        // regDist/regIdx are sorted ascending\n        out[pos].first  = regIdx[r];\n        out[pos].second = regDist[r];\n    }\n}\n\n// Host API wrapper\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    // Choose grid/block configuration.\n    // - One warp per query: with WARPS_PER_BLOCK warps per block, we need ceil(query_count / WARPS_PER_BLOCK) blocks along x.\n    // - gridDim.y is used to pipeline tiles of data across blocks in y-dimension; choose a small factor (e.g., 1 or 2).\n    //   We'll use 1 by default to maximize per-block shared memory for tiles.\n    dim3 block(THREADS_PER_BLOCK, 1, 1);\n    dim3 grid((query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK, 1, 1);\n\n    // Dynamic shared memory size:\n    // Per block:\n    // - Candidate buffers: WARPS_PER_BLOCK * (k*(sizeof(float) + sizeof(int)) + sizeof(int))\n    // - Data tile: TILE_POINTS * sizeof(float2)\n    size_t smem_bytes = 0;\n    smem_bytes += (size_t)WARPS_PER_BLOCK * (size_t)k * sizeof(float); // distances\n    smem_bytes += (size_t)WARPS_PER_BLOCK * (size_t)k * sizeof(int);   // indices\n    smem_bytes += (size_t)WARPS_PER_BLOCK * sizeof(int);               // counts\n    smem_bytes += (size_t)TILE_POINTS * sizeof(float2);                // data tile\n\n    // Launch kernel\n    knn_kernel<<<grid, block, smem_bytes>>>(query, query_count, data, data_count, result, k, TILE_POINTS);\n\n    // No device sync or error checks here; assume caller handles them if needed.\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1355, "completion_tokens": 18747, "total_tokens": 20102, "prompt_tokens_details": {"cached_tokens": 1280, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 14720, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7dba4f2c8190844caca3ed15af16", "custom_id": "knn08-07", "response": {"status_code": 200, "request_id": "33464f83bdde790f9d34474a19d955f8", "body": {"id": "chatcmpl-C4tMa0I171qLQtVpIrYNwv8YtqMQs", "object": "chat.completion", "created": 1755282164, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda.h>\n#include <cuda_runtime.h>\n#include <utility>\n#include <math_constants.h>\n\n// Optimized k-NN (k-nearest neighbors) kernel for 2D points using one warp (32 threads) per query.\n// Each warp maintains the intermediate top-k result in registers, distributed so that\n// each thread holds k/32 consecutive entries. A per-warp candidate buffer of size k\n// (indices and distances) is maintained in shared memory. The input data is processed in\n// shared-memory tiles to reduce global memory traffic. When the candidate buffer is full,\n// it is merged with the intermediate result using a bitonic sort-based procedure.\n//\n// Important implementation details:\n// - k is a power of two between 32 and 1024 (inclusive). Therefore, items_per_thread = k / 32 is also a power of two in [1, 32].\n// - Warp-level bitonic sort on a distributed array of length k is implemented using warp shuffles.\n//   For pairs where both elements are in the same lane, a local compare-exchange updates both.\n// - The \"max_distance\" threshold is the current k-th (largest) distance in the intermediate result,\n//   stored in the last element. It is updated after each merge. Candidates farther than this can be filtered early.\n// - A per-warp candidate buffer in shared memory accumulates candidates detected via warp ballots.\n//   When full, it is merged with the intermediate result following the specified 4-step procedure.\n//\n// Thread block organization and shared memory layout:\n// - blockDim.x is a multiple of 32 (warp size). Each warp handles a different query.\n// - Shared memory consists of:\n//     * A common tile cache of float2 points for the entire block.\n//     * Per-warp candidate buffers: indices[k] and distances[k].\n//     * Per-warp candidate counts.\n// - The tile size is chosen to fit within the default 48KB dynamic shared memory limit together with buffers\n//   for up to 4 warps and k up to 1024. This avoids requiring a special shared memory attribute.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// Maximum k and derived per-thread storage.\n// The per-thread register storage arrays are statically sized to cover the maximum k.\nconstexpr int MAX_K = 1024;\nconstexpr int MAX_ITEMS_PER_THREAD = MAX_K / WARP_SIZE;\n\n// Shared-memory tile size (number of points loaded per batch).\n// 1024 points = 8KB (float2). With 4 warps and k=1024, candidate buffers need ~32KB,\n// plus counts. Total ~40KB < 48KB dynamic shared memory default. Safe for A100/H100.\nconstexpr int TILE_POINTS = 1024;\n\n// Utility: lane id within the warp.\nstatic __device__ __forceinline__ int lane_id() {\n    return threadIdx.x & (WARP_SIZE - 1);\n}\n\n// Utility: warp id within the block.\nstatic __device__ __forceinline__ int warp_id_in_block() {\n    return threadIdx.x >> 5;\n}\n\n// Warp-synchronous barrier.\nstatic __device__ __forceinline__ void warp_sync() {\n#if __CUDACC_VER_MAJOR__ >= 9\n    __syncwarp();\n#endif\n}\n\n// Bitonic sort (ascending) on a warp-distributed array of length n = k.\n// Each lane holds items_per_thread consecutive elements in its registers.\n// The global index mapping is: global_i = lane * items_per_thread + j (j in [0, items_per_thread-1]).\n// For communication across lanes, warp shuffles are used. When both elements of a pair\n// in the bitonic network lie in the same lane, a local compare-exchange updates both registers.\nstatic __device__ __forceinline__\nvoid warp_bitonic_sort_distributed(float dist_reg[MAX_ITEMS_PER_THREAD],\n                                   int   idx_reg [MAX_ITEMS_PER_THREAD],\n                                   int items_per_thread, int n, int items_per_thread_log2)\n{\n    const unsigned full_mask = 0xFFFFFFFFu;\n    const int lane = lane_id();\n    const int local_mask = (1 << items_per_thread_log2) - 1;\n\n    // Outer loop: size of subsequences being merged (k in pseudocode), doubling each iteration\n    for (int size = 2; size <= n; size <<= 1) {\n        // Inner loop: distance between compared elements (j in pseudocode), halving each iteration\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            // Process each local register index j\n            for (int j = 0; j < items_per_thread; ++j) {\n                // Compute global indices and partner location\n                const int i_global = (lane << items_per_thread_log2) + j;\n                const int l_global = i_global ^ stride;\n\n                const int lane_l = l_global >> items_per_thread_log2;\n                const int j_l    = l_global & local_mask;\n\n                const bool asc   = ((i_global & size) == 0);\n\n                if (lane_l == lane) {\n                    // Both elements are in the same lane: update both if l_global > i_global to avoid double-processing.\n                    if (l_global > i_global) {\n                        float a_d = dist_reg[j];\n                        int   a_i = idx_reg[j];\n                        float b_d = dist_reg[j_l];\n                        int   b_i = idx_reg[j_l];\n\n                        // Compute min/max with a stable-ish tie-break by index (optional)\n                        bool a_lt_b = (a_d < b_d) || ((a_d == b_d) && (a_i <= b_i));\n                        float minv = a_lt_b ? a_d : b_d;\n                        int   mini = a_lt_b ? a_i : b_i;\n                        float maxv = a_lt_b ? b_d : a_d;\n                        int   maxi = a_lt_b ? b_i : a_i;\n\n                        const bool lower = ((i_global & stride) == 0); // i_global is the lower index in the pair\n                        if (asc) {\n                            // ascending: lower keeps min, upper keeps max\n                            dist_reg[j]   = minv; idx_reg[j]   = mini;\n                            dist_reg[j_l] = maxv; idx_reg[j_l] = maxi;\n                        } else {\n                            // descending: lower keeps max, upper keeps min\n                            dist_reg[j]   = maxv; idx_reg[j]   = maxi;\n                            dist_reg[j_l] = minv; idx_reg[j_l] = mini;\n                        }\n                    }\n                } else {\n                    // Cross-lane pair: symmetric update. Each lane updates its own element.\n                    float other_d = __shfl_sync(full_mask, dist_reg[j_l], lane_l);\n                    int   other_i = __shfl_sync(full_mask, idx_reg [j_l], lane_l);\n\n                    float my_d = dist_reg[j];\n                    int   my_i = idx_reg [j];\n\n                    // Compute min/max with a stable-ish tie-break by index\n                    bool my_lt_other = (my_d < other_d) || ((my_d == other_d) && (my_i <= other_i));\n                    float minv = my_lt_other ? my_d : other_d;\n                    int   mini = my_lt_other ? my_i : other_i;\n                    float maxv = my_lt_other ? other_d : my_d;\n                    int   maxi = my_lt_other ? other_i : my_i;\n\n                    const bool lower = ((i_global & stride) == 0);\n                    // If ascending: lower keeps min, upper keeps max.\n                    // If descending: lower keeps max, upper keeps min.\n                    float out_d = asc ? (lower ? minv : maxv) : (lower ? maxv : minv);\n                    int   out_i = asc ? (lower ? mini : maxi) : (lower ? maxi : mini);\n\n                    dist_reg[j] = out_d;\n                    idx_reg [j] = out_i;\n                }\n            }\n            // Synchronize to ensure updated register values are visible for the next compare-exchange stage\n            warp_sync();\n        }\n    }\n}\n\n// Merge the per-warp candidate buffer (shared memory) with the intermediate result (registers),\n// following the specified 4 steps:\n// 0) Intermediate result (registers) is sorted ascending (invariant).\n// 1) Swap contents: registers <-> buffer, so that the buffer is now in registers.\n// 2) Sort the (buffer) now in registers ascending using bitonic sort.\n// 3) Merge registers and the (old intermediate) in shared memory into a bitonic sequence by:\n//      reg[i] = min( reg[i], shared[k - 1 - i] )\n// 4) Sort the merged result (registers) ascending.\n//\n// The candidate buffer may be partially filled; pad the remainder with +inf so we always operate on k elements.\nstatic __device__ __forceinline__\nvoid warp_merge_full(int warp_local_id,\n                     int k,\n                     int items_per_thread,\n                     int items_per_thread_log2,\n                     float* s_cand_dist_all,\n                     int*   s_cand_idx_all,\n                     int*   s_cand_counts,\n                     float dist_reg[MAX_ITEMS_PER_THREAD],\n                     int   idx_reg [MAX_ITEMS_PER_THREAD],\n                     float &max_distance)\n{\n    const int lane = lane_id();\n    const unsigned full_mask = 0xFFFFFFFFu;\n\n    float* s_cand_dist = s_cand_dist_all + warp_local_id * k;\n    int*   s_cand_idx  = s_cand_idx_all  + warp_local_id * k;\n\n    // Pad the candidate buffer to size k with +inf/-1 so sort/merge operates on exactly k elements.\n    int c = s_cand_counts[warp_local_id];\n    for (int j = 0; j < items_per_thread; ++j) {\n        int i_global = (lane << items_per_thread_log2) + j;\n        if (i_global >= c && i_global < k) {\n            s_cand_dist[i_global] = CUDART_INF_F;\n            s_cand_idx [i_global] = -1;\n        }\n    }\n    warp_sync();\n\n    // Step 1: Swap content (buffer <-> registers), placing buffer into registers.\n    for (int j = 0; j < items_per_thread; ++j) {\n        int i_global = (lane << items_per_thread_log2) + j;\n        float tmp_d = s_cand_dist[i_global];\n        int   tmp_i = s_cand_idx [i_global];\n        s_cand_dist[i_global] = dist_reg[j];\n        s_cand_idx [i_global] = idx_reg [j];\n        dist_reg[j] = tmp_d;\n        idx_reg [j] = tmp_i;\n    }\n    warp_sync();\n\n    // Step 2: Sort the buffer (now in registers) ascending using bitonic sort.\n    warp_bitonic_sort_distributed(dist_reg, idx_reg, items_per_thread, k, items_per_thread_log2);\n    warp_sync();\n\n    // Step 3: Merge buffer (registers) with intermediate (shared) into registers via min(A[i], B[k-1-i]).\n    for (int j = 0; j < items_per_thread; ++j) {\n        int i_global = (lane << items_per_thread_log2) + j;\n        int comp_idx = (k - 1) - i_global;\n        float other_d = s_cand_dist[comp_idx];\n        int   other_i = s_cand_idx [comp_idx];\n\n        // Keep the smaller of the two; for ties, pick the one with smaller index (optional).\n        if ((other_d < dist_reg[j]) || ((other_d == dist_reg[j]) && (other_i < idx_reg[j]))) {\n            dist_reg[j] = other_d;\n            idx_reg [j] = other_i;\n        }\n    }\n    warp_sync();\n\n    // Step 4: Sort the merged (bitonic) sequence in registers ascending.\n    warp_bitonic_sort_distributed(dist_reg, idx_reg, items_per_thread, k, items_per_thread_log2);\n    warp_sync();\n\n    // Update max_distance to the k-th neighbor distance (last element).\n    float new_max = __shfl_sync(full_mask, dist_reg[items_per_thread - 1], WARP_SIZE - 1);\n    max_distance = new_max;\n\n    // Reset candidate count for this warp.\n    if (lane == 0) s_cand_counts[warp_local_id] = 0;\n    warp_sync();\n}\n\n// Kernel: one warp processes one query. The per-warp intermediate result is kept\n// in registers, and a per-warp candidate buffer is kept in shared memory.\n// The data points are processed in shared-memory tiles for cache efficiency.\n__global__ void knn2d_kernel(const float2* __restrict__ query,\n                             int query_count,\n                             const float2* __restrict__ data,\n                             int data_count,\n                             int k,\n                             std::pair<int, float>* __restrict__ result)\n{\n    const int lane    = lane_id();\n    const int warp_lb = warp_id_in_block();\n    const int warps_per_block = blockDim.x / WARP_SIZE;\n    const int warp_global = blockIdx.x * warps_per_block + warp_lb;\n    if (warp_global >= query_count) return;\n\n    // Compute per-thread register storage size. k is guaranteed divisible by 32.\n    const int items_per_thread = k / WARP_SIZE;\n    // log2(items_per_thread) used to map global index to lane/local registers efficiently.\n    int items_per_thread_log2 = 0;\n    if (items_per_thread > 1) {\n        // Since items_per_thread is power of two, __ffs(items_per_thread) - 1 = log2(items_per_thread)\n        items_per_thread_log2 = __ffs(items_per_thread) - 1;\n    }\n\n    // Shared memory layout:\n    // [0, TILE_POINTS) float2 tile buffer for data points (common to block)\n    // then warps_per_block * k int indices\n    // then warps_per_block * k float distances\n    // then warps_per_block int candidate counts\n    extern __shared__ unsigned char smem[];\n    float2* s_tile = reinterpret_cast<float2*>(smem);\n    size_t offset_bytes = size_t(TILE_POINTS) * sizeof(float2);\n\n    int* s_cand_idx_all = reinterpret_cast<int*>(smem + offset_bytes);\n    offset_bytes += size_t(warps_per_block) * size_t(k) * sizeof(int);\n\n    float* s_cand_dist_all = reinterpret_cast<float*>(smem + offset_bytes);\n    offset_bytes += size_t(warps_per_block) * size_t(k) * sizeof(float);\n\n    int* s_cand_counts = reinterpret_cast<int*>(smem + offset_bytes);\n\n    // Pointers to this warp's candidate buffer\n    int*   s_cand_idx  = s_cand_idx_all  + warp_lb * k;\n    float* s_cand_dist = s_cand_dist_all + warp_lb * k;\n\n    // Initialize per-warp candidate count\n    if (lane == 0) s_cand_counts[warp_lb] = 0;\n    warp_sync();\n\n    // Load the query point (all threads in the warp will use it). Use lane 0 to load and broadcast.\n    float2 q;\n    if (lane == 0) {\n        q = query[warp_global];\n    }\n    q.x = __shfl_sync(0xFFFFFFFFu, q.x, 0);\n    q.y = __shfl_sync(0xFFFFFFFFu, q.y, 0);\n\n    // Initialize the intermediate top-k in registers to +inf distances and -1 indices.\n    float reg_dist[MAX_ITEMS_PER_THREAD];\n    int   reg_idx [MAX_ITEMS_PER_THREAD];\n    for (int j = 0; j < items_per_thread; ++j) {\n        reg_dist[j] = CUDART_INF_F;\n        reg_idx [j] = -1;\n    }\n    // The k-th neighbor threshold; initially +inf (no filtering).\n    float max_distance = CUDART_INF_F;\n\n    // Process data in tiles. The whole block cooperatively loads each tile into shared memory.\n    for (int base = 0; base < data_count; base += TILE_POINTS) {\n        const int tile_count = min(TILE_POINTS, data_count - base);\n\n        // Cooperative load into shared memory tile.\n        for (int t = threadIdx.x; t < tile_count; t += blockDim.x) {\n            s_tile[t] = data[base + t];\n        }\n        __syncthreads(); // Ensure tile is loaded before any warp uses it.\n\n        // Each warp processes all points in the shared tile for its own query.\n        for (int t = lane; t < tile_count; t += WARP_SIZE) {\n            float2 p = s_tile[t];\n            float dx = p.x - q.x;\n            float dy = p.y - q.y;\n            float d  = dx * dx + dy * dy;\n            int   gidx = base + t;\n\n            // Filter by current threshold and compact using a warp ballot.\n            unsigned mask = __ballot_sync(0xFFFFFFFFu, d < max_distance);\n            int cnt = __popc(mask);\n\n            if (cnt > 0) {\n                // Check for buffer overflow: if adding cnt exceeds k, merge buffer with current result first.\n                // Lane 0 decides and broadcasts to the warp.\n                int need_flush_int = 0;\n                if (lane == 0) {\n                    int curr = s_cand_counts[warp_lb];\n                    need_flush_int = (curr + cnt > k) ? 1 : 0;\n                }\n                need_flush_int = __shfl_sync(0xFFFFFFFFu, need_flush_int, 0);\n\n                if (need_flush_int) {\n                    // Merge current (possibly partial) buffer: pad to k, swap, sort, merge-min, sort.\n                    warp_merge_full(warp_lb, k, items_per_thread, items_per_thread_log2,\n                                    s_cand_dist_all, s_cand_idx_all, s_cand_counts,\n                                    reg_dist, reg_idx, max_distance);\n                    // Re-evaluate the candidate predicate with the updated threshold.\n                    mask = __ballot_sync(0xFFFFFFFFu, d < max_distance);\n                    cnt = __popc(mask);\n                }\n\n                if (cnt > 0) {\n                    // Reserve slots and write the new candidates into the per-warp buffer.\n                    int base_pos = 0;\n                    if (lane == 0) {\n                        base_pos = s_cand_counts[warp_lb];\n                        s_cand_counts[warp_lb] = base_pos + cnt;\n                    }\n                    base_pos = __shfl_sync(0xFFFFFFFFu, base_pos, 0);\n\n                    // Compute rank of this lane among the true bits for coalesced writes.\n#if __CUDACC_VER_MAJOR__ >= 9\n                    unsigned lane_mask_lt = __lanemask_lt();\n#else\n                    unsigned lane_mask_lt = (1u << lane) - 1u; // Fallback; lane=31 yields undefined shift, but this path is for older toolkits.\n#endif\n                    int rank = __popc(mask & lane_mask_lt);\n\n                    if (d < max_distance) {\n                        int pos = base_pos + rank;\n                        s_cand_dist[pos] = d;\n                        s_cand_idx [pos] = gidx;\n                    }\n\n                    // If buffer is now full, merge it immediately.\n                    int do_flush_int = 0;\n                    if (lane == 0) {\n                        do_flush_int = (s_cand_counts[warp_lb] == k) ? 1 : 0;\n                    }\n                    do_flush_int = __shfl_sync(0xFFFFFFFFu, do_flush_int, 0);\n                    if (do_flush_int) {\n                        warp_merge_full(warp_lb, k, items_per_thread, items_per_thread_log2,\n                                        s_cand_dist_all, s_cand_idx_all, s_cand_counts,\n                                        reg_dist, reg_idx, max_distance);\n                    }\n                }\n            }\n        }\n        __syncthreads(); // Before loading the next tile\n    }\n\n    // After processing all tiles, if the candidate buffer is not empty, merge it once more.\n    int leftover = 0;\n    if (lane == 0) leftover = s_cand_counts[warp_lb];\n    leftover = __shfl_sync(0xFFFFFFFFu, leftover, 0);\n    if (leftover > 0) {\n        warp_merge_full(warp_lb, k, items_per_thread, items_per_thread_log2,\n                        s_cand_dist_all, s_cand_idx_all, s_cand_counts,\n                        reg_dist, reg_idx, max_distance);\n    }\n\n    // Write out the final sorted top-k for this query.\n    // Each thread writes its k/32 consecutive entries.\n    const int out_base = warp_global * k;\n    for (int j = 0; j < items_per_thread; ++j) {\n        int out_idx = out_base + (lane * items_per_thread + j);\n        result[out_idx].first  = reg_idx[j];\n        result[out_idx].second = reg_dist[j];\n    }\n}\n\n// Public host interface. Assumes query, data, and result are device pointers allocated with cudaMalloc.\n// k is a power of two in [32, 1024]. data_count >= k.\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data,  int data_count,\n             std::pair<int, float> *result, int k)\n{\n    // Choose block and grid configuration.\n    // We use 4 warps (128 threads) per block to keep shared memory within 48KB for k up to 1024.\n    const int warps_per_block = 4;\n    const int threads_per_block = warps_per_block * WARP_SIZE;\n    const int num_warps = (query_count + 0) / 1; // one warp per query\n    const int blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Compute dynamic shared memory size: tile + per-warp buffers + per-warp counts.\n    size_t smem_bytes = 0;\n    smem_bytes += size_t(TILE_POINTS) * sizeof(float2);                           // tile\n    smem_bytes += size_t(warps_per_block) * size_t(k) * sizeof(int);              // per-warp candidate idx\n    smem_bytes += size_t(warps_per_block) * size_t(k) * sizeof(float);            // per-warp candidate dist\n    smem_bytes += size_t(warps_per_block) * sizeof(int);                          // per-warp candidate counts\n\n    // Launch kernel.\n    knn2d_kernel<<<blocks, threads_per_block, smem_bytes>>>(query, query_count, data, data_count, k, result);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1355, "completion_tokens": 27833, "total_tokens": 29188, "prompt_tokens_details": {"cached_tokens": 1280, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 22976, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db9ff908190a924443f86b3e161", "custom_id": "knn08-08", "response": {"status_code": 200, "request_id": "37e484994fdec43b4be2a6c1ee807602", "body": {"id": "chatcmpl-C4tPo4BAeefAZGwjG4WcwZdXHmnGd", "object": "chat.completion", "created": 1755282364, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <cuda.h>\n#include <stdint.h>\n\n// Optimized CUDA k-NN (squared L2) for 2D points using one warp (32 threads) per query.\n// The implementation follows the specification:\n// - One warp processes one query.\n// - The intermediate result of size k is stored in registers, distributed so that each thread\n//   keeps k/32 consecutive elements.\n// - Each warp has a candidate buffer of size k in shared memory, plus a per-warp count.\n// - Data points are processed in block-wide shared-memory tiles.\n// - Candidates with distance < max_distance are buffered using warp ballot/prefix sums.\n// - When the candidate buffer is full, it is merged with the intermediate result using:\n//   1) Swap (bring buffer into registers, push current register result into shared)\n//   2) Bitonic sort of the buffer (registers) ascending\n//   3) Bitonic \"min-of-pairs\" merge into a bitonic sequence\n//   4) Bitonic sort again to obtain the updated sorted intermediate result.\n//\n// Notes:\n// - k is a power of two in [32, 1024]. We use warpSize=32, so items_per_thread = k/32 is a power of two in [1, 32].\n// - Bitonic sort stages are implemented with warp shuffles for cross-lane exchanges and register swaps for intra-lane exchanges.\n// - The output array is std::pair<int,float> (index, distance). We write to it using a POD type with identical layout.\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n// POD type matching std::pair<int,float> layout (first=index, second=distance).\nstruct PairIO {\n    int   first;\n    float second;\n};\n\n// Internal pair representation (distance first) for comparisons.\nstruct IdxDist {\n    float d;\n    int   idx;\n};\n\nstatic __device__ __forceinline__ void swap_pair(float &a, int &ai, float &b, int &bi) {\n    float ta = a;  int tai = ai;\n    a = b;         ai = bi;\n    b = ta;        bi = tai;\n}\n\n// Warp-wide bitonic sort of N = items_per_thread * 32 elements distributed so that\n// each lane holds items_per_thread consecutive elements in arrays d[0..items_per_thread-1], idx[...], ascending order.\n// This implementation follows the classic bitonic network:\n// for (size=2; size<=N; size*=2)\n//   for (stride=size/2; stride>0; stride/=2)\n//     compare-exchange i with i^stride in direction up = ((i & size) == 0)\n//\n// Cross-lane exchanges (stride >= items_per_thread) use __shfl_xor_sync with delta = stride / items_per_thread.\n// Intra-lane exchanges (stride < items_per_thread) swap registers directly.\nstatic __device__ __forceinline__\nvoid warp_bitonic_sort(float d[], int idx[], int items_per_thread, int N)\n{\n    const unsigned full_mask = 0xFFFFFFFFu;\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n\n    // size: current bitonic sequence size\n    for (int size = 2; size <= N; size <<= 1) {\n        // stride: current distance between compared indices\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n\n            if (stride < items_per_thread) {\n                // Intra-thread compare-exchange: partner index within the same lane's register array.\n                // Process each pair exactly once by requiring partner > j.\n                for (int j = 0; j < items_per_thread; ++j) {\n                    int partner = j ^ stride;\n                    if (partner > j) {\n                        int i_index = lane * items_per_thread + j;\n                        bool up = ((i_index & size) == 0);\n\n                        float a = d[j];       int ai = idx[j];\n                        float b = d[partner]; int bi = idx[partner];\n\n                        // Compare a and b, keep ascending order if up, descending otherwise.\n                        bool comp = (a > b);\n                        if (up ? comp : !comp) {\n                            // swap\n                            d[j] = b;       idx[j] = bi;\n                            d[partner] = a; idx[partner] = ai;\n                        }\n                    }\n                }\n            } else {\n                // Cross-thread compare-exchange: partner in different lane with the same local register index.\n                // delta is the XOR distance in lane space.\n                int delta = stride / items_per_thread;\n\n                for (int j = 0; j < items_per_thread; ++j) {\n                    int i_index = lane * items_per_thread + j;\n                    bool up = ((i_index & size) == 0);\n\n                    float self_d = d[j];\n                    int   self_i = idx[j];\n\n                    float other_d = __shfl_xor_sync(full_mask, self_d, delta);\n                    int   other_i = __shfl_xor_sync(full_mask, self_i, delta);\n\n                    bool comp = (self_d > other_d);\n                    if (up ? comp : !comp) {\n                        d[j] = other_d;\n                        idx[j] = other_i;\n                    }\n                }\n            }\n        }\n    }\n}\n\n// Merge the full candidate buffer (size k) in shared memory with the current intermediate result in registers.\n// - Load buffer (k items) into regs (bufD, bufI) and overwrite shared buffer with current registers (swap).\n// - Sort buffer ascending (bitonic).\n// - Min-of-pairs with shared array at mirrored indices to form a bitonic sequence in registers.\n// - Sort registers ascending to get the updated intermediate result.\n// On exit, regD/regI contain the updated sorted top-k, and returns the updated max_distance.\nstatic __device__ __forceinline__\nfloat merge_full_buffer_with_registers(IdxDist* __restrict__ warp_buf, // shared memory buffer of size k for this warp\n                                       float regD[], int regI[],\n                                       int items_per_thread, int k)\n{\n    const unsigned full_mask = 0xFFFFFFFFu;\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n\n    // Step 1: Swap - bring buffer into registers and push current registers into shared memory.\n    float bufD[32];\n    int   bufI[32];\n\n    for (int j = 0; j < items_per_thread; ++j) {\n        int g = lane * items_per_thread + j;\n        IdxDist v = warp_buf[g];\n        bufD[j] = v.d;\n        bufI[j] = v.idx;\n\n        // Store current intermediate result into shared memory at the same positions (swap).\n        warp_buf[g].d = regD[j];\n        warp_buf[g].idx = regI[j];\n    }\n    __syncwarp(); // ensure the swap is visible within the warp\n\n    // Step 2: Sort the buffer (in registers) ascending.\n    warp_bitonic_sort(bufD, bufI, items_per_thread, k);\n\n    // Step 3: Pairwise min with mirrored positions from the intermediate result in shared memory.\n    // After swap, warp_buf[] holds the previous intermediate result (sorted ascending).\n    for (int j = 0; j < items_per_thread; ++j) {\n        int i = lane * items_per_thread + j;\n        int oi = k - 1 - i; // mirrored index\n        IdxDist other = warp_buf[oi];\n        float a = bufD[j];\n        int   ai = bufI[j];\n        if (other.d < a) {\n            bufD[j] = other.d;\n            bufI[j] = other.idx;\n        }\n    }\n\n    // Step 4: Sort the merged sequence (in registers) ascending.\n    warp_bitonic_sort(bufD, bufI, items_per_thread, k);\n\n    // Copy back to the register-based intermediate result.\n    for (int j = 0; j < items_per_thread; ++j) {\n        regD[j] = bufD[j];\n        regI[j] = bufI[j];\n    }\n\n    // Update max_distance (the last element, global index k-1, is held by lane 31 at local index items_per_thread-1).\n    float last_local = regD[items_per_thread - 1];\n    float max_distance = __shfl_sync(full_mask, last_local, WARP_SIZE - 1);\n    return max_distance;\n}\n\n// Kernel implementing the k-NN with the described warp-centric method.\n__global__ void knn_kernel(const float2* __restrict__ queries,\n                           int query_count,\n                           const float2* __restrict__ data,\n                           int data_count,\n                           PairIO* __restrict__ out, // same layout as std::pair<int,float>\n                           int k,\n                           int tile_points)\n{\n    extern __shared__ unsigned char smem_raw[];\n    // Shared memory layout per block:\n    // [0, tile_points) float2 cached data tile\n    // Then, per warp: candidate buffer of k IdxDist, and a single int count per warp\n    float2* tile = reinterpret_cast<float2*>(smem_raw);\n    unsigned char* ptr = smem_raw + sizeof(float2) * tile_points;\n\n    const int warps_per_block = blockDim.x / WARP_SIZE;\n    IdxDist* warp_buf_base = reinterpret_cast<IdxDist*>(ptr);\n    int* warp_count_base = reinterpret_cast<int*>(warp_buf_base + warps_per_block * k);\n\n    const int lane = threadIdx.x & (WARP_SIZE - 1);\n    const int warp_in_block = threadIdx.x / WARP_SIZE;\n    const int warp_global = blockIdx.x * warps_per_block + warp_in_block;\n    const bool warp_active = (warp_global < query_count);\n    const unsigned full_mask = 0xFFFFFFFFu;\n\n    // Slice this warp's candidate buffer and count in shared memory.\n    IdxDist* warp_buf = warp_buf_base + warp_in_block * k;\n    int* warp_count = warp_count_base + warp_in_block;\n\n    // Initialize per-warp candidate count to 0.\n    if (lane == 0) {\n        *warp_count = 0;\n    }\n    __syncwarp();\n\n    // Each warp will hold k/32 items in registers.\n    const int items_per_thread = k / WARP_SIZE;\n\n    // Register-resident intermediate result (sorted ascending). Initialize with +inf.\n    float regD[32];\n    int   regI[32];\n    if (warp_active) {\n        for (int j = 0; j < items_per_thread; ++j) {\n            regD[j] = CUDART_INF_F;\n            regI[j] = -1;\n        }\n    }\n\n    // Load this warp's query point and broadcast within the warp.\n    float2 q = make_float2(0.0f, 0.0f);\n    if (warp_active) {\n        if (lane == 0) {\n            q = queries[warp_global];\n        }\n        q.x = __shfl_sync(full_mask, q.x, 0);\n        q.y = __shfl_sync(full_mask, q.y, 0);\n    }\n\n    // Current max_distance (k-th best) for filtering. Initially +inf.\n    float max_distance = CUDART_INF_F;\n\n    // Iterate over the dataset in shared-memory tiles.\n    for (int tile_start = 0; tile_start < data_count; tile_start += tile_points) {\n        int tile_count = data_count - tile_start;\n        if (tile_count > tile_points) tile_count = tile_points;\n\n        // Block-wide load of the current tile into shared memory.\n        for (int i = threadIdx.x; i < tile_count; i += blockDim.x) {\n            tile[i] = data[tile_start + i];\n        }\n        __syncthreads();\n\n        // Each warp processes the cached tile against its query point.\n        if (warp_active) {\n            for (int base = 0; base < tile_count; base += WARP_SIZE) {\n                int idx_in_tile = base + lane;\n                bool valid = (idx_in_tile < tile_count);\n\n                float2 p = valid ? tile[idx_in_tile] : make_float2(0.0f, 0.0f);\n                float dx = p.x - q.x;\n                float dy = p.y - q.y;\n                float dist = dx * dx + dy * dy;\n                int   gidx = tile_start + idx_in_tile;\n\n                // Filter by current max_distance.\n                bool is_candidate = valid && (dist < max_distance);\n                unsigned cand_mask = __ballot_sync(full_mask, is_candidate);\n                int n_cand = __popc(cand_mask);\n\n                if (n_cand > 0) {\n                    // Current buffer fill and capacity.\n                    int base_pos = 0;\n                    int capacity = 0;\n\n                    if (lane == 0) {\n                        int cur = *warp_count;\n                        capacity = k - cur;\n                        int add = (n_cand < capacity) ? n_cand : capacity;\n                        *warp_count = cur + add;\n                        base_pos = cur;\n                    }\n                    base_pos = __shfl_sync(full_mask, base_pos, 0);\n                    capacity = __shfl_sync(full_mask, capacity, 0);\n\n                    int pos_in_mask = __popc(cand_mask & ((1u << lane) - 1u));\n                    bool fits_now = is_candidate && (pos_in_mask < capacity);\n\n                    if (fits_now) {\n                        int wpos = base_pos + pos_in_mask;\n                        warp_buf[wpos].d = dist;\n                        warp_buf[wpos].idx = gidx;\n                    }\n\n                    // If the buffer is (now) full, merge it with the register-resident result.\n                    if (n_cand > capacity) {\n                        // Complete filling made count reach k -> merge.\n                        __syncwarp();\n\n                        // Before merging, ensure the buffer has exactly k items (it is full by construction).\n                        max_distance = merge_full_buffer_with_registers(warp_buf, regD, regI, items_per_thread, k);\n\n                        // Reset candidate count (buffer emptied by merge).\n                        if (lane == 0) *warp_count = 0;\n                        __syncwarp();\n\n                        // Handle leftover candidates from this batch (those beyond 'capacity'), re-filtered by updated max_distance.\n                        bool leftover = is_candidate && (pos_in_mask >= capacity) && (dist < max_distance);\n                        unsigned left_mask = __ballot_sync(full_mask, leftover);\n                        int n_left = __popc(left_mask);\n\n                        if (n_left > 0) {\n                            int lpos = __popc(left_mask & ((1u << lane) - 1u));\n                            if (leftover) {\n                                warp_buf[lpos].d = dist;\n                                warp_buf[lpos].idx = gidx;\n                            }\n                            if (lane == 0) *warp_count = n_left;\n                        }\n                        __syncwarp();\n                    }\n                }\n            }\n        }\n\n        __syncthreads(); // Ensure all warps finished using this tile before loading the next one.\n    }\n\n    // After processing all tiles, merge any remaining candidates in the buffer.\n    if (warp_active) {\n        int remain = 0;\n        if (lane == 0) remain = *warp_count;\n        remain = __shfl_sync(full_mask, remain, 0);\n\n        if (remain > 0) {\n            // Pad the buffer to k with +inf so that we can run the full merge.\n            for (int i = lane; i < k; i += WARP_SIZE) {\n                if (i >= remain) {\n                    warp_buf[i].d = CUDART_INF_F;\n                    warp_buf[i].idx = -1;\n                }\n            }\n            __syncwarp();\n\n            max_distance = merge_full_buffer_with_registers(warp_buf, regD, regI, items_per_thread, k);\n\n            // Reset count (not strictly necessary after final merge).\n            if (lane == 0) *warp_count = 0;\n            __syncwarp();\n        }\n\n        // Store the final sorted k nearest neighbors to output.\n        // Each thread writes its items_per_thread consecutive results.\n        int base_out = warp_global * k + lane * items_per_thread;\n        for (int j = 0; j < items_per_thread; ++j) {\n            PairIO w;\n            w.first = regI[j];\n            w.second = regD[j];\n            out[base_out + j] = w;\n        }\n    }\n}\n\n// Host-side launcher conforming to the requested interface.\nextern \"C\"\nvoid run_knn(const float2 *query, int query_count,\n             const float2 *data, int data_count,\n             std::pair<int, float> *result, int k)\n{\n    // Choose a reasonable number of warps per block.\n    // 8 warps (256 threads) generally provides good occupancy while leaving sufficient shared memory headroom.\n    int warps_per_block = 8;\n    int threads_per_block = warps_per_block * WARP_SIZE;\n\n    // Query device shared memory capacity (opt-in).\n    int device = 0;\n    cudaGetDevice(&device);\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&prop, device);\n\n    // Try to use the maximum opt-in shared memory per block if available; fall back to default.\n    int shmem_limit = prop.sharedMemPerBlockOptin ? prop.sharedMemPerBlockOptin : prop.sharedMemPerBlock;\n\n    // Per-warp candidate buffer bytes: k * sizeof(IdxDist) (8 bytes per entry).\n    size_t per_warp_bytes = static_cast<size_t>(k) * sizeof(IdxDist) + sizeof(int);\n    size_t per_block_warp_bytes = static_cast<size_t>(warps_per_block) * per_warp_bytes;\n\n    // Compute tile size (number of float2) based on remaining shared memory.\n    // Ensure tile_points is a multiple of threads_per_block for coalesced loads.\n    size_t bytes_for_tile = 0;\n    // Leave a small safety margin to avoid over-allocating dynamic smem due to alignment.\n    const size_t safety_margin = 0;\n\n    if (per_block_warp_bytes + safety_margin < static_cast<size_t>(shmem_limit)) {\n        bytes_for_tile = static_cast<size_t>(shmem_limit) - per_block_warp_bytes - safety_margin;\n    } else {\n        // If buffers already exceed shmem_limit with 8 warps, reduce warps_per_block to fit.\n        // This shouldn't happen for k <= 1024 on modern data center GPUs, but we handle it defensively.\n        warps_per_block = 4;\n        threads_per_block = warps_per_block * WARP_SIZE;\n        per_block_warp_bytes = static_cast<size_t>(warps_per_block) * per_warp_bytes;\n        if (per_block_warp_bytes < static_cast<size_t>(shmem_limit)) {\n            bytes_for_tile = static_cast<size_t>(shmem_limit) - per_block_warp_bytes - safety_margin;\n        } else {\n            // As a last resort, set the tile to fit at least one blockDim.x worth of points.\n            bytes_for_tile = 0;\n        }\n    }\n\n    // Ensure the tile can hold at least blockDim.x points (one per thread) if possible.\n    size_t tile_points = 0;\n    if (bytes_for_tile >= sizeof(float2)) {\n        tile_points = bytes_for_tile / sizeof(float2);\n        // Make tile_points a multiple of threads_per_block for easy coalesced loads.\n        if (tile_points < static_cast<size_t>(threads_per_block)) {\n            tile_points = threads_per_block;\n        } else {\n            tile_points = (tile_points / threads_per_block) * threads_per_block;\n        }\n    } else {\n        // Minimal tile as multiple of threads_per_block.\n        tile_points = threads_per_block;\n    }\n\n    // Dynamic shared memory size for the kernel launch.\n    size_t dynamic_smem_size = tile_points * sizeof(float2) + per_block_warp_bytes;\n\n    // Allow the kernel to use the required dynamic shared memory size (opt-in).\n    cudaFuncSetAttribute(knn_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, (int)dynamic_smem_size);\n\n    // Compute grid size: one warp per query.\n    int blocks = (query_count + warps_per_block - 1) / warps_per_block;\n\n    // Launch kernel.\n    // Reinterpret the output pointer as PairIO*, which matches std::pair<int,float> layout.\n    PairIO* out_ptr = reinterpret_cast<PairIO*>(result);\n\n    knn_kernel<<<blocks, threads_per_block, dynamic_smem_size>>>(\n        query, query_count, data, data_count, out_ptr, k, (int)tile_points\n    );\n\n    // Optional: synchronize here if the caller expects the results immediately.\n    // cudaDeviceSynchronize();\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1355, "completion_tokens": 22062, "total_tokens": 23417, "prompt_tokens_details": {"cached_tokens": 1280, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 17600, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db97478819097fcf7b11c429df3", "custom_id": "knn08-09", "response": {"status_code": 200, "request_id": "0a499ecff065d52b6a8984e767cbd092", "body": {"id": "chatcmpl-C4tScQahn5VpCKCTp5rWxHWqZVfxH", "object": "chat.completion", "created": 1755282538, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <utility>\n\n/*\nOptimized warp-cooperative k-NN for 2D points (squared Euclidean distance).\n\nHigh-level overview (see inline comments in code for details):\n- Each warp (32 threads) handles one query point.\n- The block cooperatively loads a tile of data points into shared memory.\n- Each warp computes distances from its query to the cached data points and uses a warp ballot to add the closer-than-max_distance points into a per-warp candidate buffer in shared memory.\n- When the buffer is full (k elements), the warp merges the buffer with its intermediate result (maintained in registers) using:\n    1) Swap buffer and intermediate result (buffer -> registers, result -> shared memory)\n    2) Sort the buffer (registers) with warp-level Bitonic Sort\n    3) Merge (min) buffer elements with the reversed intermediate result (shared memory) to form a bitonic sequence\n    4) Sort the bitonic sequence (registers) with Bitonic Sort -> new intermediate result\n- After all tiles, any residual candidates are merged similarly (padding with +inf).\n- Each thread keeps k/32 consecutive neighbors in registers. During bitonic sort:\n    - For j >= (k/32), swaps are across lanes at the same per-thread index via warp shuffles.\n    - For j < (k/32), swaps are within the same thread between its registers.\n- The final k neighbors per query are stored in ascending order into the output.\n\nAssumptions per prompt:\n- Hardware: NVIDIA A100/H100-class GPUs.\n- k is a power-of-two, 32 <= k <= 1024.\n- data_count >= k.\n- Inputs are large enough to benefit from parallelization.\n- Device pointers are pre-allocated via cudaMalloc.\n\nHyperparameters (chosen to fit within 96KB shared memory per block, while keeping decent tile size and parallelism):\n- WARPS_PER_BLOCK = 8 (256 threads per block)\n- TILE_POINTS = 3072 (each point is float2 => 8 bytes; 3072*8 = 24KB tile)\n- Per-warp candidate buffer: k pairs (index + distance). For k=1024 -> 8KB per warp.\n  Total per block: 8 warps * 8KB = 64KB candidates + 24KB tile \u2248 88KB (< 96KB).\n*/\n\n#ifndef WARP_SIZE\n#define WARP_SIZE 32\n#endif\n\n#define WARPS_PER_BLOCK 8\n#define THREADS_PER_BLOCK (WARPS_PER_BLOCK * WARP_SIZE)\n#define TILE_POINTS 3072\n#define FULL_MASK 0xFFFFFFFFu\n\n// Device utility: squared L2 distance for 2D points\n__device__ __forceinline__ float sqr_dist_2d(const float2 a, const float2 b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    return dx * dx + dy * dy;\n}\n\n// Warp-level broadcast of the current max_distance (k-th element).\n// Each thread holds E = k/32 values in registers, with consecutive positions.\n// The last element (k-1) is stored in lane 31, at local index E-1.\n__device__ __forceinline__ float warp_broadcast_max_distance(const float* reg_dist, int E, int lane) {\n    float last = (lane == WARP_SIZE - 1) ? reg_dist[E - 1] : 0.0f;\n    return __shfl_sync(FULL_MASK, last, WARP_SIZE - 1);\n}\n\n// Warp-level Bitonic Sort over k = 32 * E elements.\n// Each thread stores E consecutive elements in registers: reg_dist[0..E-1], reg_idx[0..E-1].\n// The global linear index of reg element (lane, pos) is i = lane * E + pos.\n// - For j >= E: partner is in a different lane but has the same local index (pos). Use __shfl_xor_sync with lane_mask = j / E.\n// - For j < E: partner is in the same thread at local index pos ^ j. Swap within registers if pos^j > pos (to avoid double swaps).\n__device__ __forceinline__ void warp_bitonic_sort(float* reg_dist, int* reg_idx, int E, int lane) {\n    const int K = E * WARP_SIZE;\n\n    // Outer loop: size of subsequences being merged into bitonic sequences\n    for (int ksize = 2; ksize <= K; ksize <<= 1) {\n        // Inner loop: distance between compared pairs\n        for (int j = ksize >> 1; j > 0; j >>= 1) {\n            if (j >= E) {\n                // Cross-lane compare-exchange at the same local position\n                int lane_mask = j / E; // partner lane = lane ^ lane_mask\n                for (int p = 0; p < E; ++p) {\n                    float v = reg_dist[p];\n                    int   id = reg_idx[p];\n                    float vp = __shfl_xor_sync(FULL_MASK, v, lane_mask);\n                    int   ip = __shfl_xor_sync(FULL_MASK, id, lane_mask);\n\n                    int i_linear = lane * E + p;\n                    int l_linear = i_linear ^ j;\n                    bool up = ((i_linear & ksize) == 0); // sort direction for this pair\n\n                    float minv = (v < vp) ? v : vp;\n                    float maxv = (v < vp) ? vp : v;\n                    int   mini = (v < vp) ? id : ip;\n                    int   maxi = (v < vp) ? ip : id;\n\n                    bool lower = (i_linear < l_linear);\n                    if (up) {\n                        reg_dist[p] = lower ? minv : maxv;\n                        reg_idx[p]  = lower ? mini : maxi;\n                    } else {\n                        reg_dist[p] = lower ? maxv : minv;\n                        reg_idx[p]  = lower ? maxi : mini;\n                    }\n                }\n            } else {\n                // Intra-lane compare-exchange between positions p and p^j\n                for (int p = 0; p < E; ++p) {\n                    int q = p ^ j;\n                    if (q > p) {\n                        float a = reg_dist[p];\n                        float b = reg_dist[q];\n                        int   ia = reg_idx[p];\n                        int   ib = reg_idx[q];\n\n                        int i_linear = lane * E + p;\n                        bool up = ((i_linear & ksize) == 0);\n                        // Compare-swap depending on direction\n                        if (a < b) {\n                            if (!up) {\n                                // descending: swap to place larger at p\n                                reg_dist[p] = b; reg_idx[p] = ib;\n                                reg_dist[q] = a; reg_idx[q] = ia;\n                            }\n                            // else ascending: already ok\n                        } else {\n                            if (up) {\n                                // ascending: swap to place smaller at p\n                                reg_dist[p] = b; reg_idx[p] = ib;\n                                reg_dist[q] = a; reg_idx[q] = ia;\n                            }\n                            // else descending: already ok\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\n// Flush and merge the per-warp candidate buffer with the intermediate result stored in registers.\n// Steps:\n//   0) The current intermediate result in registers is sorted ascending.\n//   1) Swap: bring the candidate buffer (shared memory) into registers; write the register result into the buffer.\n//   2) Sort the register buffer ascending via bitonic sort.\n//   3) Merge-by-min into the registers: reg[i] = min(reg[i], buffer_shared[k-1-i]) to form a bitonic sequence of length k.\n//   4) Sort the registers ascending via bitonic sort to obtain the updated intermediate result.\n// After merge, the per-warp candidate count is reset to 0 and max_distance is updated.\n//\n// Arguments:\n//   s_buf_dist, s_buf_idx: base pointers to shared memory candidate buffers for this warp (length k each)\n//   cand_count: pointer to the per-warp candidate count in shared memory\n//   reg_dist, reg_idx: per-thread register arrays (length E = k/32) storing the intermediate result (sorted ascending)\n//   k: number of neighbors\n//   E: elements per thread (k/32)\n//   lane: lane id within the warp\n//   max_distance: reference to per-thread copy of the current max_distance (broadcasted across warp at the end)\n//\n// Note: This function is warp-synchronous and uses __syncwarp() for intra-warp ordering.\n__device__ __forceinline__ void warp_flush_and_merge(float* s_buf_dist, int* s_buf_idx, int* cand_count,\n                                                     float* reg_dist, int* reg_idx,\n                                                     int k, int E, int lane, float &max_distance) {\n    int cc = *cand_count;\n\n    // Pad the remaining buffer slots with +inf and invalid indices (-1)\n    if (cc < k) {\n        for (int t = lane; t < (k - cc); t += WARP_SIZE) {\n            s_buf_dist[cc + t] = CUDART_INF_F;\n            s_buf_idx[cc + t] = -1;\n        }\n    }\n    __syncwarp();\n\n    // 1) Swap: buffer (shared) <-> intermediate result (registers)\n    //    After this, reg_* holds the candidate buffer and s_buf_* holds the previous intermediate result.\n    for (int p = 0; p < E; ++p) {\n        int i_linear = lane * E + p;\n        float tmpd = s_buf_dist[i_linear];\n        int   tmpi = s_buf_idx[i_linear];\n        s_buf_dist[i_linear] = reg_dist[p];\n        s_buf_idx[i_linear]  = reg_idx[p];\n        reg_dist[p] = tmpd;\n        reg_idx[p]  = tmpi;\n    }\n    __syncwarp();\n\n    // 2) Sort the buffer (now in registers) ascending\n    warp_bitonic_sort(reg_dist, reg_idx, E, lane);\n    __syncwarp();\n\n    // 3) Merge-by-min into registers: reg[i] = min(reg[i], inter_shared[k-1-i])\n    for (int p = 0; p < E; ++p) {\n        int i_linear = lane * E + p;\n        int j_linear = (k - 1) - i_linear;\n        float otherd = s_buf_dist[j_linear];\n        int   otheri = s_buf_idx[j_linear];\n        if (otherd < reg_dist[p]) {\n            reg_dist[p] = otherd;\n            reg_idx[p]  = otheri;\n        }\n    }\n    __syncwarp();\n\n    // 4) Sort the merged bitonic sequence (registers) ascending\n    warp_bitonic_sort(reg_dist, reg_idx, E, lane);\n    __syncwarp();\n\n    // Reset candidate count\n    if (lane == 0) *cand_count = 0;\n    __syncwarp();\n\n    // Update and broadcast max_distance (k-th neighbor distance, i.e., last element)\n    max_distance = warp_broadcast_max_distance(reg_dist, E, lane);\n    __syncwarp();\n}\n\n// Kernel: each warp processes a query. The block loads tiles of data into shared memory.\n__global__ void knn_kernel(const float2* __restrict__ query, int query_count,\n                           const float2* __restrict__ data, int data_count,\n                           std::pair<int, float>* __restrict__ result, int k) {\n\n    // Shared memory layout:\n    // [ tile_data (TILE_POINTS) ][ cand_dists (WARPS_PER_BLOCK*k) ][ cand_idxs (WARPS_PER_BLOCK*k) ][ cand_counts (WARPS_PER_BLOCK) ]\n    extern __shared__ unsigned char smem[];\n    float2* s_data = reinterpret_cast<float2*>(smem);\n    float* s_cand_dists = reinterpret_cast<float*>(s_data + TILE_POINTS);\n    int*   s_cand_idxs  = reinterpret_cast<int*>(s_cand_dists + WARPS_PER_BLOCK * k);\n    int*   s_cand_counts = reinterpret_cast<int*>(s_cand_idxs + WARPS_PER_BLOCK * k);\n\n    const int tid  = threadIdx.x;\n    const int lane = tid & (WARP_SIZE - 1);\n    const int warp_in_block = tid >> 5; // 0..WARPS_PER_BLOCK-1\n\n    const int warps_per_block = blockDim.x / WARP_SIZE;\n    const int warp_global = (blockIdx.x * warps_per_block) + warp_in_block;\n    const int total_warps = gridDim.x * warps_per_block;\n\n    const int E = k >> 5; // Elements per thread\n\n    // Base pointers for this warp's candidate buffer within shared memory\n    float* s_buf_dist = s_cand_dists + warp_in_block * k;\n    int*   s_buf_idx  = s_cand_idxs  + warp_in_block * k;\n    int*   s_buf_count = s_cand_counts + warp_in_block;\n\n    // Initialize per-warp candidate count\n    if (lane == 0) *s_buf_count = 0;\n    __syncwarp();\n\n    // Local registers for intermediate result (sorted ascending invariant)\n    float reg_dist[32]; // E <= 32 (since k <= 1024)\n    int   reg_idx[32];\n    #pragma unroll\n    for (int p = 0; p < 32; ++p) {\n        if (p < E) {\n            reg_dist[p] = CUDART_INF_F;\n            reg_idx[p]  = -1;\n        }\n    }\n    __syncwarp();\n\n    // Grid-stride over query warps\n    for (int qwarp = warp_global; qwarp < query_count; qwarp += total_warps) {\n\n        // Load query point for this warp\n        float2 q;\n        if (qwarp < query_count) {\n            q = query[qwarp];\n        }\n\n        // Reset candidate buffer and intermediate result for this query\n        if (lane == 0) *s_buf_count = 0;\n        __syncwarp();\n        #pragma unroll\n        for (int p = 0; p < 32; ++p) {\n            if (p < E) {\n                reg_dist[p] = CUDART_INF_F;\n                reg_idx[p]  = -1;\n            }\n        }\n        __syncwarp();\n\n        // Initialize max_distance to +inf (broadcast)\n        float max_distance = warp_broadcast_max_distance(reg_dist, E, lane);\n\n        // Process data in tiles\n        for (int tile_base = 0; tile_base < data_count; tile_base += TILE_POINTS) {\n            int tile_size = data_count - tile_base;\n            if (tile_size > TILE_POINTS) tile_size = TILE_POINTS;\n\n            // Load tile into shared memory cooperatively (whole block)\n            for (int i = tid; i < tile_size; i += blockDim.x) {\n                s_data[i] = data[tile_base + i];\n            }\n            __syncthreads(); // ensure tile is loaded before any warp consumes it\n\n            // Each warp processes the tile against its query\n            if (qwarp < query_count) {\n                // Iterate points in the tile in groups of 32 so that each lane handles one point per iteration\n                for (int i = lane; i < tile_size; i += WARP_SIZE) {\n                    // Compute squared distance to query\n                    float2 d = s_data[i];\n                    float dist = sqr_dist_2d(q, d);\n                    int   gidx = tile_base + i;\n\n                    // Check candidate against current max_distance\n                    int is_cand = (dist < max_distance) ? 1 : 0;\n\n                    // Warp ballot of candidates\n                    unsigned mask = __ballot_sync(FULL_MASK, is_cand);\n                    int n_cand = __popc(mask);\n\n                    if (n_cand > 0) {\n                        // Read current count\n                        int cc = 0;\n                        if (lane == 0) cc = *s_buf_count;\n                        cc = __shfl_sync(FULL_MASK, cc, 0);\n\n                        // If adding all candidates would overflow the buffer, flush-merge first.\n                        if (cc + n_cand > k) {\n                            // Merge the existing buffer with the intermediate result.\n                            warp_flush_and_merge(s_buf_dist, s_buf_idx, s_buf_count,\n                                                 reg_dist, reg_idx, k, E, lane, max_distance);\n                            // Re-evaluate candidacy with updated max_distance.\n                            is_cand = (dist < max_distance) ? 1 : 0;\n                            mask = __ballot_sync(FULL_MASK, is_cand);\n                            n_cand = __popc(mask);\n                            if (n_cand == 0) {\n                                // No longer a candidate\n                                continue;\n                            }\n                            if (lane == 0) cc = *s_buf_count;\n                            cc = __shfl_sync(FULL_MASK, cc, 0);\n                        }\n\n                        // Insert candidates into the buffer at positions [cc, cc + n_cand)\n                        int my_offset = __popc(mask & ((1u << lane) - 1));\n                        int base = cc;\n                        base = __shfl_sync(FULL_MASK, base, 0);\n                        if (is_cand) {\n                            int pos = base + my_offset;\n                            s_buf_dist[pos] = dist;\n                            s_buf_idx[pos]  = gidx;\n                        }\n                        __syncwarp();\n                        if (lane == 0) {\n                            *s_buf_count = cc + n_cand;\n                        }\n                        __syncwarp();\n\n                        // If the buffer is now full, flush-merge it immediately.\n                        int now_cc = 0;\n                        if (lane == 0) now_cc = *s_buf_count;\n                        now_cc = __shfl_sync(FULL_MASK, now_cc, 0);\n                        if (now_cc == k) {\n                            warp_flush_and_merge(s_buf_dist, s_buf_idx, s_buf_count,\n                                                 reg_dist, reg_idx, k, E, lane, max_distance);\n                        }\n                    }\n                }\n            }\n\n            __syncthreads(); // ensure all warps finished with the tile before loading the next\n        }\n\n        // After the last tile, if there are residual candidates, flush-merge them.\n        if (qwarp < query_count) {\n            int cc_final = 0;\n            if (lane == 0) cc_final = *s_buf_count;\n            cc_final = __shfl_sync(FULL_MASK, cc_final, 0);\n            if (cc_final > 0) {\n                warp_flush_and_merge(s_buf_dist, s_buf_idx, s_buf_count,\n                                     reg_dist, reg_idx, k, E, lane, max_distance);\n            }\n\n            // Write out the final k nearest neighbors for this query.\n            // Each thread writes its E consecutive elements.\n            int out_base = qwarp * k + (lane * E);\n            #pragma unroll\n            for (int p = 0; p < 32; ++p) {\n                if (p < E) {\n                    result[out_base + p].first  = reg_idx[p];\n                    result[out_base + p].second = reg_dist[p];\n                }\n            }\n        }\n\n        __syncthreads(); // synchronize before the next query warp (to keep shared memory usage predictable)\n    }\n}\n\n// Host entry point: launches the kernel.\n// - query, data, result are device pointers (allocated via cudaMalloc)\n// - k is a power of two, 32 <= k <= 1024\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    // Launch configuration\n    dim3 block(THREADS_PER_BLOCK, 1, 1);\n    // Number of blocks to cover all query warps; grid-stride loop will handle excess.\n    int warps_needed = (query_count + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK;\n    int max_blocks = 65535;\n    dim3 grid(warps_needed < max_blocks ? warps_needed : max_blocks, 1, 1);\n\n    // Dynamic shared memory size:\n    // tile + per-warp candidates (dists + idxs) + per-warp candidate counts\n    size_t smem_size = 0;\n    smem_size += sizeof(float2) * TILE_POINTS;\n    smem_size += WARPS_PER_BLOCK * k * sizeof(float); // distances\n    smem_size += WARPS_PER_BLOCK * k * sizeof(int);   // indices\n    smem_size += WARPS_PER_BLOCK * sizeof(int);       // counts\n\n    // Launch\n    knn_kernel<<<grid, block, smem_size>>>(query, query_count, data, data_count, result, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1355, "completion_tokens": 20702, "total_tokens": 22057, "prompt_tokens_details": {"cached_tokens": 1280, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 16192, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
{"id": "batch_req_689f7db97f688190925a43c134433024", "custom_id": "knn08-10", "response": {"status_code": 200, "request_id": "bab34c282970699fb763f82225320bdc", "body": {"id": "chatcmpl-C4tVM5eRaNSfh54moAvtS5qHeLVdi", "object": "chat.completion", "created": 1755282708, "model": "gpt-5-2025-08-07", "choices": [{"index": 0, "message": {"role": "assistant", "content": "#include <cuda_runtime.h>\n#include <stdint.h>\n\n// This implementation assigns one warp (32 threads) to each query.\n// Each thread keeps k/32 consecutive nearest neighbors in registers.\n// Candidate buffers of size k per warp are stored in shared memory.\n// Batches of data points are loaded into shared memory and processed by all warps in the block.\n// When the candidate buffer is full (or after the final batch), we merge it into the intermediate\n// result using the described bitonic sort and bitonic merge technique.\n//\n// Notes:\n// - k is a power of two in [32, 1024]; therefore k/32 (KK) is a power of two in [1, 32].\n// - The code uses warp-synchronous programming with __shfl_sync and __ballot_sync.\n// - The intermediate result (top-k) is kept sorted ascending at all times (after merges).\n\n// Tunable parameters and constants\n#ifndef KNN_WARP_SIZE\n#define KNN_WARP_SIZE 32\n#endif\n\n#ifndef KNN_THREADS_PER_BLOCK\n#define KNN_THREADS_PER_BLOCK 256  // 8 warps per block\n#endif\n\n#ifndef KNN_TILE_POINTS\n#define KNN_TILE_POINTS 4096       // Number of data points per shared-memory tile (~32KB)\n#endif\n\n#ifndef KNN_MAX_K\n#define KNN_MAX_K 1024             // Maximum k we support\n#endif\n\n#define FULL_MASK 0xFFFFFFFFu\n\nstruct PairOut {\n    int first;\n    float second;\n};\n\n// Compute squared Euclidean distance between two float2 points\n__device__ __forceinline__ float distance2(const float2 &a, const float2 &b) {\n    float dx = a.x - b.x;\n    float dy = a.y - b.y;\n    // Use FMA to improve performance and precision\n    return fmaf(dx, dx, dy * dy);\n}\n\n// Warp-wide bitonic sort for k elements distributed across threads.\n// Each thread holds KK = k / 32 consecutive elements in registers, stored in arrays dist[] and idx[].\n// The sort order is ascending by distance.\n__device__ __forceinline__ void warp_bitonic_sort(float dist[], int idx[], int k, int KK, int lane) {\n    // m = log2(KK). Since KK is a power of two, we can use __ffs(KK)-1 (ffs returns 1-based index).\n    const int m = __ffs(KK) - 1;\n\n    // Outer loop over bitonic sequence length\n    for (int size = 2; size <= k; size <<= 1) {\n        // Inner loop over half-clean distance \"stride\"\n        for (int stride = size >> 1; stride > 0; stride >>= 1) {\n            // Inter-thread or intra-thread phase depends on whether stride toggles a bit in the KK part (low m bits)\n            if (stride < KK) {\n                // Intra-thread compare-exchange: partner is within the same thread registers at index t^stride\n                for (int t = 0; t < KK; t++) {\n                    // Restrict updates to one side of each pair to avoid double-updates within the thread\n                    if ((t & stride) == 0) {\n                        int t2 = t ^ stride;\n\n                        int i = lane * KK + t;\n                        bool up = ((i & size) == 0);\n\n                        float a_d = dist[t];\n                        int   a_i = idx[t];\n\n                        float b_d = dist[t2];\n                        int   b_i = idx[t2];\n\n                        // Compare\n                        bool swap = (a_d > b_d);\n                        // If descending, reverse comparison\n                        if (!up) swap = !swap;\n\n                        // Perform compare-exchange\n                        float lo_d = swap ? b_d : a_d;\n                        int   lo_i = swap ? b_i : a_i;\n                        float hi_d = swap ? a_d : b_d;\n                        int   hi_i = swap ? a_i : b_i;\n\n                        if (up) {\n                            dist[t]  = lo_d; idx[t]  = lo_i;\n                            dist[t2] = hi_d; idx[t2] = hi_i;\n                        } else {\n                            dist[t]  = hi_d; idx[t]  = hi_i;\n                            dist[t2] = lo_d; idx[t2] = lo_i;\n                        }\n                    }\n                }\n                // No inter-thread data dependency; __syncwarp() not strictly required here\n            } else {\n                // Inter-thread compare-exchange: partner lane differs; register index t stays the same.\n                int partner_delta = stride >> m; // lane xor mask\n                unsigned mask = FULL_MASK;\n\n                for (int t = 0; t < KK; t++) {\n                    int i = lane * KK + t;\n                    bool up = ((i & size) == 0);\n\n                    float a_d = dist[t];\n                    int   a_i = idx[t];\n\n                    float b_d = __shfl_xor_sync(mask, a_d, partner_delta);\n                    int   b_i = __shfl_xor_sync(mask, a_i, partner_delta);\n\n                    bool swap = (a_d > b_d);\n                    if (!up) swap = !swap;\n\n                    float lo_d = swap ? b_d : a_d;\n                    int   lo_i = swap ? b_i : a_i;\n                    float hi_d = swap ? a_d : b_d;\n                    int   hi_i = swap ? a_i : b_i;\n\n                    if (up) {\n                        dist[t] = lo_d; idx[t] = lo_i;\n                    } else {\n                        dist[t] = hi_d; idx[t] = hi_i;\n                    }\n                }\n                __syncwarp(); // Ensure warp-synchronous progress across inter-thread exchanges\n            }\n        }\n    }\n}\n\n// Merge the candidate buffer (shared memory) with the intermediate result (registers) as described:\n// 0) The intermediate result in registers is sorted ascending (invariant).\n// 1) Swap content so that the buffer is in registers.\n// 2) Sort the buffer (now in registers) ascending using bitonic sort.\n// 3) Merge buffer and intermediate result into registers by taking min of pairs (i, k - i - 1).\n// 4) Sort the merged result ascending using bitonic sort.\n// After this function, the registers contain the updated intermediate result, sorted ascending,\n// and maxDist is updated to the last (k-th) element.\n__device__ __forceinline__ void warp_merge_buffer_with_result(\n    float bestDist[], int bestIdx[],           // registers: current intermediate top-k\n    float tmpDist[], int tmpIdx[],             // registers: temporary storage for buffer during swap/sort\n    volatile float *bufDist, volatile int *bufIdx, // shared memory: candidate buffer (size k) per warp\n    volatile int *bufCountPtr,                 // shared: candidate count\n    int k, int KK, int lane, float &maxDist)\n{\n    int cc = *bufCountPtr;\n\n    // Pad the buffer with +inf if it's partially filled\n    if (cc < k) {\n        for (int p = lane; p < (k - cc); p += KNN_WARP_SIZE) {\n            int pos = cc + p;\n            bufDist[pos] = CUDART_INF_F;\n            bufIdx[pos]  = -1;\n        }\n    }\n    __syncwarp();\n\n    // Step 1: Swap content of buffer and intermediate result so that buffer goes to registers.\n    // Load buffer into tmpDist/tmpIdx (registers), and write bestDist/bestIdx into shared buffer.\n    for (int t = 0; t < KK; t++) {\n        int i = lane * KK + t;\n        float bd = bufDist[i];\n        int   bi = bufIdx[i];\n        tmpDist[t] = bd;\n        tmpIdx[t]  = bi;\n\n        bufDist[i] = bestDist[t];\n        bufIdx[i]  = bestIdx[t];\n    }\n    __syncwarp();\n\n    // Step 2: Sort buffer (now in tmpDist/tmpIdx registers) ascending\n    warp_bitonic_sort(tmpDist, tmpIdx, k, KK, lane);\n\n    __syncwarp();\n\n    // Step 3: Merge buffer and previous result into registers by taking the element-wise minimum\n    // of tmpDist[i] and bufDist[k - 1 - i].\n    for (int t = 0; t < KK; t++) {\n        int i = lane * KK + t;\n        int j = k - 1 - i;\n\n        float other_d = bufDist[j];\n        int   other_i = bufIdx[j];\n\n        float a_d = tmpDist[t];\n        int   a_i = tmpIdx[t];\n\n        bool take_a = (a_d < other_d);\n        bestDist[t] = take_a ? a_d : other_d;\n        bestIdx[t]  = take_a ? a_i : other_i;\n    }\n    __syncwarp();\n\n    // Step 4: Sort the merged result ascending to restore invariant\n    warp_bitonic_sort(bestDist, bestIdx, k, KK, lane);\n\n    __syncwarp();\n\n    // Update maxDist to the k-th element (last one in ascending order).\n    float kth = bestDist[KK - 1];\n    // Broadcast from last lane\n    maxDist = __shfl_sync(FULL_MASK, kth, KNN_WARP_SIZE - 1);\n\n    // Reset buffer count\n    if (lane == 0) {\n        *bufCountPtr = 0;\n    }\n    __syncwarp();\n}\n\n// The main kernel. One warp processes one query point.\n__global__ void knn_kernel(\n    const float2 * __restrict__ query, int query_count,\n    const float2 * __restrict__ data,  int data_count,\n    PairOut * __restrict__ result,\n    int k)\n{\n    // Compute warp and lane identifiers\n    const int lane = threadIdx.x & (KNN_WARP_SIZE - 1);\n    const int warpInBlock = threadIdx.x >> 5; // /32\n    const int warpsPerBlock = blockDim.x >> 5;\n\n    const int queryIdx = blockIdx.x * warpsPerBlock + warpInBlock;\n    if (queryIdx >= query_count) return;\n\n    // Each warp handles a single query\n    const float2 q = query[queryIdx];\n\n    // Per-thread register storage for the intermediate top-k list (ascending order).\n    // Each thread holds k/32 consecutive entries.\n    const int KK = k / KNN_WARP_SIZE; // number of elements per thread\n    float bestDist[KNN_MAX_K / KNN_WARP_SIZE];\n    int   bestIdx [KNN_MAX_K / KNN_WARP_SIZE];\n#pragma unroll\n    for (int t = 0; t < KNN_MAX_K / KNN_WARP_SIZE; t++) {\n        if (t < KK) {\n            bestDist[t] = CUDART_INF_F;\n            bestIdx[t]  = -1;\n        }\n    }\n\n    // Temporary registers used during merge swap (to store buffer contents)\n    float tmpDist[KNN_MAX_K / KNN_WARP_SIZE];\n    int   tmpIdx [KNN_MAX_K / KNN_WARP_SIZE];\n\n    // Shared memory layout:\n    // [0] float2 tile[KNN_TILE_POINTS]\n    // [1] per-warp candidate buffers: dist[warps*k], idx[warps*k]\n    // [2] per-warp candidate counts: int[warps]\n    extern __shared__ unsigned char smem[];\n    size_t off = 0;\n\n    // Shared tile for cached data points\n    float2 *sTile = reinterpret_cast<float2*>(smem + off);\n    off += sizeof(float2) * KNN_TILE_POINTS;\n\n    // Align to 8 bytes for safety\n    off = (off + 7) & ~size_t(7);\n\n    // Candidate buffers for all warps in the block\n    float *candDistBase = reinterpret_cast<float*>(smem + off);\n    off += sizeof(float) * (size_t)warpsPerBlock * (size_t)k;\n\n    int *candIdxBase = reinterpret_cast<int*>(smem + off);\n    off += sizeof(int) * (size_t)warpsPerBlock * (size_t)k;\n\n    // Candidate counters per warp\n    int *candCountBase = reinterpret_cast<int*>(smem + off);\n    // off += sizeof(int) * warpsPerBlock; // Not needed further\n\n    // Pointers for this warp's candidate buffer and count\n    float *myCandDist = candDistBase + (size_t)warpInBlock * (size_t)k;\n    int   *myCandIdx  = candIdxBase  + (size_t)warpInBlock * (size_t)k;\n    volatile int *myCandCount = candCountBase + warpInBlock;\n\n    if (lane == 0) {\n        *myCandCount = 0;\n    }\n    __syncwarp();\n\n    // Current threshold distance: distance of the k-th nearest element in the intermediate result.\n    float maxDist = CUDART_INF_F;\n\n    // Process the data in tiles\n    for (int tileStart = 0; tileStart < data_count; tileStart += KNN_TILE_POINTS) {\n        int tileSize = data_count - tileStart;\n        if (tileSize > KNN_TILE_POINTS) tileSize = KNN_TILE_POINTS;\n\n        // Cooperative loading of the tile into shared memory by the whole block\n        for (int t = threadIdx.x; t < tileSize; t += blockDim.x) {\n            sTile[t] = data[tileStart + t];\n        }\n        __syncthreads();\n\n        // Each warp processes all points in the tile\n        for (int t = lane; t < tileSize; t += KNN_WARP_SIZE) {\n            float2 d = sTile[t];\n            float dist = distance2(q, d);\n\n            // Filter by current maxDist (k-th neighbor distance)\n            int take = (dist < maxDist) ? 1 : 0;\n\n            // Warp ballot to count candidates\n            unsigned mask = __ballot_sync(FULL_MASK, take);\n            int add = __popc(mask);\n\n            if (add > 0) {\n                // If the buffer would overflow, merge it first\n                int needMerge = 0;\n                if (lane == 0) {\n                    int cc = *myCandCount;\n                    if (cc == k || (cc + add > k)) {\n                        needMerge = 1;\n                    }\n                }\n                needMerge = __shfl_sync(FULL_MASK, needMerge, 0);\n                if (needMerge) {\n                    // Merge the buffer with the intermediate result\n                    warp_merge_buffer_with_result(\n                        bestDist, bestIdx,\n                        tmpDist, tmpIdx,\n                        myCandDist, myCandIdx, myCandCount,\n                        k, KK, lane, maxDist\n                    );\n                }\n\n                // Reserve space in the candidate buffer\n                int base = 0;\n                if (lane == 0) {\n                    int cc = *myCandCount;\n                    *myCandCount = cc + add;\n                    base = cc;\n                }\n                base = __shfl_sync(FULL_MASK, base, 0);\n\n                if (take) {\n                    // Offset of this lane's element within the newly added block\n                    unsigned lower = mask & ((1u << lane) - 1u);\n                    int offset = __popc(lower);\n                    int pos = base + offset;\n                    // Store candidate (we ensured no overflow)\n                    myCandDist[pos] = dist;\n                    myCandIdx[pos]  = tileStart + t;\n                }\n            }\n        }\n\n        // If the buffer is full after processing this tile, merge it\n        int needMergeAfterTile = 0;\n        if (lane == 0) {\n            int cc = *myCandCount;\n            if (cc == k) needMergeAfterTile = 1;\n        }\n        needMergeAfterTile = __shfl_sync(FULL_MASK, needMergeAfterTile, 0);\n        if (needMergeAfterTile) {\n            warp_merge_buffer_with_result(\n                bestDist, bestIdx,\n                tmpDist, tmpIdx,\n                myCandDist, myCandIdx, myCandCount,\n                k, KK, lane, maxDist\n            );\n        }\n\n        __syncthreads(); // Ensure all warps finished using sTile before reloading\n    }\n\n    // Final merge if the buffer has remaining candidates\n    int finalMerge = 0;\n    if (lane == 0) {\n        int cc = *myCandCount;\n        if (cc > 0) finalMerge = 1;\n    }\n    finalMerge = __shfl_sync(FULL_MASK, finalMerge, 0);\n    if (finalMerge) {\n        warp_merge_buffer_with_result(\n            bestDist, bestIdx,\n            tmpDist, tmpIdx,\n            myCandDist, myCandIdx, myCandCount,\n            k, KK, lane, maxDist\n        );\n    }\n\n    // Write out the results for this query in row-major order:\n    // result[queryIdx * k + j] = (index, distance), j in [0, k)\n    size_t outBase = (size_t)queryIdx * (size_t)k;\n    for (int t = 0; t < KK; t++) {\n        int j = lane * KK + t;\n        PairOut out;\n        out.first  = bestIdx[t];\n        out.second = bestDist[t];\n        result[outBase + j] = out;\n    }\n}\n\n// Host function to launch the kernel.\n// query: device pointer to float2 query points\n// data: device pointer to float2 data points\n// result: device pointer to std::pair<int, float> results (treated as PairOut here)\n// k: power of two in [32, 1024]\nvoid run_knn(const float2 *query, int query_count, const float2 *data, int data_count, std::pair<int, float> *result, int k) {\n    if (query_count <= 0 || data_count <= 0 || k <= 0) return;\n\n    const int threadsPerBlock = KNN_THREADS_PER_BLOCK; // multiple of 32\n    const int warpsPerBlock = threadsPerBlock / KNN_WARP_SIZE;\n    const int blocks = (query_count + warpsPerBlock - 1) / warpsPerBlock;\n\n    // Compute dynamic shared memory size:\n    // tile: KNN_TILE_POINTS * sizeof(float2)\n    // per-warp candidate buffers: warpsPerBlock * k * (sizeof(float) + sizeof(int))\n    // per-warp counts: warpsPerBlock * sizeof(int)\n    size_t smem_size = 0;\n    smem_size += sizeof(float2) * (size_t)KNN_TILE_POINTS;\n    // Align to 8\n    smem_size = (smem_size + 7) & ~size_t(7);\n    smem_size += (size_t)warpsPerBlock * (size_t)k * (sizeof(float) + sizeof(int));\n    smem_size += sizeof(int) * (size_t)warpsPerBlock;\n\n    PairOut *out = reinterpret_cast<PairOut*>(result);\n\n    knn_kernel<<<blocks, threadsPerBlock, smem_size>>>(query, query_count, data, data_count, out, k);\n}", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1355, "completion_tokens": 19295, "total_tokens": 20650, "prompt_tokens_details": {"cached_tokens": 1280, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 15168, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
