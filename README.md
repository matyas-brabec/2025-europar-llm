# 🔬🤖 Tutoring LLM into a Better CUDA Optimizer ⚡🎯

[![license](https://img.shields.io/badge/license-MIT-blue.svg)](./LICENCE) [![doi](https://img.shields.io/badge/DOI-TODO-blue)](todo)

This repository is associated with the following paper:

```
@inproceedings { TODO }
```

## About 🧠💡

Large language models (LLMs) are increasingly used for code generation, debugging, and optimization. This project explores their ability to generate efficient CUDA code for well-known parallel computing tasks. While LLMs can apply some optimizations independently, their performance improves significantly with targeted guidance. Our evaluation combines automatic benchmarking and manual code review, revealing that LLMs require tutoring to reach expert-level optimization but show promising potential for assisting developers.

## Index 🔍📖

To navigate this repository more easily, here is an index:

| 📂 Section | 📝 Description with Links |
| --- | --- |
| ⚙️ Managing Code | The main folder for managing the implementation of three provided test cases (*histogram*, *Game of Life*, and *k-NN*) is in the [~/framework](./framework) directory. For further information, read the [README](./framework/README.md) inside. |
| | The subdirectories [~/framework/histogram](./framework/histogram/), [~/framework/game-of-life](./framework/game-of-life/), and [~/framework/knn](./framework/knn/) contain the respective test cases. Each has a `<Hist\|GoL\|kNN>/reference` subfolder with reference solutions. |
| 🔗 Open API | The code managing calls to OpenAI's ChatGPT is located in [~/gpt-querying](./gpt-querying/) along with a [README](./gpt-querying/README.md). |
| 💬 Interactive Prompting | All data related to interactive prompting is in [~/interactive-scenarios](./interactive-scenarios/), including scripts for each problem, resulting source files, and some notes. |
| ✍️ Single-Response Prompts | All prompts are in the [~/prompts](./prompts/) folder. Apart from the prompt files (`.prompt.md`), it also contains common parts that can be included in prompts. |
| 🤖 Generated "Single-Response" Sources | Generated code in the *"Single-Response"* mode is in [~/results](./results/). Individual solutions for each prompt and its 10 attempts are in [~/results/histogram](./results/histogram/), [~/results/gol](./results/gol/), and [~/results/knn](./results/knn/). |
| 🧐 Reviews | Code reviews for *"Single-Response"* generated code are in the [~/reviews](./reviews/) directory. |
| 📊 Measurements | Code measurements for tested architectures (`Ampere`, `Volta`, and `Hopper`) are in [~/measured-times](./measured-times/). The format is CSV. For each architecture and problem, there are three CSVs: one for *"Single-Response"* codes, one for reference solutions, and one for interactive prompting. The pregenerated graph are located in [~/measured-times/graphs](./measured-times/graphs/). |

## Tutorial 🚀

We used **GCC 13.2.0** and **NVCC 12.6.77** to compile the code.

### Quick Setup ⚡🛠️

```bash
git clone https://github.com/matyas-brabec/2025-europar-llm
```

Depending on the problem you want to test, enter one of the three directories:

```bash
cd framework/histogram              # Histogram base directory
cd framework/game-of-life/infrastructure  # Game of Life base directory
cd framework/knn                   # k-NN base directory
```

To compile and run with the default parameters (those tested in the paper), simply run `make` followed by `make run` in the chosen base directory:

```bash
make      # Compiling the code
make run  # Example: running in the Game of Life base directory

# Example output:
# 595.5927 0.2180192881375571 OK
```

The script runs **13 test cases** in total (the first 3 are warm-up iterations). The first number in the output is the **mean execution time** (ms) of the 10 hot runs, the second is the **standard deviation**, and the last is the **verification result** against a baseline implementation.

---

## Replication 🔄

If you want to replicate our results, there are multiple options. You can:

1. **Generate new code** using the prompts we provided.
2. **Test the existing generated code** with our framework.

### Generating Code 🤖📜

To generate new implementations, provide the selected **LLM** with the **system prompt** and individual prompts located in the [~/prompts](./prompts/) directory. Follow the documentation of your chosen LLM for inference.

---

## Framework 🏗️

Once you have generated or selected an implementation to test, navigate to the [framework](./framework) directory. We provide a Python script, [run-experiments.py](./framework/run-experiments.py), to test selected solutions.

#### Running Experiments 🏃‍♂️📊

```bash
cd framework
python3 run-experiments.py <infrastructure_directory> <implementations_directory>
```

The first argument points to the infrastructure code that manages test cases. Use the following paths for individual cases:

- **Histogram:** `~/framework/histogram`
- **Game of Life:** `~/framework/game-of-life/infrastructure`
- **k-NN:** `~/framework/knn`

The second argument points to the directory containing the tested sources, which can be either **reference solutions** or **generated implementations**:

| Test Case | Reference Solutions | Single-Response Generated |
|-----------|---------------------|--------------------|
| **Histogram** | [~/framework/histogram/reference](./framework/histogram/reference/) | [~/results/histogram](./results/histogram/) |
| **Game of Life** | [~/framework/game-of-life/reference](./framework/game-of-life/reference/) | [~/results/gol](./results/gol/) |
| **k-NN** | [~/framework/knn/reference](./framework/knn/reference/) | [~/results/knn](./results/knn/) |

Each directory contains **CUDA (`.cu`) source files**, which may be nested at any depth.

#### Example: Running Game of Life Reference 🏃‍♂️🎮🦠

```bash
cd framework
python3 run-experiments.py ./game-of-life/infrastructure ./game-of-life/reference
```

This produces a **CSV file** with the following format:

```csv
experiment_id;time;std;compiled;verified;runtime_err;extra
```

Example output:

```csv
experiment_id;time;std;compiled;verified;runtime_err;extra
baseline/gol;590.8121;0.021988;True;True;;
naive-bitwise/gol;110.5659;0.012549;True;True;;
popc-bitwise-macro/gol;59.1469;0.008811;True;True;;
popc-bitwise/gol;80.3456;0.009631;True;True;;
sota-adder/gol;9.4531;0.010524;True;True;;
tiled-bitwise-macro/gol;35.0066;0.003656;True;True;;
dumb-bit-per-thr/gol;512.6912;0.003487;True;True;;
smarter-bit-per-thr/gol;520.7016;0.005765;True;True;;
```

---

## Graphs 📊

To reproduce graphs similar to those in the paper, use the Python script [print-graph.py](./framework/print-graph.py):

```bash
python3 print-graph.py <config_keys> <path_to_experiments> <path_to_references> [tag]
```

### Parameters ⚙️📝

1. **Config keys**: Specifies graph styles based on [print-graph.config.json](./framework/print-graph.config.json). Combine multiple keys using `@` (e.g., `hist@with-log-scale`).
2. **Path to experiments**: The CSV file containing experiment results.
3. **Path to references**: The CSV file containing reference results.
4. **Tag (optional)**: A tag for naming the output file.

#### Example Usage 📌

```bash
cd framework
python3 print-graph.py gol@with-log-scale ../measured-times/gol-hopper.csv ../measured-times/gol-hopper.csv TESTING_GRAPH_
```

This generates a file `TESTING_GRAPH_gol-hopper.graph.pdf` in the framework directory.

#### Requirements 🛠️📦

To run **print-graph.py**, install the following Python packages:

```bash
pip install pandas matplotlib numpy json5
```

Additionally, **LaTeX is required** for proper graph rendering. If LaTeX is not installed, disable it in `print-graph.py` by setting:

```python
plt.rcParams["text.usetex"] = False
```
