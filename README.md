# üî¨ü§ñ Tutoring LLM into a Better CUDA Optimizer ‚ö°üéØ

[![LICENSE](https://img.shields.io/badge/license-MIT-blue.svg)](./LICENSE) [![zenodo](https://img.shields.io/badge/zenodo-10.5281/zenodo.15580207-blue)](https://doi.org/10.5281/zenodo.15580207) [![DOI](https://img.shields.io/badge/DOI-TODO-blue)](todo)

> For the presented results, see the [main](https://github.com/matyas-brabec/2025-europar-llm/tree/main) branch of this repository. This branch contains replication of the experiments with a newer language model, `gpt-5-2025-08-07`. This branch does not include the original experiments with `o3-mini-2025-01-31` and, most notably, the interactive prompting scenarios and detailed review of the implementations in [results](./results). The differences in performance between the two models are documented in the [GPT-5 vs. o3-mini](#gpt-5-vs-o3-mini-‚öîÔ∏è) section below.

This repository contains the replication package for the paper titled *"Tutoring LLM into a Better CUDA Optimizer"* presented at Euro-Par 2025.

```bibtex
@inproceedings { TODO }
```

## GPT-5 vs. o3-mini ‚öîÔ∏è

The experiments conducted with the `gpt-5-2025-08-07` model demonstrate notable improvements in generating optimized CUDA code compared to the earlier `o3-mini-2025-01-31` model; most notably, the newer model generally achieves higher efficiency with less direction compared to the older model and achieves a better success rate at generating working solutions for more complex tasks. The following three sections highlight the key differences in the GoL, Histogram, and k-NN "single-response" tasks.

### Game of Life

The `gpt-5` model generates code at the target efficiency ("Full Adder") with a 100% success rate on all prompts `gol02`-`gol06`. In contrast, the `o3-mini` model achieved this target consistently only on the (most detailed) `gol06` prompt, while most generated results performed at the "Naive Bitwise" reference solution. See [~/measured-times/graphs/log-scale_gol-hopper.graph.pdf](measured-times/graphs/log-scale_gol-hopper.graph.pdf) in both branches of the repository for the visual comparison.

### Histogram

The `gpt-5` model consistently generates more efficient code for prompts `hist01`-`hist05`; however, the mean efficiency on prompts `hist06` and `hist07` is slightly lower than on the `hist05` prompt that achieves the expected efficiency. This performance degradation is not present in the `hist06` and `hist07` implementations generated by the `o3-mini` model, which perform at the expected efficiency level. See [~/measured-times/graphs/log-scale_histogram-loremipsum-hopper.graph.pdf](measured-times/graphs/log-scale_histogram-loremipsum-hopper.graph.pdf) in both branches of the repository for the visual comparison.

### k-NN

The `gpt-5` model demonstrates a significant improvement in generating efficient code for the k-NN task. The success rate of generating a working solution has increased from 8%  with the `o3-mini` model (virtually all of the successes are for the `knn01` prompt that doesn't specify any optimization technique) to 38% with the `gpt-5` model. Furthermore, `gpt-5` generates at least one working solution for each of the `knn01`-`knn08` prompts.

The efficiency of the `gpt-5` implementations gradually improves with more detailed prompts for the $k=32$ use case - this shows that the tutoring approach is effective for the k-NN task as well (if the language model is sufficiently powerful). The results for the $k=1024$ case are more varied; however, the mean efficiency of the implementations generated by the `gpt-5` model is the highest for the (most detailed) `knn08` prompt, further confirming the validity of the tutoring approach for the k-NN task, which was not possible with the `o3-mini` model.

With the 38% success rate of the `gpt-5` model at generating working solutions for the k-NN task for the given optimization hints, the k-NN task still presents challenges for the state-of-the-art language models. The research presented at the conference concluded that the state-of-the-art language models struggle with the semantics of CUDA implementations involving inter-thread communication and synchronization; with `gpt-5`, this issue is partially alleviated, and the model demonstrates an improved understanding of these concepts.

The current success rate allows for a sufficiently informative visualization of the model's performance across different `knn` prompts. See [~/measured-times/graphs/log-scale_knn-32-hopper.graph.pdf](measured-times/graphs/log-scale_knn-32-hopper.graph.pdf) for the visualization of the performance for the $k=32$ use case and [~/measured-times/graphs/log-scale_knn-1024-hopper.graph.pdf](measured-times/graphs/log-scale_knn-1024-hopper.graph.pdf) for the $k=1024$ use case.

## About üß†üí°

Large language models (LLMs) are increasingly used for code generation, debugging, and optimization. This project explores their ability to generate efficient CUDA code for well-known parallel computing tasks. While LLMs can apply some optimizations independently, their performance improves significantly with targeted guidance. Our evaluation combines automatic benchmarking and manual code review, revealing that LLMs require tutoring to reach expert-level optimization but show promising potential for assisting developers.

## Index üîçüìñ

To navigate this repository more easily, here is an index:

| üìÇ Section | üìù Description with Links |
| --- | --- |
| ‚öôÔ∏è Managing Code | The main folder for managing the implementation of three provided test cases (*histogram*, *Game of Life*, and *k-NN*) is in the [~/framework](./framework) directory. For further information, read the [README](./framework/README.md) inside. |
| | The subdirectories [~/framework/histogram](./framework/histogram/), [~/framework/game-of-life](./framework/game-of-life/), and [~/framework/knn](./framework/knn/) contain the respective test cases. Each has a `<Hist\|GoL\|kNN>/reference` subfolder with reference solutions. |
| üîó Open API | The code managing calls to OpenAI's ChatGPT is located in [~/gpt-querying](./gpt-querying/) along with a [README](./gpt-querying/README.md). |
| ‚úçÔ∏è Single-Response Prompts | All prompts are in the [~/prompts](./prompts/) folder. Apart from the prompt files (`.prompt.md`), it also contains common parts that can be included in prompts. |
| ü§ñ Generated "Single-Response" Sources | Generated code in the *"Single-Response"* mode is in [~/results](./results/). Individual solutions for each prompt and its 10 attempts are in [~/results/histogram](./results/histogram/), [~/results/gol](./results/gol/), and [~/results/knn](./results/knn/). |
| üßê Reviews | Code reviews for *"Single-Response"* generated code are in the [~/reviews](./reviews/) directory. |
| üìä Measurements | Code measurements for tested architectures (`Ampere`, `Volta`, and `Hopper`) are in [~/measured-times](./measured-times/). The format is CSV. For each architecture and problem, there are two CSVs: one for *"Single-Response"* codes, and one for reference solutions. The pre-generated graphs are located in [~/measured-times/graphs](./measured-times/graphs/). |

## Tutorial üöÄ

We used **GCC 13.2.0** and **NVCC 12.6.77** to compile the code.

### Quick Setup ‚ö°üõ†Ô∏è

```bash
git clone https://github.com/matyas-brabec/2025-europar-llm
```

Depending on the problem you want to test, enter one of the three directories:

```bash
cd framework/histogram              # Histogram base directory
cd framework/game-of-life/infrastructure  # Game of Life base directory
cd framework/knn                   # k-NN base directory
```

To compile and run with the default parameters (those tested in the paper), simply run `make` followed by `make run` in the chosen base directory (note that the `knn` part requires `CMake` of version 3.20 or higher):

```bash
make      # Compiling the code
make run  # Example: running in the Game of Life base directory

# Example output:
# 595.5927 0.2180192881375571 OK
```

The script runs **13 test cases** in total (the first 3 are warm-up iterations). The first number in the output is the **mean execution time** (ms) of the 10 hot runs, the second is the **standard deviation**, and the last is the **verification result** against a baseline implementation.

---

## Replication üîÑ

If you want to replicate our results, there are multiple options. You can:

1. **Generate new code** using the prompts we provided.
2. **Test the existing generated code** with our framework.

### Generating Code ü§ñüìú

To generate new implementations, provide the selected **LLM** with the **system prompt** and individual prompts located in the [~/prompts](./prompts/) directory. Follow the documentation of your chosen LLM for inference.

---

## Framework üèóÔ∏è

Once you have generated or selected an implementation to test, navigate to the [framework](./framework) directory. We provide a Python script, [run-experiments.py](./framework/run-experiments.py), to test selected solutions (requiring Python 3.8 or higher and the virtual environment set up as described in [framework/README.md](./framework/README.md)).

### Running Experiments üèÉ‚Äç‚ôÇÔ∏èüìä

```bash
cd framework

# (Set up and activate the virtual environment if not done yet)

python3 run-experiments.py <infrastructure_directory> <implementations_directory>
```

The first argument points to the infrastructure code that manages test cases. Use the following paths for individual cases:

- **Histogram:** `~/framework/histogram`
- **Game of Life:** `~/framework/game-of-life/infrastructure`
- **k-NN:** `~/framework/knn`

The second argument points to the directory containing the tested sources, which can be either **reference solutions** or **generated implementations**:

| Test Case | Reference Solutions | Single-Response Generated |
|-----------|---------------------|--------------------|
| **Histogram** | [~/framework/histogram/reference](./framework/histogram/reference/) | [~/results/histogram](./results/histogram/) |
| **Game of Life** | [~/framework/game-of-life/reference](./framework/game-of-life/reference/) | [~/results/gol](./results/gol/) |
| **k-NN** | [~/framework/knn/reference](./framework/knn/reference/) | [~/results/knn](./results/knn/) |

Each directory contains **CUDA (`.cu`) source files**, which may be nested at any depth.

#### Example: Running Game of Life Reference üèÉ‚Äç‚ôÇÔ∏èüéÆü¶†

```bash
cd framework
python3 run-experiments.py ./game-of-life/infrastructure ./game-of-life/reference
```

This produces a **CSV file** with the following format:

```csv
experiment_id;time;std;compiled;verified;runtime_err;extra
```

Example output:

```csv
experiment_id;time;std;compiled;verified;runtime_err;extra
baseline/gol;590.8121;0.021988;True;True;;
naive-bitwise/gol;110.5659;0.012549;True;True;;
popc-bitwise-macro/gol;59.1469;0.008811;True;True;;
popc-bitwise/gol;80.3456;0.009631;True;True;;
sota-adder/gol;9.4531;0.010524;True;True;;
tiled-bitwise-macro/gol;35.0066;0.003656;True;True;;
dumb-bit-per-thr/gol;512.6912;0.003487;True;True;;
smarter-bit-per-thr/gol;520.7016;0.005765;True;True;;
```

---

## Graphs üìä

To reproduce graphs presented in the paper, run the following command in the `measured-times` directory (this requires Python 3.8 or higher and automatically sets up a virtual environment `.venv` in the `framework` directory):

```bash
bash generate_all_graphs.sh
```

This script generates all presented graphs (and some unused variants) from the CSV files in the `measured-times` directory and saves them in the `graphs` subdirectory.
